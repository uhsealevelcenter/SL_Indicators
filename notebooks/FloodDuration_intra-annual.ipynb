{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Duration\n",
    "Tralala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.ipynb\n",
    "data_dir = Path('../data' )\n",
    "output_dir = Path('../output') \n",
    "\n",
    "# load the data\n",
    "SL_daily_max = xr.open_dataset(data_dir / 'SL_daily_max.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the necessary data\n",
    "flood_day = SL_daily_max['flood_day']\n",
    "time = SL_daily_max['time']\n",
    "station_names = SL_daily_max['station_name'].values\n",
    "\n",
    "# Convert time to pandas datetime\n",
    "time = pd.to_datetime(time.values)\n",
    "\n",
    "# Initialize a dictionary to hold results\n",
    "flood_events = {}\n",
    "\n",
    "# Loop through each station\n",
    "for i, station in enumerate(station_names):\n",
    "    station_flood_days = flood_day[:, i].values\n",
    "    station_flood_days = pd.Series(station_flood_days, index=time)\n",
    "    \n",
    "    # Group by year\n",
    "    station_flood_days_by_year = station_flood_days.groupby(station_flood_days.index.year)\n",
    "    \n",
    "    # Initialize list to hold all events for this station\n",
    "    station_events = []\n",
    "    \n",
    "    for year, data in station_flood_days_by_year:\n",
    "        flood_event_durations = []\n",
    "        current_event_length = 0\n",
    "        \n",
    "        for day in data:\n",
    "            if day:\n",
    "                current_event_length += 1\n",
    "            else:\n",
    "                if current_event_length > 0:\n",
    "                    flood_event_durations.append(current_event_length)\n",
    "                current_event_length = 0\n",
    "        \n",
    "        # Append the last event if it was ongoing at the end of the year\n",
    "        if current_event_length > 0:\n",
    "            flood_event_durations.append(current_event_length)\n",
    "        \n",
    "        # Store the events for this year\n",
    "        station_events.append({\n",
    "            \"year\": year,\n",
    "            \"events\": flood_event_durations\n",
    "        })\n",
    "    \n",
    "    # Store the results for the station\n",
    "    flood_events[station] = station_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the plot grid with larger station names\n",
    "\n",
    "# Extract unique years from the dataset, sorted for plotting\n",
    "years = sorted({year_data[\"year\"] for station_events in flood_events.values() for year_data in station_events})\n",
    "\n",
    "# Define the number of stations and 5-year interval ticks\n",
    "num_stations = len(station_names)\n",
    "five_year_ticks = [year for year in years if year % 5 == 0]\n",
    "\n",
    "# Create subplots: one per station, sharing the x-axis\n",
    "fig, axes = plt.subplots(nrows=num_stations, ncols=1, figsize=(6, 10), sharex=True)\n",
    "plt.subplots_adjust(hspace=0.05)  # Decrease hspace value to bring subplots closer\n",
    "\n",
    "# Generate boxplots for each station\n",
    "for ax, station in zip(axes, station_names):\n",
    "    yearly_durations = {year: [] for year in years}\n",
    "\n",
    "    # Gather flood event durations per year for the current station\n",
    "    for year_data in flood_events[station]:\n",
    "        yearly_durations[year_data[\"year\"]].extend(year_data[\"events\"])\n",
    "\n",
    "    # Prepare the boxplot data\n",
    "    boxplot_data = [yearly_durations[year] for year in years]\n",
    "\n",
    "    # Create the boxplot with customized aesthetics\n",
    "    ax.boxplot(\n",
    "        boxplot_data,\n",
    "        positions=range(len(years)),\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='lightblue', color='blue', alpha=0.7),\n",
    "        medianprops=dict(color='red'),\n",
    "        whiskerprops=dict(color='blue', alpha=0.5),\n",
    "        capprops=dict(color='blue', alpha=0.5)\n",
    "    )\n",
    "    ax.set_ylim(0, 30)\n",
    "    # Label each subplot with the station name\n",
    "    ax.text(2, ax.get_ylim()[1] * 0.8, station, fontsize=12, ha='left', color='black')\n",
    "    ax.set_yticks(range(0, 26, 10))\n",
    "    ax.set_yticklabels(range(0, 26, 10),fontsize=10)\n",
    "    \n",
    "\n",
    "# Remove x-axis labels from all but the last subplot\n",
    "for ax in axes[:-1]:\n",
    "    ax.set_xticks([])\n",
    "\n",
    "# Set x-ticks at 5-year intervals on the last subplot\n",
    "axes[-1].set_xticks([years.index(year) for year in five_year_ticks])\n",
    "axes[-1].set_xticklabels(five_year_ticks, ha='right', fontsize=10)\n",
    "axes[-1].set_xlabel('Year', fontsize=12)\n",
    "\n",
    "# Add a single y-axis label for the entire figure\n",
    "fig.text(0.04, 0.5, 'Duration (days)', va='center', rotation='vertical', fontsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hourly data\n",
    "hourly_data = xr.open_dataset(data_dir / 'SL_hourly_data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flood_ds(hourly_data = hourly_data, threshold=30):\n",
    "    flood_hour = (hourly_data.sea_level_MHHW.values > threshold)\n",
    "    flood_hour\n",
    "    flood_hour = np.transpose(flood_hour)\n",
    "    flood_hour_df = pd.DataFrame(flood_hour, index = hourly_data.time.values, columns = hourly_data.record_id.values)\n",
    "\n",
    "    # get sea level data where flood_hour is True\n",
    "\n",
    "    flood_data = hourly_data.sea_level_MHHW.where(flood_hour.T)\n",
    "    flood_data_df = pd.DataFrame(flood_data.T-threshold, index = hourly_data.time.values, columns = hourly_data.record_id.values)\n",
    "\n",
    "    # Find indices of flood events, and the duration of each event for record id = 57\n",
    "    # Initialize a dictionary to store the durations\n",
    "    flood_durations = {}\n",
    "    flood_heights = {}\n",
    "\n",
    "\n",
    "    df_flood = flood_hour_df \n",
    "\n",
    "    # Loop through each sensor column\n",
    "    for column in flood_hour_df.columns:\n",
    "        flood_durations[column] = []\n",
    "        flood_heights[column] = []\n",
    "\n",
    "        # Find the indices where flooding starts and ends\n",
    "        flood_events = df_flood[column].ne(df_flood[column].shift()).cumsum()\n",
    "\n",
    "        # Group by the flood event indices, filtering out False events\n",
    "        for event_id, group in df_flood.groupby(flood_events):\n",
    "            if group[column].iloc[0]:  # Only consider True (flood) events\n",
    "                start_time = group.index.min()\n",
    "                end_time = group.index.max()\n",
    "                duration = end_time - start_time \n",
    "                # round to nearest hour\n",
    "                duration = np.round(duration.total_seconds() / 3600) + 1\n",
    "                flood_durations[column].append((start_time, duration))\n",
    "                height = flood_data_df.loc[start_time:end_time, column].max() # max height\n",
    "                flood_heights[column].append((start_time, height))\n",
    "\n",
    "    # make dataframe from dictionary\n",
    "    # Assuming flood_durations and hourly_data are already defined\n",
    "\n",
    "    # Initialize an empty list to store DataArrays\n",
    "    data_arrays = []\n",
    "\n",
    "    # Loop through each record_id and create a DataArray\n",
    "    for record_id in hourly_data.record_id.values:\n",
    "        flood_durations_df = pd.DataFrame(flood_durations[record_id])\n",
    "        flood_durations_df.columns = ['time', 'duration']\n",
    "\n",
    "        flood_heights_df = pd.DataFrame(flood_heights[record_id])   \n",
    "        flood_heights_df.columns = ['time', 'height']\n",
    "\n",
    "        # Merge the two DataFrames on 'time'\n",
    "        merged_df = pd.merge(flood_durations_df, flood_heights_df, on='time')\n",
    "\n",
    "        # Create a DataArray with both 'duration' and 'height'\n",
    "        flood_data_da = xr.DataArray(\n",
    "            merged_df[['duration', 'height']].values,\n",
    "            dims=['time', 'variable'],\n",
    "            coords={'time': merged_df.time.values, 'variable': ['duration', 'height'], 'record_id': record_id}\n",
    "        )\n",
    "\n",
    "        data_arrays.append(flood_data_da)\n",
    "\n",
    "\n",
    "    # Combine all DataArrays into a single Dataset\n",
    "    flood_data_ds = xr.concat(data_arrays, dim='record_id')\n",
    "\n",
    "    # Convert to Dataset\n",
    "    flood_data_ds = flood_data_ds.to_dataset(dim='variable')\n",
    "\n",
    "    # make storm time by shifting time by 4 months, such that May 1st is the start of the storm season\n",
    "    # Convert the time values to pandas DatetimeIndex\n",
    "    time_values = pd.to_datetime(flood_data_ds.time.values)\n",
    "    \n",
    "    # Jan 1 should be the equivalent of May 1\n",
    "    shifted_time_values = time_values + pd.DateOffset(months=4)\n",
    "    \n",
    "    #remove anything in 1982\n",
    "    shifted_time_values = shifted_time_values[shifted_time_values.year != 1982]\n",
    "    \n",
    "    \n",
    "    # Assign the shifted time values back to the dataset\n",
    "    flood_data_ds['time'] = shifted_time_values\n",
    "\n",
    "    return flood_data_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_data_ds = get_flood_ds(hourly_data = hourly_data, threshold=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot flood duration vs year\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# make figure size small\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Create a custom color ramp and exclude the lightest shade\n",
    "full_palette = sns.color_palette(\"Blues\", n_colors=256)\n",
    "# adjusted_heatmap_palette = full_palette[50:]  # Skip the lightest 50 shades\n",
    "\n",
    "# Convert the adjusted palette to a colormap\n",
    "# custom_palette = LinearSegmentedColormap.from_list(\"custom_blues\", adjusted_heatmap_palette)\n",
    "custom_palette = sns.color_palette(\"OrRd\", as_cmap=True)\n",
    "\n",
    "\n",
    "# selected_data['duration'] = selected_data['duration'].astype(float)\n",
    "# flood_durations_ds['month'] = flood_durations_ds['month'].astype(float)\n",
    "\n",
    "record_ids = flood_data_ds['record_id'].values\n",
    "station_names = hourly_data['station_name'].sel(record_id=record_ids).values\n",
    "\n",
    "for i, record_id in enumerate(record_ids):\n",
    "    # Select data for the current record_id and drop NaNs\n",
    "    selected_data = flood_data_ds.sel(record_id=record_id).dropna(dim='time')\n",
    "    \n",
    "    # Ensure 'duration' is numeric\n",
    "    selected_data['duration'] = selected_data['duration'].astype(float)\n",
    "    \n",
    "    # Plot each station's data on a different y-coordinate\n",
    "    scatter = plt.scatter(selected_data['time'], [i + 1] * len(selected_data['time']), s=10 * selected_data['duration'], c=selected_data['height'], cmap=custom_palette)\n",
    "\n",
    "plt.xlabel('Time (Storm Year)')\n",
    "plt.yticks(range(1, len(record_ids) + 1), hourly_data['station_name'].sel(record_id=record_ids).values)  # Set y-ticks to show station labels\n",
    "plt.colorbar(scatter, label='Max height \\nabove threshold (cm)')\n",
    "\n",
    "# show the colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot height vs duration for each station\n",
    "# make figure size small\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def plot_flood_duration_vs_height(flood_data_ds, station_names):\n",
    "    # Create figure and subplots\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(10, 4), sharex=True, sharey=True)  # 2 rows, 4 columns\n",
    "\n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop through each station\n",
    "    for i, record_id in enumerate(record_ids):\n",
    "        # Select data for the current record_id and drop NaNs\n",
    "        selected_data = flood_data_ds.sel(record_id=record_id).dropna(dim='time')\n",
    "\n",
    "        # Ensure 'duration' is numeric\n",
    "        selected_data['duration'] = selected_data['duration'].astype(float)\n",
    "\n",
    "        # Extract year and apply jitter to duration for better visualization\n",
    "        jitter_duration = selected_data['duration'] + np.random.uniform(-0.1, 0.1, size=selected_data['duration'].size)\n",
    "\n",
    "        # Calculate point density\n",
    "        xy = np.vstack([jitter_duration, selected_data['height']])\n",
    "        z = gaussian_kde(xy)(xy)\n",
    "\n",
    "        # Plot each station's data on its respective subplot, using the density (z) as color\n",
    "        scatter = axes[i].scatter(jitter_duration, selected_data['height'], c=z, cmap='plasma', s=20)\n",
    "\n",
    "        # # Set labels for each subplot\n",
    "        axes[i].set_title(station_names[i], fontsize=8)\n",
    "        # axes[i].set_xlabel('Duration (hours)')\n",
    "        # axes[i].set_ylabel('Max height above threshold (cm)')\n",
    "\n",
    "        # set x and y limits\n",
    "        axes[i].set_xlim(0, 10)\n",
    "        axes[i].set_ylim(0, 30)\n",
    "\n",
    "    # Add a common colorbar\n",
    "    cbar = fig.colorbar(scatter, ax=axes, orientation='vertical', label='Density')\n",
    "\n",
    "    # Set custom ticks and labels for the colorbar\n",
    "    cbar.set_ticks([np.min(z), np.mean(z), np.max(z)])\n",
    "    cbar.set_ticklabels(['Sparse', 'Moderate', 'Dense'], rotation=90,verticalalignment='center',fontsize=8)  # Custom labels\n",
    "\n",
    "    # add x and y labels\n",
    "    fig.text(0.45, -0.01, 'Duration (hours)', ha='center', fontsize=12)\n",
    "    fig.text(0.05, 0.5, 'Max height above threshold (cm)', va='center', rotation='vertical', fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous plots, it is clear that Midway is experiencing more frequent flooding, but less intense. Is this a remove-the-trend thing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_data_30 = get_flood_ds(hourly_data=hourly_data,threshold=30)\n",
    "flood_data_15 = get_flood_ds(hourly_data=hourly_data,threshold=15)\n",
    "plot_flood_duration_vs_height(flood_data_15, station_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Example data for one station (you can adjust this to loop through your stations)\n",
    "record_id = 50\n",
    "selected_data = flood_data_15.sel(record_id=record_id).dropna(dim='time')\n",
    "selected_data['duration'] = selected_data['duration'].astype(float)\n",
    "jitter_duration = selected_data['duration'] + np.random.uniform(-0.1, 0.1, size=selected_data['duration'].size)\n",
    "\n",
    "# Calculate point density for color mapping\n",
    "xy = np.vstack([jitter_duration, selected_data['height']])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "# Create the figure and GridSpec layout\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "gs = GridSpec(4, 4, figure=fig)\n",
    "\n",
    "# Create scatter plot in the center\n",
    "ax_main = fig.add_subplot(gs[1:4, 0:3])\n",
    "scatter = ax_main.scatter(jitter_duration, selected_data['height'], c=z, cmap='plasma', s=10)\n",
    "ax_main.set_xlabel('Duration (hours)')\n",
    "ax_main.set_ylabel('Peak water level above threshold (cm)')\n",
    "\n",
    "# Add colorbar to scatter plot\n",
    "# cbar = fig.colorbar(scatter, ax=ax_main, orientation='vertical', label='Density')\n",
    "# cbar.set_ticks([np.min(z), np.mean(z), np.max(z)])\n",
    "# cbar.ax.set_yticklabels(['Sparse', 'Moderate', 'Dense'], rotation=90, verticalalignment='center')\n",
    "\n",
    "# Create top histogram for duration\n",
    "ax_histx = fig.add_subplot(gs[0, 0:3], sharex=ax_main)\n",
    "ax_histx.hist(jitter_duration, bins=30, color='lightblue', alpha=0.6, edgecolor='black')\n",
    "ax_histx.axis('off')  # Hide axes for cleaner look\n",
    "\n",
    "# Create side histogram for height\n",
    "ax_histy = fig.add_subplot(gs[1:4, 3], sharey=ax_main)\n",
    "ax_histy.hist(selected_data['height'], bins=30, orientation='horizontal', color='lightblue', alpha=0.6, edgecolor='black')\n",
    "ax_histy.axis('off')  # Hide axes for cleaner look\n",
    "\n",
    "# Adjust layout to fit everything nicely\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = selected_data['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durations\n",
    "sorted_durations = np.sort(selected_data['duration'])\n",
    "exceedance_prob_duration = 1.0 - np.arange(1, len(sorted_durations) + 1) / len(sorted_durations)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.step(sorted_durations, exceedance_prob_duration, where='post')\n",
    "plt.title('Exceedance Probability of Durations')\n",
    "plt.xlabel('Duration (hours)')\n",
    "plt.ylabel('Exceedance Probability')\n",
    "plt.grid(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_heights = np.sort(selected_data['height'])\n",
    "exceedance_prob_height = 1.0 - np.arange(1, len(sorted_heights) + 1) / len(sorted_heights)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.step(sorted_heights, exceedance_prob_height, where='post')\n",
    "plt.title('Exceedance Probability of Heights')\n",
    "plt.xlabel('Height Above Threshold (cm)')\n",
    "plt.ylabel('Exceedance Probability')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = selected_data.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize durations\n",
    "\n",
    "# Define the ordered categories\n",
    "duration_order = ['Short (1-2 hours)', 'Medium (3-6 hours)', 'Sustained (>6 hours)']\n",
    "\n",
    "def categorize_duration(duration):\n",
    "    if duration <= 2:\n",
    "        return duration_order[0]\n",
    "    elif 3 <= duration <= 6:\n",
    "        return duration_order[1]\n",
    "    else:\n",
    "        return duration_order[2]\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function to create a new column\n",
    "selected_data['duration_category'] = selected_data['duration'].apply(categorize_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency of each category\n",
    "category_counts = selected_data['duration_category'].value_counts()\n",
    "total_events = len(data)\n",
    "\n",
    "# Calculate the probability for each category\n",
    "category_probs = category_counts / total_events\n",
    "\n",
    "print(\"Frequency of Duration Categories:\")\n",
    "print(category_counts)\n",
    "\n",
    "print(\"\\nProbability of Duration Categories:\")\n",
    "print(category_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the categories are ordered\n",
    "category_counts = category_counts.reindex(duration_order)\n",
    "\n",
    "# Compute cumulative counts from shortest to longest\n",
    "cumulative_counts = category_counts[::-1].cumsum()[::-1]\n",
    "\n",
    "# Compute survival probabilities\n",
    "survival_probs = cumulative_counts / total_events\n",
    "\n",
    "print(\"\\nCumulative Counts by Duration Category:\")\n",
    "print(cumulative_counts)\n",
    "\n",
    "print(\"\\nSurvival Probabilities by Duration Category:\")\n",
    "print(survival_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total observation period in years\n",
    "total_years = (time.max()-time.min())/np.timedelta64(1, 's')/3600/24/365.25\n",
    "\n",
    "# Calculate lambda\n",
    "lambda_ = total_events / total_years\n",
    "\n",
    "print(f\"\\nAverage number of events per year (lambda): {lambda_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_periods = 1 / (lambda_.item() * survival_probs)\n",
    "\n",
    "# Display return periods\n",
    "print(\"\\nReturn Periods by Duration Category (in years):\")\n",
    "print(return_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the frequency of each duration category\n",
    "plt.figure(figsize=(8,6))\n",
    "category_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Frequency of Duration Categories')\n",
    "plt.xlabel('Duration Category')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the survival function\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.step(duration_order, survival_probs, where='post', marker='o', label='Survival Probability')\n",
    "plt.title('Survival Function by Duration Category')\n",
    "plt.xlabel('Duration Category')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by duration category\n",
    "grouped_data = selected_data.groupby('duration_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store GEV parameters for each category\n",
    "gev_params = {}\n",
    "\n",
    "for category, group in grouped_data:\n",
    "    # Extract height data\n",
    "    heights = group['height'].values\n",
    "    \n",
    "    # Fit GEV distribution\n",
    "    c, loc, scale = genextreme.fit(heights)\n",
    "    \n",
    "    # Store parameters\n",
    "    gev_params[category] = (c, loc, scale)\n",
    "\n",
    "    print(f\"GEV parameters for {category} category: c={c:.4f}, loc={loc:.4f}, scale={scale:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define return periods of interest\n",
    "return_periods = np.arange(2,100,2)  # in years\n",
    "\n",
    "# Calculate the non-exceedance probabilities\n",
    "probabilities = 1 - (1 / (np.array(return_periods) * lambda_.item()))\n",
    "\n",
    "# Create a DataFrame to store return levels\n",
    "return_levels_df = pd.DataFrame(index=return_periods, columns=duration_order)\n",
    "\n",
    "for category in duration_order:\n",
    "    if category not in gev_params:\n",
    "        continue\n",
    "    c, loc, scale = gev_params[category]\n",
    "    # Calculate return levels for each return period\n",
    "    return_levels = genextreme.ppf(probabilities, c, loc=loc, scale=scale)\n",
    "    return_levels_df[category] = return_levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume duration_order, gev_params, and lambda_ are predefined\n",
    "\n",
    "return_periods_interest = [2, 5, 10, 50]  # in years\n",
    "\n",
    "\n",
    "\n",
    "# Create an empty dict to store the return levels for the chosen return period\n",
    "return_levels_by_period = {period: [] for period in return_periods_interest}\n",
    "\n",
    "for return_period in return_periods_interest:\n",
    "    # Calculate the non-exceedance probability for the chosen return period\n",
    "    probability = 1 - (1 / (return_period * lambda_.item()))\n",
    "    \n",
    "    # Iterate over each duration category to calculate the return level for the chosen period\n",
    "    for category in duration_order:\n",
    "        if category not in gev_params:\n",
    "            continue\n",
    "        c, loc, scale = gev_params[category]\n",
    "        \n",
    "        # Calculate the return level for the specific return period\n",
    "        return_level = genextreme.ppf(probability, c, loc=loc, scale=scale)\n",
    "        return_levels_by_period[return_period].append(return_level)\n",
    "\n",
    "# Plot the return level curve for the chosen return period (e.g., 2-year)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for return_period in return_periods_interest:\n",
    "    # Plot the return level curve for each return period\n",
    "    plt.plot(duration_order, return_levels_by_period[return_period], marker='o', label=f'{return_period}-year return level')\n",
    "\n",
    "plt.title(f'IDF Curves for' + ' ' + station_names[i])\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Height Above Threshold (cm)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't do a curve here because the distribution of durations is very limited (at most stations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the return levels for each duration category\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for category in duration_order:\n",
    "    if category not in gev_params:\n",
    "        continue\n",
    "    c, loc, scale = gev_params[category]\n",
    "    \n",
    "    # Calculate the return levels for each return period\n",
    "    return_levels = genextreme.ppf(1 - (1 / (np.array(return_periods) * lambda_.item())), c, loc=loc, scale=scale)\n",
    "    \n",
    "    # Plot the return levels for each duration category\n",
    "    plt.plot(return_periods, return_levels,  label=category)\n",
    "\n",
    "plt.title('Return Levels for Each Duration Category')\n",
    "plt.xlabel('Return Period (years)')\n",
    "plt.ylabel('Height Above Threshold (cm)')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLI311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
