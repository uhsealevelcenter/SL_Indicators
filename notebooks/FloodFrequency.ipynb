{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87540d93",
   "metadata": {},
   "source": [
    "# Minor Flood Frequency\n",
    "```{glue:figure} threshold_counts_days_fig\n",
    ":scale: 50%\n",
    ":align: right\n",
    "```\n",
    "\n",
    "In this notebook we will plot two indicators concerning flooding at the Hawaii tide gauges, after first taking a general look at the type of data we are able to plot. These indicators are based on a 'flooding' threshold, using [relative sea level](https://tidesandcurrents.noaa.gov/sltrends/faq.html). \n",
    "\n",
    "Download Files:\n",
    "[Map](https://uhslc.soest.hawaii.edu/jfiedler/SeaLevelIndicators/output/Hawaii_Region_Output/SL_FloodFrequency_map.png) |\n",
    "[Time Series Plot](https://uhslc.soest.hawaii.edu/jfiedler/SeaLevelIndicators/output/Hawaii_Region_Output/SL_FloodFrequency_threshold_counts_days.png) |\n",
    "[Table](https://uhslc.soest.hawaii.edu/jfiedler/SeaLevelIndicators/output/Hawaii_Region_Output/flood_frequency_table.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65a954",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We first need to import the necessary libraries, establish our input/output directories, and set up some basic plotting rules. As with our other notebooks, we'll do this by running another notebook called \"setup.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae97166",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%run setup.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60ab70",
   "metadata": {},
   "source": [
    " ##  Retrieve the Tide Station(s) Data Set(s)\n",
    "\n",
    "We stored this previously in our data directory as \"rsl_hawaii.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb726b",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "rsl = xr.open_dataset(data_dir / 'rsl_hawaii_noaa.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee27457",
   "metadata": {},
   "source": [
    "and we'll save a few variables that will come up later for report generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bf368",
   "metadata": {},
   "source": [
    "### Set the Datum to MHHW\n",
    "\n",
    "```{margin} A Note on Datums\n",
    "\n",
    "The sea level variable in the netcdf file is sea level **relative to the station datum**. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80e10f",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# convert sea level to MHHW\n",
    "rsl['sea_level_MHHW'] = rsl['sea_level'] - rsl['MHHW']\n",
    "rsl['sea_level_MHHW'].attrs['units'] = 'm'\n",
    "rsl['sea_level_MHHW'].attrs['long_name'] = 'Sea Level, relative to MHHW'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67563db8",
   "metadata": {},
   "source": [
    "###  Assess Station Data Quality for the POR (1983-2024)\n",
    "\n",
    "To do this, we'll plot all the sea level data to make sure our data looks correct, and then we'll truncate the data set to the time period of record (POR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59794c96",
   "metadata": {},
   "source": [
    "```{margin} Watch the units!\n",
    "```{caution} Note that the sea_level variable here is in meters (m)! If we want to plot things on a centimeter (cm) scale, we have to multiply by 100.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba9fc5",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(sharex=True, figsize=(10, 10))\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "rsl = rsl.sortby('lat')\n",
    "# Initial offset\n",
    "offset = 0\n",
    "# The amount to offset each successive line\n",
    "offset_increment = 150  # Adjust this value based on your data scale and visual preference\n",
    "\n",
    "station_ids = rsl['station_id'].values\n",
    "station_names = rsl['station_name'].values\n",
    "\n",
    "for i, (station_id, station_name) in enumerate(zip(station_ids, station_names)):\n",
    "    sea_level_data = 100 * rsl.sea_level_MHHW.sel(station_id=station_id).values  # cm\n",
    "    ax.plot(rsl.time.values, sea_level_data + offset, label=station_name)\n",
    "    ax.axhline(offset, color='black', linewidth=0.5, linestyle=':')\n",
    "    ax.annotate(\n",
    "        station_name,\n",
    "        xy=(rsl.time.values[0], offset-20),\n",
    "        xytext=(5, 0),\n",
    "        textcoords='offset points',\n",
    "        color='black',\n",
    "        fontsize=10,\n",
    "        ha='left',\n",
    "        va='top'\n",
    "    )\n",
    "    offset += offset_increment\n",
    "\n",
    "ax.set_ylabel(rsl['sea_level_MHHW'].long_name + ' (cm, offset by 150cm per station)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acad942",
   "metadata": {},
   "source": [
    "#### Identify timespan for the flood frequency analysis\n",
    "\n",
    "Now, we'll calculate trend starting from the beginning of the tidal datum analysis period epoch to the last time processed. The  epoch information is given in the datums table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16dd6e",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# make POR_start equal to Jan 1 1983 in datetime format\n",
    "POR_start = dt.datetime(1983, 1, 1)\n",
    "\n",
    "# and for now, end time will be the end of 2024\n",
    "POR_end = dt.datetime(2024, 12, 31)\n",
    "\n",
    "\n",
    "hourly_data = rsl.sel(dict(time=slice(POR_start, POR_end)))\n",
    "hourly_data = hourly_data.sortby('lat')\n",
    "\n",
    "glue(\"startPORDateTime\",POR_start.strftime('%Y-%m-%d'), display=False)\n",
    "glue(\"endPORDateTime\",POR_end.strftime('%Y-%m-%d'), display=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c812eb",
   "metadata": {},
   "source": [
    "and plot the hourly time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be20172",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "hourly_data['sea_level_MHHW'] = hourly_data['sea_level_MHHW'] * 100\n",
    "\n",
    "hourly_data['sea_level_MHHW'].attrs['units'] = 'cm'\n",
    "hourly_data['sea_level_MHHW'].attrs['long_name'] = 'Sea Level, relative to MHHW'\n",
    "\n",
    "hourly_data['sea_level_MHHW'].plot.line(x='time',label=hourly_data.station_name.values)\n",
    "\n",
    "#put the legend outside the plot\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# set xlimits\n",
    "plt.xlim([POR_start, POR_end])\n",
    "\n",
    "# set title \n",
    "titlestr = f'Tide Gauges ({POR_start.strftime(\"%Y\")}-{POR_end.strftime(\"%Y\")})'\n",
    "plt.title(titlestr)\n",
    "\n",
    "fig  = plt.gcf()\n",
    "\n",
    "glue(\"TS_full_fig\",fig,display=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7a3aa",
   "metadata": {},
   "source": [
    "```{glue:figure} TS_full_fig\n",
    ":name: \"fig-TS_full\"\n",
    "\n",
    "Full time series at the {glue:}`SL_Data_Wrangling.ipynb::station_group` tide gauges for the entire record from {glue:text}`startPORDateTime` to {glue:text}`endPORDateTime`. Note that the sea level is plotted in units of cm, relative to MHHW.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7c8e6",
   "metadata": {},
   "source": [
    "### Adjust the data from calendar year to storm year\n",
    "\n",
    "Storm year goes from May-April. We'll keep this in our back pocket if we need to change our analysis to storm years instead of calendar years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d272899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGNORING FOR NOW, DOES HAWAII DO STORM YEAR?\n",
    "\n",
    "hourly_data['day'] = (('time'), hourly_data.time.dt.dayofyear.data)\n",
    "hourly_data['month'] = (('time'), hourly_data.time.dt.month.data)    \n",
    "hourly_data['year'] = (('time'), hourly_data.time.dt.year.data)\n",
    "\n",
    "# adjust year to storm year, where the storm year starts on May 1st\n",
    "# if the month is less than 5, subtract a year\n",
    "hourly_data['year_storm'] = (('time'), hourly_data.year.data - (hourly_data.month.data < 5))\n",
    "\n",
    "hourly_data['year_storm'] = hourly_data['year_storm'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1c0c9",
   "metadata": {},
   "source": [
    "Save the data to the data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming year_storm is created from the 'time' column\n",
    "hourly_data['year_storm'] = hourly_data['time'].dt.year\n",
    "hourly_data['year_storm'] = hourly_data['year_storm'].astype(int)\n",
    "\n",
    "# save the data\n",
    "hourly_data.to_netcdf(data_dir / 'SL_hourly_data.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389b26b",
   "metadata": {},
   "source": [
    "## Calculate and Plot Flood Frequency\n",
    "To analyze flood frequency, we will look for daily maximum sea levels for each day in our dataset, following {cite:t}`thompson_statistical_2019` and others. Then, we can group our data by year and month to visualize temporal patterns in daily SWL exceedance.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c0ede",
   "metadata": {},
   "source": [
    "```{glue:figure} histogram_fig\n",
    ":name: \"fig-histogram\"\n",
    ":figclass: margin\n",
    "\n",
    "Histogram of daily maximum water levels at the {glue:}`SL_Data_Wrangling.ipynb::station_group` tide gauges for the entire record from {glue:text}`startPORDateTime` to {glue:text}`endPORDateTime`, relative to {glue:}`SL_Data_Wrangling.ipynb::datumname`. The dashed red line indicates the chosen NOS threshold of {glue:text}`threshold_nos` cm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d727a68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the hourly data to daily maximum sea level\n",
    "SL_daily_max = hourly_data.resample(time='D').max()\n",
    "\n",
    "# remove time dimension from every variable except sea_level_MHHW\n",
    "SL_daily_max\n",
    "timevars = ['sea_level_MHHW','sea_level','flood_day','flood_hour','day','month','year','year_storm']\n",
    "\n",
    "#remove time from vars that aren't timevars\n",
    "for var in SL_daily_max.data_vars:\n",
    "    if var not in timevars:\n",
    "        SL_daily_max[var] = SL_daily_max[var].isel(time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new figure that is 15 x 5\n",
    "fig, ax = plt.subplots(sharex=True)\n",
    "plt.plot(SL_daily_max.time.values, SL_daily_max.sea_level_MHHW.values,label=SL_daily_max.station_name.values)\n",
    "plt.xlabel('Date (Calendar Year)')\n",
    "plt.ylabel('Sea Level (cm)')\n",
    "plt.title('Sea Level Daily Maximum Time Series')\n",
    "\n",
    "# add legend outside plot\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e12b8",
   "metadata": {},
   "source": [
    "```{margin} What's in a threshold?\n",
    "Different flood thresholds will produce different results, and different data sources have different thresholds. While certain locations may have unique characteristics that give rise to lower or higher thresholds, in practice using a different threshold on a regional or national scale can get a little finicky. The national weather service (NWS) impact thresholds are localized to the gauges themselves based on historical observations and used to issue coastal flood advisories for local areas. The national ocean service (NOS) minor flood thresholds more broadly applied, and correspond to  roughly 0.5m above the local diurnal tide range on the US mainland. For Hawaii (and Midway) they are set at ~30cm above MHHW.\n",
    "\n",
    "Exact formula given in {cite:t}`sweet_patterns_2018` is  1.04*[Local GT tidal datum] + 0.50 m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba777af",
   "metadata": {},
   "source": [
    "## Define flood thresholds\n",
    "\n",
    "\n",
    "The choice of water level exceendance threshold at a tide gauge (aka, the level at which minor flooding occurs once exceeded) can significant affect any calculated statistics of coastal flooding. Quite often, a still water level threshold may not directly correspond to what is seen in terms of impacts, such as flooded coastal streets or backed up storm drains. This is because still water levels only really tell you _some_ of the story. In reality, compounding effects like rain, upstream flooding, and waves, will change what qualifies as a flooded. Lacking calibration data of in situ flooding, however, these still water levels serve as a useful proxy. Think of it as \"setting the stage.\" In the following analysis we'll explore how to calculate 'flood days' and 'flood hours' based on the still water level recorded at a tide gauge. \n",
    "\n",
    "### Percentile-Based Threshold\n",
    "One technique of defining a flood threshold is through the use of percentiles. \n",
    "Here, we'll define it as the 95th percentile of the daily max water levels. Change at will!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54255c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS TO PLAY WITH PERCENTILE-BASED THREHOLDS\n",
    "percentile = 95\n",
    "thresholds = np.nanpercentile(SL_daily_max['sea_level_MHHW'], percentile, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecfb30c",
   "metadata": {},
   "source": [
    "Now we'll plot it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e44a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(SL_daily_max['station_name']), 1, sharex=True, figsize=(4, 4))\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.hist(hourly_data['sea_level_MHHW'][i,:], bins=100, density=True, label='Sea Level Data')\n",
    "    ax.axvline(thresholds[i], color='r', linestyle='dashed', linewidth=2, label=f'{percentile}th Percentile')\n",
    "\n",
    "    ax.text(0.05, 0.8, hourly_data['station_name'][i].values, transform=ax.transAxes, fontsize=8, verticalalignment='top')\n",
    "    ax.text(0.95,0.5, f'{thresholds[i]:.1f}', color='r', fontsize=8, transform=ax.transAxes, horizontalalignment='right')\n",
    "\n",
    "    # remove y-axis label for all\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "axs[-1].set_xlabel('Sea Level (cm)')\n",
    "plt.subplots_adjust(hspace=0)\n",
    "\n",
    "axs[0].set_title(f'Sea level histogram with \\n{percentile}th Percentile Thresholds')\n",
    "\n",
    "# set xlim to -150,75\n",
    "plt.xlim(-100,50)\n",
    "\n",
    "\n",
    "#make threshold for (XX))th percentile on each gauge\n",
    "thresholds = np.nanpercentile(SL_daily_max['sea_level_MHHW'], percentile, axis=0)\n",
    "glue(\"threshold_percentile\",percentile,display=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce9a60",
   "metadata": {},
   "source": [
    "### Impact-based thresholds\n",
    "Now we'll set the threshold to align with NOAA CO-OPS's API, which has both the NOS and NWS thresholds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi/'\n",
    "\n",
    "threshold_nws = {}\n",
    "threshold_nos = {}\n",
    "for station in station_ids:\n",
    "    thresholds_url = url + f'stations/{station}/floodlevels.json?units=metric'\n",
    "    thresholdsNOAA = requests.get(thresholds_url).json()\n",
    "    try:\n",
    "        nws_minor = thresholdsNOAA['nws_minor']\n",
    "        nos_minor = thresholdsNOAA['nos_minor']\n",
    "        mhhw = rsl['MHHW'].sel(station_id=station).item()\n",
    "        if nws_minor is not None:\n",
    "            threshold_nws[station] = nws_minor - mhhw\n",
    "        if nos_minor is not None:\n",
    "            threshold_nos[station] = nos_minor - mhhw\n",
    "    except KeyError:\n",
    "        threshold_nws[station] = None  # or np.nan\n",
    "        threshold_nos[station] = None  # or np.nan\n",
    "\n",
    "# multiply by 100 to convert to cm\n",
    "threshold_nws = {k: v * 100 for k, v in threshold_nws.items() if v is not None}\n",
    "threshold_nos = {k: v * 100 for k, v in threshold_nos.items() if v is not None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64bdf0b",
   "metadata": {},
   "source": [
    "Note that for the Hawaiian Island region, the NOS minor threshold is 30.4 cm for all stations, whereas the NWS minor threshold varies. There is no NWS threshold for Midway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pdf of the data with NOS minor threshold\n",
    "fig, axs = plt.subplots(len(hourly_data['station_name']), 1, sharex=True)\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.hist(hourly_data['sea_level_MHHW'][i,:], bins=100, density=True, label='Sea Level Data')\n",
    "    ax.axvline(threshold_nos[station_ids[i]], color='r', linestyle='--', label='Threshold: {:.4f} cm'.format(threshold_nos[station_ids[i]]))\n",
    "\n",
    "    ax.text(0.05, 0.8, hourly_data['station_name'][i].values, transform=ax.transAxes, fontsize=8, verticalalignment='top')\n",
    "\n",
    "    # remove y-axis label for all\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "axs[-1].set_xlabel('Sea Level (cm)')\n",
    "plt.subplots_adjust(hspace=0)\n",
    "\n",
    "\n",
    "ax.set_xlabel('Sea Level (cm)')\n",
    "fig.text(0.04, 0.5, 'Probability Density', va='center', rotation='vertical')\n",
    "# make the title two lines\n",
    "axs[0].set_title('Sea Level Histogram\\nwith Defined Threshold')\n",
    "\n",
    "\n",
    "# add label to dashed line\n",
    "# get value of middle of y-axis for label placement\n",
    "ymin, ymax = ax.get_ylim()\n",
    "yrange = ymax - ymin\n",
    "y_middle = ymin + yrange/2\n",
    "\n",
    "axs[0].text(threshold_nos[station_ids[0]]+10, y_middle, '{:.1f} cm'.format(threshold_nos[station_ids[0]]), rotation=0, va='center', ha='left', color='r')\n",
    "glue(\"histogram_fig\", fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d4afc",
   "metadata": {},
   "source": [
    "## Define 'flood days' and 'flood hours'\n",
    "\n",
    "These are the days and hours in which the water levels surpass the given flood threshold. For the following analysis we'll stick with the 95th percentile values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d181e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_day = (SL_daily_max.sea_level_MHHW.values > thresholds)\n",
    "\n",
    "flood_hour = (hourly_data.sea_level_MHHW.values > thresholds[:,None]) # need to account for (stations,time) dimensions\n",
    "flood_hour\n",
    "\n",
    "# flip the array so that the first dimension is time\n",
    "flood_hour = np.transpose(flood_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe33626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'flood_day' variable to the daily_max dataset\n",
    "SL_daily_max['flood_day'] = (('time', 'station_id'), flood_day.data)\n",
    "\n",
    "\n",
    "# Assign metadata variables from hourly_data to SL_daily_max\n",
    "meta_vars = ['lat', 'lon', 'station_name', 'station_country', 'MSL', 'MHHW']\n",
    "for var in meta_vars:\n",
    "    SL_daily_max[var] = hourly_data[var]\n",
    "\n",
    "# put the threshold into the daily_max dataset\n",
    "SL_daily_max['threshold'] = (('station_id'), thresholds.data)\n",
    "SL_daily_max['threshold'].attrs['units'] = 'cm'\n",
    "SL_daily_max['threshold'].attrs['long_name'] = f'{percentile}th Percentile Threshold'\n",
    "\n",
    "SL_daily_max['flood_day'].attrs['units'] = 'days'\n",
    "SL_daily_max['flood_day'].attrs['long_name'] = f'Flood Day above {percentile}th Percentile'\n",
    "\n",
    "# save SL_daily_max to netcdf\n",
    "SL_daily_max.to_netcdf(data_dir / 'SL_daily_max.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5979ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for flood days again now that 'flood_day' has been correctly added\n",
    "flood_days_data = SL_daily_max.where(SL_daily_max.flood_day, drop=True)\n",
    "\n",
    "# Initialize an empty DataFrame again for the loop\n",
    "flood_days_per_year = pd.DataFrame()\n",
    "\n",
    "for station_id in SL_daily_max.station_id.values:\n",
    "    # Extracting flood days for each station_id\n",
    "    flood_days_df = flood_days_data.sel(station_id=station_id).dropna(dim='time', how='all').to_dataframe().reset_index()\n",
    "\n",
    "    # Extract year from the 'time' column and count flood days\n",
    "    flood_days_df['year'] = flood_days_df['time'].dt.year\n",
    "    flood_days_count = flood_days_df.groupby('year').size().reset_index(name=station_id)\n",
    "\n",
    "    # Merge this count with the main DataFrame\n",
    "    if flood_days_per_year.empty:\n",
    "        flood_days_per_year = flood_days_count.set_index('year')\n",
    "    else:\n",
    "        flood_days_per_year = flood_days_per_year.join(flood_days_count.set_index('year'), how='outer')\n",
    "\n",
    "# Replace missing values with 0\n",
    "flood_days_per_year.fillna(0, inplace=True)\n",
    "flood_days_per_year = xr.DataArray(flood_days_per_year.values, dims=('year', 'station_id'), coords={'year': flood_days_per_year.index, 'station_id': flood_days_per_year.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7539bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'flood_day' variable to the dataset\n",
    "hourly_data['flood_hour'] = (('time', 'station_id'), flood_hour.data)\n",
    "# Filtering for flood days again now that 'flood_hour' has been correctly added\n",
    "flood_hours_data = hourly_data.where(hourly_data.flood_hour, drop=True)\n",
    "# Initialize an empty DataFrame again for the loop\n",
    "flood_hours_per_year = pd.DataFrame()\n",
    "for station_id in hourly_data.station_id.values:\n",
    "    # Extracting flood days for each station_id\n",
    "    flood_hours_df = flood_hours_data.sel(station_id=station_id).dropna(dim='time', how='all').to_dataframe().reset_index()\n",
    "\n",
    "    # Extract year from the 'time' column and count flood days\n",
    "    flood_hours_df['year'] = flood_hours_df['time'].dt.year\n",
    "    flood_hours_count = flood_hours_df.groupby('year').size().reset_index(name=station_id)\n",
    "\n",
    "    # Merge this count with the main DataFrame\n",
    "    if flood_hours_per_year.empty:\n",
    "        flood_hours_per_year = flood_hours_count.set_index('year')\n",
    "    else:\n",
    "        flood_hours_per_year = flood_hours_per_year.join(flood_hours_count.set_index('year'), how='outer')\n",
    "# Replace missing values with 0\n",
    "flood_hours_per_year.fillna(0, inplace=True)\n",
    "flood_hours_per_year = xr.DataArray(flood_hours_per_year.values, dims=('year', 'station_id'), coords={'year': flood_hours_per_year.index, 'station_id': flood_hours_per_year.columns})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03575b59",
   "metadata": {},
   "source": [
    "Make a new xarray dataset with attributes from SL_daily_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85044aba",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#make new xarray dataset with attributes from SL_daily_max\n",
    "ds = xr.Dataset()\n",
    "\n",
    "ds['flood_hours_per_year'] = (('year', 'station_id'), flood_hours_per_year.values)\n",
    "\n",
    "ds['flood_days_per_year'] = (('year', 'station_id'), flood_days_per_year.values)\n",
    "\n",
    "ds['flood_days_per_year'].attrs = {'long_name': 'Number of flood days per year', 'units': 'days'}\n",
    "ds['flood_hours_per_year'].attrs = {'long_name': 'Number of flood hours per year', 'units': 'hours'}\n",
    "\n",
    "# set year and station_id as coordinates\n",
    "ds['year'] = flood_hours_per_year.year\n",
    "ds['station_id'] = flood_hours_per_year.station_id\n",
    "\n",
    "ds['year'].attrs = {'long_name': 'Year', 'units': 'calendar year'}\n",
    "ds['station_id'].attrs = {'long_name': 'Station ID', 'units': '1'}\n",
    "\n",
    "\n",
    "ds['lat'] = SL_daily_max['lat']\n",
    "ds['lat'].attrs = {'long_name': 'Latitude', 'units': 'degrees_north'}\n",
    "\n",
    "ds['lon'] = SL_daily_max['lon']\n",
    "ds['lon'].attrs = {'long_name': 'Longitude', 'units': 'degrees_east'}\n",
    "\n",
    "ds['station_name'] = SL_daily_max['station_name']\n",
    "ds['station_name'].attrs = {'long_name': 'Station Name', 'units': '1'}\n",
    "\n",
    "ds['station_country'] = SL_daily_max['station_country']\n",
    "ds['station_country'].attrs = {'long_name': 'Station Country', 'units': '1'}\n",
    "\n",
    "\n",
    "# Take a peek at the data:\n",
    "ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find minimum value in flood_days_per_year\n",
    "min_flood_days = flood_days_per_year.min().min()\n",
    "max_flood_days = flood_days_per_year.max().max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d3b9e",
   "metadata": {},
   "source": [
    "### Plot Flood Frequency Counts \n",
    "\n",
    "The flood frequency counts are defined as the number of time periods that exceed a given threshold within a year. This plot follows {cite:t}`center_for_operational_oceanographic_products_and_services_us_sea_2014`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the heatmap palette to improve readability\n",
    "adjusted_heatmap_palette = sns.color_palette(\"YlOrRd\", as_cmap=True)\n",
    "\n",
    "station_id = '1617760'\n",
    "df = ds['flood_days_per_year'].sel(station_id=station_id).to_dataframe().reset_index()\n",
    "\n",
    "norm = plt.Normalize(df['flood_days_per_year'].min(), df['flood_days_per_year'].max())\n",
    "colors = [adjusted_heatmap_palette(norm(value)) for value in df['flood_days_per_year']]\n",
    "\n",
    "threshold = round(thresholds[station_id==ds['station_id'].values].item(), 2)\n",
    "glue(\"threshold\",threshold,display=False)\n",
    "\n",
    "# Plotting with the adjusted settings\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x='year', \n",
    "    y='flood_days_per_year', \n",
    "    hue='year', \n",
    "    data=df,\n",
    "    palette=colors,\n",
    "    dodge=False,\n",
    "    legend=False\n",
    ")\n",
    "ax.set_xticks(range(0, len(df), 5))  # Setting x-ticks to show every 5th year\n",
    "year_ticks = df['year'][::5].astype(int)  # Selecting every 5th year for the x-axis\n",
    "ax.set_xticklabels(year_ticks, rotation=45)\n",
    "\n",
    "# Adding a light gray grid\n",
    "station_name = ds['station_name'].sel(station_id=station_id).values.item()\n",
    "\n",
    "ax.text(0.05, 0.9, station_name + '\\nabove ' + str(threshold) + ' cm threshold (' + str(percentile) + 'th percentile)', ha='left', va='center', transform=ax.transAxes)\n",
    "\n",
    "#save the figure\n",
    "figname = 'SL_FloodFrequency_threshold_counts_DAYS_'+station_name+'.png'\n",
    "fig.savefig(output_dir / figname, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49da2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the heatmap palette to improve readability\n",
    "adjusted_heatmap_palette = sns.color_palette(\"YlOrRd\", as_cmap=True)\n",
    "\n",
    "df = ds['flood_hours_per_year'].sel(station_id=station_id).to_dataframe().reset_index()\n",
    "\n",
    "norm = plt.Normalize(df['flood_hours_per_year'].min(), df['flood_hours_per_year'].max())\n",
    "colors = [adjusted_heatmap_palette(norm(value)) for value in df['flood_hours_per_year']]\n",
    "\n",
    "\n",
    "# Plotting with the adjusted settings\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x='year', \n",
    "    y='flood_hours_per_year', \n",
    "    hue='year', \n",
    "    data=df,\n",
    "    palette=colors,\n",
    "    dodge=False,\n",
    "    legend=False\n",
    ")\n",
    "ax.set_xticks(range(0, len(df), 5))  # Setting x-ticks to show every 5th year\n",
    "year_ticks = df['year'][::5].astype(int)  # Selecting every 5th year for the x-axis\n",
    "ax.set_xticklabels(year_ticks, rotation=45)\n",
    "\n",
    "# Adding a light gray grid\n",
    "station_name = ds['station_name'].sel(station_id=station_id).values.item()\n",
    "\n",
    "ax.text(0.05, 0.9, station_name + '\\nabove ' + str(threshold) + ' cm threshold (' + str(percentile) + 'th percentile)', ha='left', va='center', transform=ax.transAxes)\n",
    "\n",
    "#save the figure\n",
    "figname = 'SL_FloodFrequency_threshold_counts_HOURS_'+station_name+'.png'\n",
    "fig.savefig(output_dir / figname, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ad661",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Adjusting the heatmap palette to improve readability\n",
    "adjusted_heatmap_palette = sns.color_palette(\"YlOrRd\", as_cmap=True)\n",
    "norm = plt.Normalize(flood_days_per_year.min().min(), flood_days_per_year.max().max())\n",
    "colors = [adjusted_heatmap_palette(norm(value)) for value in flood_days_per_year.values]\n",
    "\n",
    "# do a pcolormesh plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.pcolormesh(ds['year'], ds['station_name'], ds['flood_days_per_year'].T, cmap=adjusted_heatmap_palette, norm=norm)\n",
    "\n",
    "# add a colorbar\n",
    "cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # adjust the position and size of the colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=adjusted_heatmap_palette, norm=norm)\n",
    "plt.colorbar(sm, cax=cax,label= 'Number of Flood Days')\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_title('Number of Flood Days Per Year')             \n",
    "\n",
    "glue(\"threshold_counts_days_fig\", fig, display=False)\n",
    "\n",
    "# save the figure\n",
    "fig.savefig(output_dir / 'SL_FloodFrequency_threshold_counts_days.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e57a85",
   "metadata": {},
   "source": [
    "```{glue:figure} threshold_counts_days_fig\n",
    ":name: \"fig-threshold_counts\"\n",
    "\n",
    "Flood frequency counts above the {glue:text}`threshold_percentile:.0f`th percentile threshold per year at {glue:}`SL_Data_Wrangling.ipynb::station_group` tide gauges from {glue:text}`startPORDateTime` to {glue:text}`endPORDateTime`. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f3a02",
   "metadata": {},
   "source": [
    "### Plot Flood Duration\n",
    "\n",
    "This next plot examines the average duration of flooding events as defined by the threshold. \n",
    "I have a few issues with this plot being \"duration,\" as it's just counts of hours above the threshold. These hours need not be continuous...which to me is what duration is all about. Anyway, we carry on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4bb38a",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Adjusting the heatmap palette to improve readability\n",
    "adjusted_heatmap_palette = sns.color_palette(\"YlOrRd\", as_cmap=True)\n",
    "norm = plt.Normalize(flood_hours_per_year.min().min(), flood_hours_per_year.max().max())\n",
    "colors = [adjusted_heatmap_palette(norm(value)) for value in flood_hours_per_year.values]\n",
    "\n",
    "# do a pcolormesh plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.pcolormesh(ds['year'], ds['station_name'], ds['flood_hours_per_year'].T, cmap=adjusted_heatmap_palette, norm=norm)\n",
    "\n",
    "# add a colorbar\n",
    "cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # adjust the position and size of the colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=adjusted_heatmap_palette, norm=norm)\n",
    "plt.colorbar(sm, cax=cax,label= 'Number of Flood Hours')\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_title('Number of Flood Hours Per Year')             \n",
    "# ax.set_yticklabels(SL_daily_max['station_name'].values[0])\n",
    "\n",
    "# save the figure\n",
    "fig.savefig(output_dir / 'SL_FloodFrequency_threshold_counts_hours.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "glue(\"duration_fig\", fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d967f",
   "metadata": {},
   "source": [
    "```{glue:figure} duration_fig\n",
    ":name: \"fig-duration\"\n",
    "\n",
    "Average flood duration in hours above the {glue:text}`threshold_percentile:.0f`th percentile flood threshold per year for {glue:}`SL_Data_Wrangling.ipynb::station_group` region tide gauges from {glue:text}`startPORDateTime` to {glue:text}`endPORDateTime`. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b522e",
   "metadata": {},
   "source": [
    "## Calculate the change over time\n",
    "\n",
    "Next we'll calculate the change in flood days and hours over the POR at the tide station/s, for both Frequency and Duration.\n",
    "\n",
    "The next code cell fits a trend line to the flood days per year data and calculates the trend line.\n",
    "The slope of the trend line is the change in flood days per year. The same process is repeated for flood hours per year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d7275",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_flood_trend(ds, timescale = 'days'):\n",
    "\n",
    "    if timescale == 'days':\n",
    "        dsvar = 'flood_days_per_year'\n",
    "    elif timescale == 'hours':\n",
    "        dsvar = 'flood_hours_per_year'\n",
    "\n",
    "    slopes = []\n",
    "    intercepts = []\n",
    "    rate_changes = []\n",
    "    \n",
    "    #trends is an empty array the length of ds['year'] and the number of records\n",
    "    trends = np.empty((len(ds['year']), len(ds['station_id'])))\n",
    "\n",
    "    for station_id in range(len(ds['station_id'])):\n",
    "        slope, intercept, _, _, _ = stats.linregress(ds['year'].values, ds[dsvar].isel(station_id=station_id).values)\n",
    "        trend = intercept + slope * ds['year']\n",
    "        # Use slope as the indicator of change (units: days/year or hours/year)\n",
    "        rate_change = slope\n",
    "        slopes.append(slope)\n",
    "        intercepts.append(intercept)\n",
    "        rate_changes.append(rate_change)\n",
    "        trends[:, station_id] = trend\n",
    "\n",
    "    return slopes, intercepts, rate_changes, trends\n",
    "\n",
    "slopes, intercepts, rate_changes, trends = calculate_flood_trend(ds, 'days')\n",
    "\n",
    "#add to dataset\n",
    "ds['slope_days'] = (('station_id'), slopes)\n",
    "ds['intercept_days'] = (('station_id'), intercepts)\n",
    "ds['rate_change_days'] = (('station_id'), np.squeeze(rate_changes))\n",
    "ds['trend_days'] = (('year', 'station_id'), trends)\n",
    "\n",
    "ds['slope_days'].attrs = {'long_name': 'Slope of the trend line', 'units': 'days/year'}\n",
    "ds['intercept_days'].attrs = {'long_name': 'Intercept of the trend line', 'units': 'days'}\n",
    "ds['rate_change_days'].attrs = {'long_name': 'Rate of change in flood days per year', 'units': 'days/year'}\n",
    "ds['trend_days'].attrs = {'long_name': 'Trend line of flood days per year', 'units': 'days'}\n",
    "\n",
    "\n",
    "slopes, intercepts, rate_changes, trends = calculate_flood_trend(ds, 'hours')\n",
    "\n",
    "#add to dataset\n",
    "ds['slope_hours'] = (('station_id'), slopes)\n",
    "ds['intercept_hours'] = (('station_id'), intercepts)\n",
    "ds['rate_change_hours'] = (('station_id'), np.squeeze(rate_changes))\n",
    "ds['trend_hours'] = (('year', 'station_id'), trends)\n",
    "\n",
    "ds['slope_hours'].attrs = {'long_name': 'Slope of the trend line', 'units': 'hours/year'}\n",
    "ds['intercept_hours'].attrs = {'long_name': 'Intercept of the trend line', 'units': 'hours'}\n",
    "ds['rate_change_hours'].attrs = {'long_name': 'Rate of change in flood hours per year', 'units': 'hours/year'}\n",
    "ds['trend_hours'].attrs = {'long_name': 'Trend line of flood hours per year', 'units': 'hours'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b65bb",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "summary_stats_all = []\n",
    "\n",
    "for station_idx in range(len(ds['station_id'])):\n",
    "    station_name = ds['station_name'][station_idx].item()\n",
    "    station_id = ds['station_id'][station_idx].item()\n",
    "    stats = {\n",
    "        'Station Name': station_name,\n",
    "        'Station ID': station_id,\n",
    "        'Threshold (cm)': round(thresholds[station_idx],1),\n",
    "        'Total Flood Days': int(ds['flood_days_per_year'][:, station_idx].sum()),\n",
    "        'Average Flood Days per Year': round(float(ds['flood_days_per_year'][:, station_idx].mean()), 1),\n",
    "        'Max Flood Days in a Single Year': int(ds['flood_days_per_year'][:, station_idx].max()),\n",
    "        'Year of Max Flood Days': int(ds['year'][ds['flood_days_per_year'][:, station_idx].argmax()].item()),\n",
    "        'Total Flood Hours': int(ds['flood_hours_per_year'][:, station_idx].sum()),\n",
    "        'Average Flood Hours per Year': round(float(ds['flood_hours_per_year'][:, station_idx].mean()), 1),\n",
    "        'Max Flood Hours in a Single Year': int(ds['flood_hours_per_year'][:, station_idx].max()),\n",
    "        'Year of Max Flood Hours': int(ds['year'][ds['flood_hours_per_year'][:, station_idx].argmax()].item()),\n",
    "        'Change in Flood Days per Year': round(float(ds['rate_change_days'][station_idx]), 1),\n",
    "        'Change in Flood Hours per Year': round(float(ds['rate_change_hours'][station_idx]), 1  )\n",
    "    }\n",
    "    summary_stats_all.append(stats)\n",
    "\n",
    "summary_stats_df_all = pd.DataFrame(summary_stats_all)\n",
    "summary_stats_df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f221d",
   "metadata": {},
   "source": [
    "Make it pretty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e645f1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Turn into a pretty table with great tables\n",
    "from great_tables import GT, html, style, loc\n",
    "\n",
    "PORstring = f'{POR_start.strftime(\"%Y\")}-{POR_end.strftime(\"%Y\")}'\n",
    "floodDaysCols = [col for col in summary_stats_df_all.columns if 'Days' in col]\n",
    "floodHoursCols = [col for col in summary_stats_df_all.columns if 'Hours' in col]\n",
    "\n",
    "cols_label_dict = {\n",
    "    col: html(col.replace(\"Flood Days\", \"\").replace(\"Flood Hours\", \"\").strip())\n",
    "    for col in summary_stats_df_all.columns\n",
    "}\n",
    "\n",
    "cols_label_dict['Change in Flood Days per Year'] = html('Change <br> (days/yr)')\n",
    "cols_label_dict['Change in Flood Hours per Year'] = html('Change <br> (hours/yr)')\n",
    "cols_label_dict['Max Flood Days in a Single Year'] = html('Yearly <br>Max')\n",
    "cols_label_dict['Max Flood Hours in a Single Year'] = html('Yearly <br>Max')\n",
    "cols_label_dict['Average Flood Days per Year'] = html('Average <br>per Year')\n",
    "cols_label_dict['Average Flood Hours per Year'] = html('Average <br>per Year')\n",
    "cols_label_dict['Year of Max Flood Days'] = html('Year of <br>Max')\n",
    "cols_label_dict['Year of Max Flood Hours'] = html('Year of <br>Max')\n",
    "cols_label_dict['Station ID'] = html('ID')\n",
    "\n",
    "# Create a Table object\n",
    "table = (\n",
    "    GT(summary_stats_df_all)\n",
    "    .tab_options(table_font_size=\"13px\")\n",
    "    .cols_width(cases={\"Station Name\": \"120px\"})\n",
    "    .cols_label(**cols_label_dict)\n",
    "    .cols_align(align=\"left\", columns = ['Threshold (cm)','Station ID'])\n",
    "    .tab_spanner(\n",
    "        label=\"Flood Days\", columns=floodDaysCols)\n",
    "    .tab_spanner(\n",
    "        label=\"Flood Hours\", columns=floodHoursCols)\n",
    "    .tab_header(\n",
    "        subtitle='Hawaiian Island Region',title='Minor Flood Statistics')        \n",
    "    .tab_source_note(\n",
    "         source_note = 'Data: NOAA CO-OPS Hourly Water Levels (' + PORstring + '), Threshold: ' + str(percentile) +'th percentile of daily max water level')\n",
    ")\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the table as an HTML file\n",
    "savepath = output_dir / 'minor_flood_statistics.html'\n",
    "GT.write_raw_html(table, savepath)\n",
    "# Save the table as a png file\n",
    "savepath = output_dir / 'minor_flood_statistics.png'\n",
    "table.save(savepath, scale=2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe54161",
   "metadata": {},
   "source": [
    "### Plot time series of all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556d8b1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1, sharex=True, figsize=(6, 6))\n",
    "\n",
    "# Set the color for each station\n",
    "colors = sns.color_palette('Dark2', n_colors=len(ds['station_id']))\n",
    "\n",
    "# Plot the data for each station with the same color\n",
    "for i, station_id in enumerate(ds['station_id']):\n",
    "    axs[0].plot(ds['year'], ds['flood_days_per_year'].sel(station_id=station_id), label=None, color=colors[i], linestyle=':')\n",
    "    axs[1].plot(ds['year'], ds['flood_hours_per_year'].sel(station_id=station_id), label=None, color=colors[i], linestyle=':')\n",
    "\n",
    "    # Plot the trend lines\n",
    "    axs[0].plot(ds['year'], ds['trend_days'].sel(station_id=station_id), label=None , color=colors[i])\n",
    "    axs[1].plot(ds['year'], ds['trend_hours'].sel(station_id=station_id), label=ds['station_name'].sel(station_id=station_id).values, color=colors[i])\n",
    "\n",
    "# Set the labels and tick labels\n",
    "axs[0].set_ylabel('Flood Days')\n",
    "# axs[0].set_xticklabels([])\n",
    "\n",
    "axs[1].set_xlabel('Year')\n",
    "axs[1].set_ylabel('Flood Hours')\n",
    "\n",
    "# add a legend \n",
    "axs[1].legend(ncol=3, columnspacing=0.5, bbox_to_anchor=(0, 2.25), loc='upper left',fontsize=5)\n",
    "\n",
    "#color the legend text the same as the lines and then remove the lines\n",
    "for i, station_id in enumerate(ds['station_id']):\n",
    "    axs[1].get_legend().get_texts()[i].set_color(colors[i])\n",
    "    axs[1].get_legend().get_texts()[i].set_fontsize('small')\n",
    "    # axs[1].get_legend().get_lines()[i].set_linewidth(0)\n",
    "\n",
    "#remove box around legend\n",
    "axs[1].get_legend().get_frame().set_linewidth(0.0)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63ac6e",
   "metadata": {},
   "source": [
    "### Create a Simple Table\n",
    "Now we'll generate a table with this information, which will be saved as a .csv in the output directory specified at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713d716",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# make a dataframe with the rate change in flood days and hours per year, with given threshold\n",
    "rate_change_df = pd.DataFrame({'rate_change_days': ds['rate_change_days'].values, 'rate_change_hours': ds['rate_change_hours'].values, 'threshold': thresholds.round(2)})\n",
    "rate_change_df = rate_change_df.round(2)\n",
    "# add the station name and country\n",
    "rate_change_df['station'] = ds['station_name'].values\n",
    "rate_change_df['country'] = ds['station_country'].values\n",
    "\n",
    "# reorder the columns\n",
    "rate_change_df = rate_change_df[['station', 'threshold', 'rate_change_days', 'rate_change_hours']]\n",
    "\n",
    "# Define your attributes\n",
    "attributes = {\n",
    "    'station': 'Station name',\n",
    "    'threshold': 'Threshold in cm above MHHW',\n",
    "    'rate_change_days': 'Rate of change in flood days per year',\n",
    "    'rate_change_hours': 'Rate of change in flood hours per year'\n",
    "}\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(output_dir / 'SL_FloodFrequency_rate_change.csv', 'w') as f:\n",
    "    # Write the attributes as comments\n",
    "    for column, attribute in attributes.items():\n",
    "        f.write(f'# {column}: {attribute}\\n')\n",
    "\n",
    "    # Write the DataFrame to the file\n",
    "    rate_change_df.to_csv(f, index=False)\n",
    "\n",
    "rate_change_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472dc062",
   "metadata": {},
   "source": [
    "And now we'll make that pretty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d1658",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#make a pretty pdf of the table with great_tables\n",
    "from great_tables import GT, html\n",
    "thresholdstr = str(percentile) + 'th Percentile'\n",
    "# Create a Table object\n",
    "table = (\n",
    "    GT(rate_change_df[[\"station\", \"threshold\", \"rate_change_days\", \"rate_change_hours\"]])\n",
    "    .cols_label(\n",
    "        station=html('Station'),\n",
    "        threshold=html(f'Threshold<br>{thresholdstr}<br>(cm above MHHW)'),\n",
    "        rate_change_days=html('Rate of Change<br>in Flood Days<br>(days/yr)'),\n",
    "        rate_change_hours=html('Rate of Change<br>in Flood Hours<br>(hours/yr)')\n",
    "    )\n",
    "    .fmt_number(\n",
    "        columns=[\"rate_change_days\", \"rate_change_hours\"], decimals=1\n",
    "    )\n",
    "    .tab_header(\n",
    "            title='Flood Frequency Analysis', subtitle='Hawaiian Island Region')\n",
    "        .tab_source_note(\n",
    "            source_note=html(\n",
    "                f\"Data: NOAA CO-OPS Hourly Water Levels. Threshold is calculated with data from {POR_start.strftime('%Y-%m-%d')} to {POR_end.strftime('%Y-%m-%d')} (relative to MHHW)\"\n",
    "            )\n",
    "        )\n",
    ")\n",
    "\n",
    "table\n",
    "output_path = output_dir / f'flood_frequency_table.png'\n",
    "table.save(str(output_path), scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2008302",
   "metadata": {},
   "source": [
    "Finally, let's compare thresholds from the different sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d03dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe of thresholds from thresholds, thresholds_NOS, and thresholds_NWS\n",
    "thresholds_dict = {station: round(threshold, 2) for station, threshold in zip(hourly_data['station_id'].values, thresholds)}\n",
    "\n",
    "station_info = []\n",
    "for station in hourly_data['station_id'].values:\n",
    "    nos_val = threshold_nos.get(station, None)\n",
    "    nws_val = threshold_nws.get(station, None)\n",
    "    threshold_val = thresholds_dict.get(station, None)\n",
    "    station_info.append({\n",
    "        'station_id': station,\n",
    "        'station_name': hourly_data['station_name'].sel(station_id=station).item(),\n",
    "        'threshold_95th_percentile_cm': threshold_val,\n",
    "        'threshold_NOS_minor_cm': round(nos_val, 2) if nos_val is not None else None,\n",
    "        'threshold_NWS_minor_cm': round(nws_val, 2) if nws_val is not None else None\n",
    "    })\n",
    "thresholds_df = pd.DataFrame(station_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d4dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf02c0",
   "metadata": {},
   "source": [
    "## Plot a Map\n",
    "And here is our final plotting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae78b0",
   "metadata": {
    "tags": [
     "hide-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "xlims = [-185, -152]\n",
    "ylims = [14, 40]\n",
    "\n",
    "crs = ccrs.PlateCarree()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 6), subplot_kw={'projection': crs})\n",
    "\n",
    "rsl['lon_west'] = -(360 - rsl['lon'])\n",
    "\n",
    "# Colormap setup\n",
    "cmap_days = plt.get_cmap('YlOrRd')\n",
    "cmap_hours = plt.get_cmap('YlOrRd')\n",
    "norm_days = plt.Normalize(ds['slope_days'].min().values, ds['slope_days'].max().values)\n",
    "norm_hours = plt.Normalize(ds['slope_hours'].min().values, ds['slope_hours'].max().values)\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_xlim(xlims)\n",
    "    ax.set_ylim(ylims)\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.LAND, color='lightgrey')\n",
    "    ax.text(0.95, 0.95, f'({chr(97 + i)})',\n",
    "            horizontalalignment='right', verticalalignment='top', transform=ax.transAxes,\n",
    "            fontsize=16)\n",
    "    # ax.add_feature(cfeature.OCEAN, color='lightblue')\n",
    "    gl = ax.gridlines(draw_labels=False, linestyle=':', color='black',\n",
    "                      alpha=0.2, xlocs=ax.get_xticks(), ylocs=ax.get_yticks())\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    if ax == axs[1]:\n",
    "        gl.left_labels = False\n",
    "\n",
    "# Plot colored dots for flood days slope\n",
    "sc_days = axs[0].scatter(\n",
    "    rsl['lon_west'], rsl['lat'],\n",
    "    c=ds['slope_days'].values, alpha=0.7,\n",
    "    cmap=cmap_days, norm=norm_days, s=120, edgecolor='black', transform=crs, zorder=3\n",
    ")\n",
    "sc_hours = axs[1].scatter(\n",
    "    rsl['lon_west'], rsl['lat'],\n",
    "    c=ds['slope_hours'].values, alpha=0.7,\n",
    "    cmap=cmap_hours, norm=norm_hours, s=120, edgecolor='black', transform=crs, zorder=3\n",
    ")\n",
    "\n",
    "axs[0].set_title('Change in Flood Days')\n",
    "axs[1].set_title('Change in Flood Hours')\n",
    "\n",
    "# Add colorbars with custom axes to match figure height and add padding between them\n",
    "axs_bottom = axs[0].get_position().y0\n",
    "axs0_x0 = axs[0].get_position().x0\n",
    "axs0_width = axs[0].get_position().width * 0.9  # slightly shrink width\n",
    "axs1_x0 = axs[1].get_position().x0\n",
    "axs1_width = axs[1].get_position().width * 0.9  # slightly shrink width\n",
    "\n",
    "padding = 0.1  # horizontal gap between colorbars\n",
    "\n",
    "cbar_ax_days = fig.add_axes([axs0_x0+padding, 0.6, axs0_width-padding, 0.02])  # [left, bottom, width, height]\n",
    "cbar_ax_hours = fig.add_axes([axs1_x0 + padding, 0.6, axs1_width-padding, 0.02])\n",
    "\n",
    "\n",
    "cb_days = fig.colorbar(sc_days, cax=cbar_ax_days, label='(days/year)', orientation='horizontal')\n",
    "cb_hours = fig.colorbar(sc_hours, cax=cbar_ax_hours, label='(hours/year)', orientation='horizontal')\n",
    "\n",
    "cb_days.ax.xaxis.set_ticks_position('top')\n",
    "cb_days.ax.xaxis.set_label_position('top')\n",
    "\n",
    "cb_hours.ax.xaxis.set_ticks_position('top')\n",
    "cb_hours.ax.xaxis.set_label_position('top')\n",
    "\n",
    "glue(\"mag_fig\", fig, display=False)\n",
    "output_file_path = output_dir / 'SL_FloodFrequency_map.png'\n",
    "fig.savefig(output_file_path, dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d925b3",
   "metadata": {},
   "source": [
    "```{glue:figure} mag_fig\n",
    ":name: \"mag_fig\"\n",
    "\n",
    "Map of the rate of change in average flood (a) days and (b) hours per year above the {glue:text}`threshold_percentile:.0f`th threshold of water levels at at {glue:}`SL_Data_Wrangling.ipynb::station_group` tide gauges from {glue:text}`startPORDateTime` to {glue:text}`endPORDateTime`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32719bef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":style: plain\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLI311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
