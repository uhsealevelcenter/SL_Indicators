{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "In this notebook we'll obtain all of our data, arrange it how we want, and store it in the proper directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.ipynb\n",
    "import copernicusmarine\n",
    "\n",
    "tg_nc_dir = data_dir / 'tide_gauge_nc'\n",
    "\n",
    "# Check if the directories exist, if not create them\n",
    "for d in [data_dir, output_dir, tg_nc_dir]:\n",
    "    if not d.exists():\n",
    "        d.mkdir()\n",
    "\n",
    "# make a climate_indices directory if it doesn't exist\n",
    "climate_indices_dir = data_dir / 'climate_indices'\n",
    "if not climate_indices_dir.exists():\n",
    "    climate_indices_dir.mkdir()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Tide Gauge Data\n",
    "\n",
    "We are interested in getting tide gauge and alitmetry data for the Hawaiian Islands (and surrounds) for 1993 through 2022.\n",
    "Let's first establish where the tide gauges are by looking at the tide gauge dataset. We'll retrieve tide gauge data from the UHSLC (University of Hawaii Sea Level Center) fast-delivery dataset {cite:t}`patrick_c_caldwell_sea_2015`. The fast-delivery data are released within 1-2 months of data collection and are subject only to basic quality control. \n",
    "\n",
    "We'll be retrieving the hourly data for our station group at from UHSLC, and saving this to our data directory so we don't have to download again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hawaii stations are: \n",
    "stationdict = {\n",
    "    'Hilo': '060',\n",
    "    'Kawaihae': '552',\n",
    "    'Kahului': '059',\n",
    "    'Mokuoloe': '061',\n",
    "    'Honolulu': '057',\n",
    "    'Nawiliwili': '058',\n",
    "    'Johnston Island': '052',\n",
    "    'Midway Island': '050',\n",
    "    'Kaumalapau': '548',\n",
    "    'Barbers Point': '547',\n",
    "    'French Frigate Shoals': '014',\n",
    "}\n",
    "stationdict.values()\n",
    "\n",
    "station_group = 'Hawaiian Islands'\n",
    "\n",
    "glue('station_group', station_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{margin}\n",
    "```{note}\n",
    "What about research quality data (RQD)? \n",
    "RQD undergo thorough and time-consuming QC, and are usually released 1-2 years after data is received. \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://uhslc.soest.hawaii.edu/data/netcdf/fast/hourly/\" \n",
    "uhslc_ids = list(stationdict.values())\n",
    "\n",
    "for uhslc_id in uhslc_ids:\n",
    "    fname = f'h{uhslc_id}.nc' # h for hourly, d for daily\n",
    "\n",
    "    path = os.path.join(data_dir, 'tide_gauge_nc',fname)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        urlretrieve(os.path.join(url, fname), path) \n",
    "        print(f'Downloading {fname} from {url} to {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge all the datasets. This can take a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{caution}\n",
    "In the following section I remove the trailing zero from the record-id of each tide gauge. This trailing zero is only on fast-delivery data products.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "import glob        \n",
    "# Get a list of all .nc files in the data directory\n",
    "# filenames should be in the format h*.nc with * being the station id in the stationdict\n",
    "filenames = 'h' + pd.Series(stationdict.values()) + '.nc'\n",
    "files = [glob.glob(str(data_dir / 'tide_gauge_nc' / file))[0] for file in filenames]\n",
    "\n",
    "\n",
    "# Open the datasets\n",
    "datasets = [xr.open_dataset(file) for file in files]\n",
    "\n",
    "#merge in batches of 2 to avoid memory issues\n",
    "batch_size = 2\n",
    "merged_datasets = []\n",
    "\n",
    "for i in range(0, len(datasets), batch_size):\n",
    "    batch = datasets[i:i+batch_size]\n",
    "    merged_batch = xr.merge(batch, compat='no_conflicts', join='outer')\n",
    "    merged_datasets.append(merged_batch)\n",
    "\n",
    "#merge the merged datasets\n",
    "rsl = xr.merge(merged_datasets, compat='no_conflicts', join='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename record_id to station_id - this will align with NOAA and other data sources\n",
    "rsl = rsl.rename({'record_id': 'station_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert byte strings to normal strings\n",
    "rsl['station_name'] = xr.DataArray(\n",
    "    np.array(rsl['station_name'].values, dtype='U50'),\n",
    "    dims=('station_id',)\n",
    ")\n",
    "rsl['station_country'] = xr.DataArray(\n",
    "    np.array(rsl['station_country'].values, dtype='U30'),\n",
    "    dims=('station_id',)\n",
    ")\n",
    "rsl['ssc_id'] = xr.DataArray(\n",
    "    np.array(rsl['ssc_id'].values, dtype='U30'),\n",
    "    dims=('station_id',)\n",
    ")\n",
    "\n",
    "# remove the trailing zero from each station_id\n",
    "rsl['station_id'] =(rsl['station_id']/10).astype(int)\n",
    "rsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MHHW_uhslc_datums(id, datumname, table=None):\n",
    "    \n",
    "    url = 'https://uhslc.soest.hawaii.edu/stations/TIDES_DATUMS/fd/LST/fd'+f'{int(id):03}'+'/datumTable_'+f'{int(id):03}'+'_m_GMT.csv'\n",
    "    datumtable = pd.read_csv(url)\n",
    "    datum = datumtable[datumtable['Name'] == datumname]['Value'].values[0]\n",
    "    # ensure datum is a float\n",
    "    datum = float(datum)\n",
    "    if table:\n",
    "        return datum, datumtable\n",
    "    else:\n",
    "        return datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add MHHW to the dataset\n",
    "rsl['MHHW'] = xr.DataArray([get_MHHW_uhslc_datums(id, 'MHHW') for id in rsl['uhslc_id'].values], dims='station_id', coords={'station_id': rsl['station_id']})\n",
    "\n",
    "rsl['MHHW'].attrs['units'] = 'm'\n",
    "rsl['MHHW'].attrs['long_name'] = 'Mean Higher High Water, rel. to station datum'\n",
    "\n",
    "glue('datumname', 'MHHW')\n",
    "\n",
    "# add MSL to the dataset\n",
    "rsl['MSL'] = xr.DataArray([get_MHHW_uhslc_datums(id, 'MSL') for id in rsl['uhslc_id'].values], dims='station_id', coords={'station_id': rsl['station_id']})\n",
    "\n",
    "rsl['MSL'].attrs['units'] = 'm'\n",
    "rsl['MSL'].attrs['long_name'] = 'Mean Sea Level, rel. to station datum'\n",
    "\n",
    "# add MSL to the dataset\n",
    "rsl['MLLW'] = xr.DataArray([get_MHHW_uhslc_datums(id, 'MLLW') for id in rsl['uhslc_id'].values], dims='station_id', coords={'station_id': rsl['station_id']})\n",
    "\n",
    "rsl['MLLW'].attrs['units'] = 'm'\n",
    "rsl['MLLW'].attrs['long_name'] = 'Mean Low Low Water, rel. to station datum'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round the time to the nearest hour\n",
    "rsl['time'] = rsl['time'].dt.round('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sea level variable to m\n",
    "rsl['sea_level'] = rsl['sea_level'] / 1000  # convert from mm to m\n",
    "rsl['sea_level'].attrs['units'] = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure rsl is netcdf4 format\n",
    "rsl.to_netcdf(data_dir / 'rsl_hawaii_uhslc.nc',engine='netcdf4',mode='w')\n",
    "\n",
    "rsl.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from NOAA CO-OPS API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "url = \"https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi/stations.json\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# get a list of all the stations\n",
    "stations = json.loads(response.text)\n",
    "\n",
    "#from list of stations, get NOAA stations within the bounding box of UHSLC gauges\n",
    "hawaii_stations = [station for station in stations['stations'] if station['lat'] >= rsl['lat'].min()-2 and station['lat'] <= rsl['lat'].max()+2 and station['lng'] >= -(360-rsl['lon'].min())-2 and station['lng'] <= -(360-rsl['lon'].max())+2]\n",
    "\n",
    "#make a dictionary of all stations in hawaii with station name: station id\n",
    "stationdictNOAA  = {station['name']: station['id'] for station in hawaii_stations}\n",
    "stationdictNOAA \n",
    "\n",
    "# remove Pearl Harbor gauge  - tidesandcurrents lists the station that started in 2023, and we're not here to combine data.\n",
    "stationdictNOAA.pop('Pearl Harbor', None)\n",
    "\n",
    "stationdictNOAA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def fetch_data_chunk(stationID, start_date, end_date):\n",
    "    # Format dates for the URL\n",
    "    begin_date_str = start_date.strftime('%Y%m%d %H:%M')\n",
    "    end_date_str = end_date.strftime('%Y%m%d %H:%M')\n",
    "    product = 'hourly_height'\n",
    "    \n",
    "    # Create the URL\n",
    "    server='https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?'\n",
    "    url = f'{server}begin_date={begin_date_str}&end_date={end_date_str}&station={stationID}&datum=STND&product={product}&units=metric&time_zone=gmt&format=json'\n",
    "    \n",
    "    # Request data from NOAA API\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            if 'data' in data:\n",
    "                return data['data']\n",
    "            else:\n",
    "                # print(f\"No 'data' key in response for {begin_date_str} to {end_date_str}: {data}\")\n",
    "                return []\n",
    "        except ValueError as e:\n",
    "            print(f\"JSON decoding failed for {begin_date_str} to {end_date_str}: {e}\")\n",
    "            return []\n",
    "    else:\n",
    "        # print(f\"Failed to fetch data for {begin_date_str} to {end_date_str}, status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def fetch_noaa_water_level_parallel(stationID, start_date, end_date):\n",
    "    # Convert dates to datetime objects\n",
    "    start_date = datetime.strptime(start_date, '%Y%m%d %H:%M')\n",
    "    end_date = datetime.strptime(end_date, '%Y%m%d %H:%M')\n",
    "    \n",
    "    # List to hold all data\n",
    "    all_data = []\n",
    "    \n",
    "    # Generate date ranges in 31-day increments\n",
    "    date_ranges = []\n",
    "    current_start_date = start_date\n",
    "    while current_start_date < end_date:\n",
    "        current_end_date = current_start_date + timedelta(days=31)\n",
    "        if current_end_date > end_date:\n",
    "            current_end_date = end_date\n",
    "        date_ranges.append((current_start_date, current_end_date))\n",
    "        current_start_date = current_end_date + timedelta(seconds=1)\n",
    "    \n",
    "    total_ranges = len(date_ranges)\n",
    "    \n",
    "    # Fetch data in parallel\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_date_range = {executor.submit(fetch_data_chunk, stationID, start, end): (start, end) for start, end in date_ranges}\n",
    "        completed_ranges = 0\n",
    "        for future in as_completed(future_to_date_range):\n",
    "            try:\n",
    "                data_chunk = future.result()\n",
    "                if data_chunk:\n",
    "                    all_data.extend(data_chunk)\n",
    "            except Exception as e:\n",
    "                start, end = future_to_date_range[future]\n",
    "                # print(f\"Error fetching data for {start} to {end}: {e}\")  #uncomment if you want to track progress\n",
    "            \n",
    "            completed_ranges += 1\n",
    "            # print(f\"Progress: {completed_ranges}/{total_ranges} chunks completed.\") #uncomment if you want to track progress\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    if not df.empty:\n",
    "        df['t'] = pd.to_datetime(df['t'])\n",
    "        \n",
    "        # Clean data: remove rows with empty 'v' values\n",
    "        df = df[df['v'].str.strip() != '']\n",
    "        \n",
    "        # Convert 'v' to float\n",
    "        df['v'] = df['v'].astype(float)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates(subset='t')\n",
    "        \n",
    "        # Sort by time\n",
    "        df = df.sort_values(by='t')\n",
    "        \n",
    "        # Create time series\n",
    "        sea_level_ts = pd.Series(df['v'].values, index=df['t'])\n",
    "        \n",
    "        # Resample to hourly data\n",
    "        sea_level_ts.index = sea_level_ts.index.round('H')\n",
    "        sea_level_ts = sea_level_ts.resample('h').mean()\n",
    "        \n",
    "        # Ensure unique timestamps for xarray\n",
    "        sea_level_ts = sea_level_ts[~sea_level_ts.index.duplicated(keep='first')]\n",
    "        \n",
    "        # Create xarray dataset with the correct dimension\n",
    "        ds = xr.Dataset({'sea_level': ('t', sea_level_ts.values)}, coords={'t': sea_level_ts.index})\n",
    "\n",
    "        # rename t to time\n",
    "        ds = ds.rename({'t': 'time'})\n",
    "        \n",
    "        \n",
    "        return ds\n",
    "    else:\n",
    "        print(\"No data fetched.\")\n",
    "        return xr.Dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_date = '19050101 00:00'\n",
    "end_date = '20241231 00:00'\n",
    "\n",
    "for station_name, station_id in stationdictNOAA.items():\n",
    "    # check if we already have the data\n",
    "    if os.path.exists(data_dir / f'tide_gauge_nc/noaa_{station_id}.nc'):\n",
    "        #check last date in nc file\n",
    "        dsOLD = xr.open_dataset(data_dir / f'tide_gauge_nc/noaa_{station_id}.nc')\n",
    "        last_date = dsOLD['time'].values[-1]\n",
    "        dsOLD.close()\n",
    "        # if last date is after end_date, skip\n",
    "        if pd.to_datetime(last_date) >= datetime.strptime(end_date, '%Y%m%d %H:%M'):\n",
    "            print(f\"Data for {station_name} already exists and is up to date, skipping\")\n",
    "            continue\n",
    "        \n",
    "        if pd.to_datetime(last_date) < datetime.strptime(end_date, '%Y%m%d %H:%M'):\n",
    "            start_date = pd.to_datetime(last_date) + timedelta(hours=1)\n",
    "            start_date = start_date.strftime('%Y%m%d %H:%M')\n",
    "            print(f\"Data for {station_name} already exists, will append from {start_date}\")\n",
    "    else:\n",
    "        print(f\"Fetching data for {station_name}\")\n",
    "\n",
    "    ds = fetch_noaa_water_level_parallel(station_id, start_date, end_date)\n",
    "    ds['station_id'] = station_id\n",
    "    ds['station_name'] = station_name\n",
    "    ds['station_country'] = 'USA'\n",
    "    ds['lat'] = float([station['lat'] for station in hawaii_stations if station['id'] == station_id][0])\n",
    "    ds['lon'] = float([station['lng'] for station in hawaii_stations if station['id'] == station_id][0])\n",
    "\n",
    "    # if the file already exists, append the new data\n",
    "    if os.path.exists(data_dir / f'tide_gauge_nc/noaa_{station_id}.nc'):\n",
    "        with xr.open_dataset(data_dir / f'tide_gauge_nc/noaa_{station_id}.nc') as dsOLD:\n",
    "            dsNEW = xr.concat([dsOLD, ds], dim='time')\n",
    "        dsNEW = dsNEW.drop_duplicates(dim='time')\n",
    "        dsNEW.to_netcdf(data_dir / f'tide_gauge_nc/noaa_{station_id}.nc')\n",
    "    else:\n",
    "        ds.to_netcdf(data_dir /  f'tide_gauge_nc/noaa_{station_id}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll combine them into one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##COMMENTED OUT IF DATA HAS ALREADY BEEN COMBINED< TO FIX LATER\n",
    "# Load the data\n",
    "\n",
    "      \n",
    "# Get a list of all noaa*.nc files in the data directory\n",
    "files = glob.glob(os.path.join(data_dir,'tide_gauge_nc','noaa*.nc'))\n",
    "\n",
    "# Open the datasets\n",
    "datasets = [xr.open_dataset(file) for file in files]\n",
    "# add the station_id as a coordinate\n",
    "for ds in datasets:\n",
    "    # ds = remove_time_dim(ds)\n",
    "    ds.coords['station_id'] = ds['station_id']\n",
    "rslNOAA = xr.concat(datasets, dim='station_id',join='outer')\n",
    "\n",
    "rslNOAA['sea_level'].attrs['units'] = 'm'\n",
    "rslNOAA['sea_level'].attrs['long_name'] = 'Sea level, relative to station datum'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslNOAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some more metadata to this dataset, including MSL and MHHW datums for each gauge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MHHW = np.zeros(len(rslNOAA['station_id']))\n",
    "MSL = np.zeros(len(rslNOAA['station_id']))\n",
    "MLLW = np.zeros(len(rslNOAA['station_id']))\n",
    "\n",
    "for i in range(len(rslNOAA['station_id'])):\n",
    "    url = 'https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi/stations/'+ str(rslNOAA['station_id'][i].values) +'/datums.json?units=metric'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    datums = json.loads(response.text)\n",
    "    \n",
    "    def extract_datum_value(data, datum_name):\n",
    "        # Iterate through the datums list\n",
    "        for datum in data.get('datums', []):\n",
    "            # Check if the name matches the desired datum name\n",
    "            if datum.get('name') == datum_name:\n",
    "                # Return the value if found\n",
    "                return datum.get('value')\n",
    "        # Return None if the datum name is not found\n",
    "        return None\n",
    "    \n",
    "    #extract the MHHW and MSL datums\n",
    "    MHHW[i] = extract_datum_value(datums, 'MHHW')\n",
    "    MSL[i] = extract_datum_value(datums, 'MSL')\n",
    "    MLLW[i] = extract_datum_value(datums, 'MLLW')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslNOAA['MHHW'] = xr.DataArray(MHHW, dims='station_id', coords={'station_id': rslNOAA['station_id']})\n",
    "rslNOAA['MHHW'].attrs['units'] = 'm'\n",
    "rslNOAA['MHHW'].attrs['long_name'] = 'Mean Higher High Water, rel. to station datum'\n",
    "\n",
    "rslNOAA['MSL'] = xr.DataArray(MSL, dims='station_id', coords={'station_id': rslNOAA['station_id']})\n",
    "rslNOAA['MSL'].attrs['units'] = 'm'\n",
    "rslNOAA['MSL'].attrs['long_name'] = 'Mean Sea Level, rel. to station datum'\n",
    "\n",
    "rslNOAA['MLLW'] = xr.DataArray(MLLW, dims='station_id', coords={'station_id': rslNOAA['station_id']})\n",
    "rslNOAA['MLLW'].attrs['units'] = 'm'\n",
    "rslNOAA['MLLW'].attrs['long_name'] = 'Mean Lower Low Water, rel. to station datum'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the longitude to match UHSLC 360-degree convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslNOAA['lon'] = rslNOAA['lon'] + 360\n",
    "rslNOAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save rsl to the data directory\n",
    "#explicitly assign U50 to the station_name\n",
    "rslNOAA['station_name'] = xr.DataArray(\n",
    "    np.array(rslNOAA['station_name'].values, dtype='U50'),\n",
    "    dims=('station_id',)\n",
    ")\n",
    "\n",
    "rslNOAA.to_netcdf(data_dir / 'rsl_hawaii_noaa.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve altimetry data \n",
    "We are using the global ocean gridded L4 [Sea Surface Heights and Derived Variables](https://doi.org/10.48670/moi-00148) from Copernicus. \n",
    "\n",
    "To download a subset of the global altimetry data, uncomment out the call to get_CMEMS_data and run it in this notebook. To read more about how to download the data from the Copernicus Marine Toolbox (new as of December 2023), visit https://help.marine.copernicus.eu/en/articles/7949409-copernicus-marine-toolbox-introduction. \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "You will need a username and password to access the CMEMS (Copernicus Marine Service) data if this is the first time running the client. To register for data access (free), visit https://data.marine.copernicus.eu/register.  \n",
    "```\n",
    "````\n",
    "\n",
    "```{admonition} Large data download!\n",
    ":class: warning\n",
    "Getting errors on the code block below? Remember to uncomment \"get_CMEMS_data()\" to download. Note that if you change nothing in the function, it will download ~600 MB of data, which may take a long time!! You will only need to do this once. The dataset will be stored in the data directory you specify (which should be the same data directory we defined above).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the min and max lat and lon of rsl for altimetry data retrieval\n",
    "minlat = float(rslNOAA.lat.min().values)\n",
    "maxlat = float(rslNOAA.lat.max().values)\n",
    "minlon = float(rslNOAA.lon.min().values)\n",
    "maxlon = float(rslNOAA.lon.max().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CMEMS_data(minlat, maxlat, minlon, maxlon, data_dir=data_dir):\n",
    "        \n",
    "    #maxlat = 15\n",
    "    #minlat = 0\n",
    "    #minlon = 125\n",
    "    #maxlon = 140\n",
    "    maxlat = maxlat+10\n",
    "    minlat = minlat-10\n",
    "    maxlon = maxlon+10\n",
    "    minlon  = minlon-10\n",
    "    \n",
    "    start_date_str = \"1993-01-01T00:00:00\"\n",
    "    end_date_str = \"2025-04-30T23:59:59\"\n",
    "    data_dir = data_dir\n",
    "    \n",
    "    \"\"\"\n",
    "    Retrieves Copernicus Marine data for a specified region and time period.\n",
    "    \n",
    "    Args:\n",
    "        minlon (float): Minimum longitude of the region.\n",
    "        maxlon (float): Maximum longitude of the region.\n",
    "        minlat (float): Minimum latitude of the region.\n",
    "        maxlat (float): Maximum latitude of the region.\n",
    "        start_date_str (str): Start date of the data in ISO 8601 format.\n",
    "        end_date_str (str): End date of the data in ISO 8601 format.\n",
    "        data_dir (str): Directory to save the retrieved data.\n",
    "    \n",
    "    Returns:\n",
    "        str: The filename of the retrieved data.\n",
    "    \"\"\"\n",
    "    copernicusmarine.subset(\n",
    "        dataset_id=\"cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.125deg_P1D\",\n",
    "        variables=[\"sla\"],\n",
    "        minimum_longitude=minlon,\n",
    "        maximum_longitude=maxlon,\n",
    "        minimum_latitude=minlat,\n",
    "        maximum_latitude=maxlat,\n",
    "        start_datetime=start_date_str,\n",
    "        end_datetime=end_date_str,\n",
    "        output_directory=data_dir,\n",
    "        output_filename=\"cmems_L4_SSH_0.125deg_\" + start_date_str[0:4] + \"_\" + end_date_str[0:4] + \"hawaii.nc\"\n",
    "    )\n",
    "fname_cmems = 'cmems_L4_SSH_0.125deg_1993_2025hawaii.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copernicusmarine\n",
    "\n",
    "# check if the file exists, if not, download it\n",
    "if not os.path.exists(data_dir / fname_cmems):\n",
    "    print('You will need to download the CMEMS data in a separate script')\n",
    "    get_CMEMS_data(minlat, maxlat, minlon, maxlon, data_dir) #<<--- COMMENT OUT TO AVOID ACCIDENTAL DATA DOWNLOADS.\n",
    "else:\n",
    "    print('CMEMS data already downloaded, good to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the CMEMS data and take a look. We will want to make an ASL dataset similar in structure to the RSL data so that we can easily compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the CMEMS data\n",
    "fname_cmems = 'cmems_L4_SSH_0.125deg_1993_2025hawaii.nc'\n",
    "ds = xr.open_dataset(data_dir / fname_cmems)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use NOAA data\n",
    "rsl = xr.open_dataset(data_dir / 'rsl_hawaii_noaa.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for the nearest point to the tide gauge location that has data\n",
    "sla = []\n",
    "for lat, lon in zip(rsl['lat'].values, rsl['lon'].values):\n",
    "    sla.append(ds['sla'].sel(\n",
    "        longitude=lon, latitude=lat, method='nearest'\n",
    "    ))\n",
    "\n",
    "    #if the data is null, nan average over the nearest 4 points\n",
    "    tol = 0.25\n",
    "    if sla[-1].isnull().all():\n",
    "        sla[-1] = ds['sla'].sel(\n",
    "            longitude=slice(lon-360-tol, lon-360+tol), \n",
    "            latitude=slice(lat-tol, lat+tol)\n",
    "        ).mean(dim=['latitude', 'longitude'])\n",
    "        sla[-1]['latitude'] = np.mean(lat)\n",
    "        sla[-1]['longitude'] = np.mean(lon)\n",
    "\n",
    "sla = xr.concat(sla, dim='station_id')\n",
    "\n",
    "# make sla a dataset with variables from rsl\n",
    "sla = sla.to_dataset(name='sla')\n",
    "sla['station_name'] = rsl['station_name']\n",
    "\n",
    "# Creating lat_str and lon_str arrays with 'station_id' as their dimension\n",
    "lat_str = [f'{np.abs(lat):.3f}\\u00B0{\"N\" if lat > 0 else \"S\"}' for lat in sla.latitude.values]\n",
    "lon_str = [f'{np.abs(lon):.3f}\\u00B0{\"E\" if lon > 0 else \"W\"}' for lon in sla.longitude.values]  \n",
    "\n",
    "# Convert lists to DataArrays with 'station_id' as their dimension\n",
    "lat_str_da = xr.DataArray(lat_str, dims=['station_id'], coords={'station_id': rsl['station_id']})\n",
    "lon_str_da = xr.DataArray(lon_str, dims=['station_id'], coords={'station_id': rsl['station_id']})\n",
    "\n",
    "# Assign these DataArrays to the sla dataset\n",
    "sla['lat_str'] = lat_str_da\n",
    "sla['lon_str'] = lon_str_da\n",
    "\n",
    "# add original data source to attributes\n",
    "sla.attrs['original_data_source'] = 'CMEMS L4 SSH 0.125deg'\n",
    "sla.attrs['title'] = ds.attrs['title']\n",
    "sla.attrs['source_file'] = str(data_dir / fname_cmems)\n",
    "\n",
    "# ensure latitude and longitude are coordinates associated with a location\n",
    "sla = sla.set_coords(['latitude', 'longitude'])\n",
    "\n",
    "\n",
    "\n",
    "sla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful here! The mean SLA for the 1993-2012 reference period is NOT zero, it is near 2.5cm, due to ensuring that the 1993 mean sea level is zero. We'll use this later when making our tide gauge comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save sla to the data directory\n",
    "sla.to_netcdf(data_dir / 'asl_hawaii.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "### Process the tide gauge data to match CMEMS\n",
    " Now we'll convert tide gauge data into a daily record for the POR in units of meters to match the CMEMS data. \n",
    " \n",
    " The next code block:\n",
    " - extracts tide gauge data for the period 1993-2022\n",
    " - converts it to meters\n",
    " - removes any NaN values\n",
    " - resamples the data to daily mean\n",
    " - and normalizes it relative to the 1993-2012 epoch. \n",
    " \n",
    "\n",
    "The resulting data is stored in the variable 'rsl_daily' with units in meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's establish a period of record from 1993-2022.\n",
    "# establish the time period of interest\n",
    "start_date = dt.datetime(1993,1,1)\n",
    "end_date = dt.datetime(2024,12,31)\n",
    "#\n",
    "# also save them as strings, for plotting\n",
    "start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data for the period of record (POR)\n",
    "tide_gauge_data_POR = rsl['sea_level'].sel(time=slice(start_date, end_date))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get daily tide gauge data to match altimetry record\n",
    "To make a fair comparison with the CMEMS satellite data, we first need to remove the tides from our tide-gauge records (isn't it ironic). The satellite product (a daily value) already has the regular daily and twice-daily tidal signals taken out, so we do the same for the gauge data. To do this, we apply a Godin filter — a well-established method in oceanography that smooths hourly sea level records with three succesive running moving averages (24, 24, and 25 hours long). This process effectively strips away the repeating tidal cycles and leaves behind the slower, non-tidal changes in sea level that we want to study. \n",
    "\n",
    "\n",
    "```{margin} What's daily data, anyway?\n",
    "The University of Hawai‘i Sea Level Center (UHSLC) daily product uses a 119-point convolution filter — a weighted ~5-day low-pass window with maximum weight at the center (noon) and tapering at the edges. This filter acts very much like the Godin filter in that it removes tidal energy at 1 and 2 cycles per day, but is somewhat smoother, further damping signals with periods shorter than a few days. Both filters are suitable for tide-gauge/altimetry comparisons for general trend calculations, although direct removal of predicted tidal signals should in theory align more directly with altimetry products. The UHSLC approach, described by {cite:t}`kilonsky_pursuit_1991`, first performs a harmonic tidal analysis (with UTide) and removes all constituents with periods shorter than 30 h. The resulting residual hourly series is then averaged with the noon-centered 119-hour filter, producing a daily sea level time series.   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def godin_xr(da: xr.DataArray, allow_missing: float = 0.0) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Godin filter (24h → 24h → 25h centered moving averages) along 'time'.\n",
    "    - da: DataArray with a 'time' dimension (hourly, sorted).\n",
    "    - allow_missing: fraction of a window allowed to be NaN before output is NaN.\n",
    "        0.0 = strict Godin (full window required)\n",
    "        0.25 = allow up to 25% missing within each window\n",
    "    \"\"\"\n",
    "    assert \"time\" in da.dims, \"Input must have a 'time' dimension.\"\n",
    "\n",
    "    def ma(x, n):\n",
    "        # min samples needed in the n-hour window\n",
    "        min_periods = int(np.ceil((1.0 - allow_missing) * n))\n",
    "        return x.rolling(time=n, center=True, min_periods=min_periods).mean()\n",
    "\n",
    "    y = ma(da, 24)\n",
    "    y = ma(y, 24)\n",
    "    y = ma(y, 25)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_level_detided = godin_xr(tide_gauge_data_POR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have filtered hourly data, so let's resample that to noon values. We'll do this to align with the altimetry data (and UHSLC's 119-point noon-centered filter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one sample per day: closest to 12:00\n",
    "t = pd.DatetimeIndex(sea_level_detided.time.values)\n",
    "frac_hours = t.hour + t.minute/60 + t.second/3600\n",
    "dist = np.abs(frac_hours - 12.0)\n",
    "noon_idx = (\n",
    "    pd.Series(dist, index=np.arange(len(t)))\n",
    "    .groupby(t.normalize())\n",
    "    .idxmin()\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "daily = sea_level_detided.isel(time=noon_idx).rename({'time': 'time_noon'})\n",
    "daily = daily.assign_coords(time_noon=sea_level_detided.time.isel(time=noon_idx).values)\n",
    "\n",
    "# drop any NaNs from Godin edge effects\n",
    "daily = daily.dropna(dim='time_noon', how='all')\n",
    "\n",
    "# rename time_noon to time\n",
    "daily = daily.rename({'time_noon': 'time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare daily data from different sources, let's do Honolulu, which is 57 for UHSLC and 1612340 for NOAA\n",
    "url = \"https://uhslc.soest.hawaii.edu/data/netcdf/fast/daily/\" \n",
    "uhslc_id = '057'\n",
    "fname = f'd{uhslc_id}.nc' # h for hourly, d for daily\n",
    "path = os.path.join(data_dir, 'tide_gauge_nc',fname)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    urlretrieve(os.path.join(url, fname), path) \n",
    "    print(f'Downloading {fname} from {url} to {path}')\n",
    "\n",
    "# open the UHSLC data\n",
    "rsl_uhslc_daily = xr.open_dataset(path)\n",
    "# change record_id to station_id\n",
    "rsl_uhslc_daily = rsl_uhslc_daily.rename({'record_id': 'station_id'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the two datasets\n",
    "rsl_uhslc_daily = rsl_uhslc_daily.sel(time=slice(start_date + pd.DateOffset(days=1), end_date - pd.DateOffset(days=1)))\n",
    "rsl_uhslc_daily['sea_level_m'] = rsl_uhslc_daily['sea_level']/1000   # convert from mm to m\n",
    "\n",
    "\n",
    "# truncate both datasets to end in 2019-12-31, start on 1993-01-02\n",
    "daily = daily.sel(time=slice('1993-01-02', '2019-12-31'))\n",
    "rsl_uhslc_daily = rsl_uhslc_daily.sel(time=slice('1993-01-02', '2019-12-31'))\n",
    "\n",
    "\n",
    "# make a figure with 3 rows and 1 column\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "fig,axs = plt.subplots(3,1,figsize=(10,15))\n",
    "# first plot the full time series\n",
    "axs[0].plot(daily.sel(station_id='1612340').time, daily.sel(station_id='1612340'), label='Godin', color='blue')\n",
    "axs[0].plot(rsl_uhslc_daily.sel(station_id=570).time, rsl_uhslc_daily.sel(station_id=570).sea_level_m, label='UHSLC', color='orange')\n",
    "axs[0].set_ylabel('Sea Level (m)')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Full Time Series')\n",
    "# axs[0].set_xlim(pd.to_datetime('2023-01-01'), pd.to_datetime('2023-12-31'))\n",
    "\n",
    "# second plot the difference\n",
    "axs[1].plot(daily.sel(station_id='1612340').time, 1000*(daily.sel(station_id='1612340') - rsl_uhslc_daily.sel(station_id=570).sea_level_m), label='Difference', color='green')\n",
    "axs[1].set_ylabel('Difference (mm)')\n",
    "axs[1].legend()\n",
    "axs[1].set_title('Difference Between Godin and UHSLC')\n",
    "\n",
    "# third plot the scatter plot\n",
    "axs[2].scatter(rsl_uhslc_daily.sel(station_id=570).sea_level_m, daily.sel(station_id='1612340'), color='purple', alpha=0.5)\n",
    "axs[2].set_xlabel('UHSLC Daily (m)')\n",
    "axs[2].set_ylabel('Godin Daily (m)')\n",
    "axs[2].set_title('Scatter Plot')\n",
    "\n",
    "\n",
    "# # add bias and rmsd to the plot\n",
    "bias = (daily.sel(station_id='1612340') - rsl_uhslc_daily.sel(station_id=570).sea_level_m).mean().values\n",
    "rmsd = np.sqrt(((daily.sel(station_id='1612340') - rsl_uhslc_daily.sel(station_id=570).sea_level_m)**2).mean()).values\n",
    "plt.title(f'Comparison of Godin and UHSLC Daily Sea Level at Honolulu\\nBias: {bias*1000:.2f} mm, RMSD: {rmsd*1000:.2f} mm')\n",
    "\n",
    "# # set date to be all of 2017\n",
    "# plt.xlim(pd.to_datetime('2017-01-01'), pd.to_datetime('2017-12-31'))\n",
    "# plt.ylim(1.4,1.9)\n",
    "\n",
    "# axs[2].scatter(rsl_uhslc_daily.sel(station_id=570).sea_level_m, daily.sel(station_id='1612340'), color='green', alpha=0.5)\n",
    "# # plot 1:1 line\n",
    "# lims = [min(rsl_uhslc_daily.sel(station_id=570).sea_level_m.min().values, daily.sel(station_id='1612340').min().values), \n",
    "#         max(rsl_uhslc_daily.sel(station_id=570).sea_level_m.max().values, daily.sel(station_id='1612340').max().values)]\n",
    "# axs[2].plot(lims, lims, 'k--', alpha=0.75, zorder=0)\n",
    "# axs[2].set_xlim(lims)\n",
    "# axs[2].set_ylim(lims)\n",
    "# axs[2].set_xlabel('UHSLC Daily (m)')\n",
    "# axs[2].set_ylabel('Godin Daily (m)')\n",
    "# axs[2].set_title('Scatter Plot')\n",
    "# set aspect ratio to 1:1\n",
    "axs[2].set_aspect('equal', 'box')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the tide gauge data relative to the 1993-2012 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch_start, epoch_end = start_date, '2012-12-31'\n",
    "epoch_daily_avg = daily.sel(time=slice(epoch_start, epoch_end))\n",
    "epoch_daily_mean = epoch_daily_avg.mean(dim='time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_daily_mean_cmems = sla.sel(time=slice(epoch_start, epoch_end)).mean(dim='time')\n",
    "epoch_daily_mean_cmems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_daily_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display station names in a cleaner format\n",
    "print(\"Station Names:\")\n",
    "for i, name in enumerate(rsl['station_name'].values):\n",
    "    station_id = rsl['station_id'].values[i]\n",
    "    print(f\"{i}: {name} (Station ID: {station_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the epoch daily mean from the tide gauge daily average\n",
    "rsl_daily = daily - epoch_daily_mean\n",
    "\n",
    "# add in the CMEMS mean for the epoch to remove bias from 1993 start.\n",
    "rsl_daily = rsl_daily + epoch_daily_mean_cmems['sla']\n",
    "\n",
    "# Set the attributes of the rsl_daily data\n",
    "rsl_daily.attrs = tide_gauge_data_POR.attrs\n",
    "rsl_daily.attrs['units'] = 'm'\n",
    "\n",
    "# add lat and lon to the dataset\n",
    "rsl_daily['lat'] = rsl['lat']\n",
    "rsl_daily['lon'] = rsl['lon']\n",
    "\n",
    "# add the station name and country\n",
    "rsl_daily['station_name'] = rsl['station_name']\n",
    "\n",
    "# change the variable name to sea level anomaly\n",
    "rsl_daily.name = 'rsl_anomaly'\n",
    "\n",
    "# change long name of the variable to sea level anomaly\n",
    "rsl_daily.attrs['long_name'] = 'Sea Level Anomaly'\n",
    "rsl_daily.attrs['epoch'] = '1993-2012'\n",
    "\n",
    "rsl_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = rsl_daily.plot(x='time', col='station_id', col_wrap=1, sharey=False, sharex=True, figsize=(15, 10))\n",
    "\n",
    "# Use g.axs to iterate over the axes in the FacetGrid\n",
    "for ax, id in zip(g.axs.flat, rsl_daily.station_id):\n",
    "    # Accessing the station_name coordinate for the current station_id directly\n",
    "    station_name = rsl_daily.station_name.sel(station_id=id).item()\n",
    "    ax.set_title(station_name)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationdictNOAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tide gauge daily average relative to MHHW\n",
    "rsl_daily_mhhw = daily - rsl['MHHW']\n",
    "\n",
    "# Set the attributes of the rsl_daily data\n",
    "rsl_daily_mhhw.attrs = tide_gauge_data_POR.attrs\n",
    "rsl_daily_mhhw.attrs['long_name'] = 'water level above MHHW'\n",
    "rsl_daily_mhhw.attrs['units'] = 'm'\n",
    "# add lat and lon to the dataset\n",
    "rsl_daily_mhhw['lat'] = rsl['lat']\n",
    "rsl_daily_mhhw['lon'] = rsl['lon']\n",
    "# add the station name and country\n",
    "rsl_daily_mhhw['station_name'] = rsl['station_name']\n",
    "\n",
    "rsl_daily_mhhw.name = 'rsl_mhhw'\n",
    "# save rsl_daily to the data directory\n",
    "# rsl_daily_mhhw.to_netcdf(data_dir / 'rsl_daily_hawaii_mhhw.nc')\n",
    "\n",
    "\n",
    "#combine the two datasets\n",
    "rsl_daily_combined = xr.merge([rsl_daily, rsl_daily_mhhw])\n",
    "\n",
    "rsl_daily_combined['storm_time'] = rsl_daily_combined['time']\n",
    "rsl_daily_combined['storm_time'] = xr.DataArray(\n",
    "    pd.to_datetime(rsl_daily_combined['storm_time'].values).to_series().apply(\n",
    "        lambda x: x if x.month >= 5 else x - pd.DateOffset(years=1)\n",
    "    ),\n",
    "    dims=rsl_daily_combined['storm_time'].dims,\n",
    "    coords=rsl_daily_combined['storm_time'].coords\n",
    ")\n",
    "\n",
    "#make storm time a coordinate\n",
    "rsl_daily_combined = rsl_daily_combined.assign_coords(storm_time = rsl_daily_combined.storm_time, compat='no_conflicts')\n",
    "\n",
    "# save rsl_daily_combined to the data directory\n",
    "rsl_daily_combined.to_netcdf(data_dir / 'rsl_daily_hawaii.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsl_daily_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what's the mean sla_tg anomaly from 1993-2012\n",
    "sla_mean = sla.sel(time=slice('1993-01-01', '2012-12-31')).mean(dim='time')\n",
    "sla_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsl_daily_combined['storm_time'] = rsl_daily_combined['time']\n",
    "# rsl_daily_combined['storm_time'] = xr.DataArray(\n",
    "#     pd.to_datetime(rsl_daily_combined['storm_time'].values).to_series().apply(\n",
    "#         lambda x: x if x.month >= 5 else x - pd.DateOffset(years=1)\n",
    "#     ),\n",
    "#     dims=rsl_daily_combined['storm_time'].dims,\n",
    "#     coords=rsl_daily_combined['storm_time'].coords\n",
    "# )\n",
    "\n",
    "# #make storm time a coordinate\n",
    "# rsl_daily_combined = rsl_daily_combined.assign_coords(storm_time = rsl_daily_combined.storm_time)\n",
    "\n",
    "# # save rsl_daily_combined to the data directory\n",
    "# rsl_daily_combined.to_netcdf(data_dir / 'rsl_daily_hawaii.nc')\n",
    "\n",
    "# rsl_daily_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Climate Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_psl_url(url):\n",
    "    response = requests.get(url)\n",
    "    data = response.text\n",
    "    # Split the data into lines\n",
    "    lines = data.strip().split('\\n')\n",
    "    # Collect data lines until we encounter -9999\n",
    "    data_lines = []\n",
    "    for line in lines[1:]:  # Skip the first line if it's a header\n",
    "        stripped_line = line.strip()\n",
    "        if stripped_line == '':\n",
    "            continue  # Skip empty lines\n",
    "        # Detect possible footer lines (either starting with -99.9, -9999, or a textual footer)\n",
    "        if stripped_line.startswith('-99.9') or stripped_line.startswith('-99.99') or stripped_line.startswith('-9999') or any(char.isalpha() for char in stripped_line):\n",
    "            break  # Stop processing when footer is encountered\n",
    "        # Add valid data lines to the list\n",
    "        data_lines.append(line)\n",
    "    # Combine data lines into a single string\n",
    "    data_str = '\\n'.join(data_lines)\n",
    "    data_io = io.StringIO(data_str)\n",
    "    return data_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_psl_format(csvfile, varName):\n",
    "    df = pd.read_csv(csvfile, sep=r'\\s+', skiprows=1, skipfooter=0, header=None, engine='python')\n",
    "    CI = df.melt(id_vars=0, var_name='Month', value_name=varName)\n",
    "    #Replace placeholder values with NaN\n",
    "    CI.replace([-9.99, -9999, -9999.000, -999.99,-99.90,-99.99], np.nan, inplace=True)\n",
    "    # Drop rows with NaN values\n",
    "    CI = CI.dropna(subset=[varName])\n",
    "    CI['time'] = pd.to_datetime({'year': CI[0], 'month': CI['Month'], 'day': 1})\n",
    "    # Sort by 'time'\n",
    "    CI = CI.sort_values('time').reset_index(drop=True)\n",
    "    CI = CI[['time', varName]]\n",
    "\n",
    "    return CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ncei_format(csvfile, varName):\n",
    "    df = pd.read_csv(csvfile, sep=r'\\s+', skiprows=0, skipfooter = 0,header=1, engine='python')\n",
    "    ClimateIndex = df.melt(id_vars='Year', var_name='Month', value_name=varName)\n",
    "    ClimateIndex['Month'] = ClimateIndex['Month'].map(month_mapping)\n",
    "    # Replace placeholder values with NaN\n",
    "    ClimateIndex.replace(99.99, np.nan, inplace=True)\n",
    "    # Drop rows with NaN values\n",
    "    ClimateIndex = ClimateIndex.dropna(subset=[varName])\n",
    "    ClimateIndex['time'] = pd.to_datetime({'year': ClimateIndex['Year'], 'month': ClimateIndex['Month'], 'day': 1})\n",
    "    # Sort by 'time'\n",
    "    ClimateIndex = ClimateIndex.sort_values('time').reset_index(drop=True)\n",
    "    ClimateIndex = ClimateIndex[['time', varName]]\n",
    "    return ClimateIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_mapping = {\n",
    "    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4,\n",
    "    'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8,\n",
    "    'Sep': 9, 'Oct': 10, 'Nov':11, 'Dec':12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert fraction of the year to datetime\n",
    "def convert_fraction_to_date(date_float):\n",
    "    # Extract year and fraction\n",
    "    year = int(date_float)\n",
    "    fraction = date_float - year\n",
    "    \n",
    "    # Calculate the total number of days in the year (consider leap years)\n",
    "    days_in_year = 366 if (year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)) else 365\n",
    "    \n",
    "    # Calculate the day of the year based on the fraction\n",
    "    day_of_year = int(np.floor(fraction * days_in_year)) + 1\n",
    "    \n",
    "    # Create a datetime object for January 1st of the given year\n",
    "    start_date = pd.to_datetime(f\"{year}-01-01\")\n",
    "    \n",
    "    # Add the day offset to get the actual date\n",
    "    return start_date + pd.to_timedelta(day_of_year - 1, unit='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pacific Meridional Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pmm(yearStart='1950', yearEnd='2024', data_dir=data_dir):\n",
    "    plottingUrlBase = 'https://psl.noaa.gov/cgi-bin/data/climateindices/corr.pl?'\n",
    "    plotSpecs1 = 'tstype1=45&custname1=&custtitle1=&tstype2=0&custname2=&custtitle2=&'\n",
    "    timeRange = 'year1=' + yearStart + '&year2=' + yearEnd + '&'\n",
    "    plotSpecs2 = 'itypea=0&y1=&y2=&plotstyle=0&length=&lag=&iall=0&iseas=0&mon1=0&mon2=11&anom=0&climo1_yr1=&climo1_yr2=&climo2_yr1=&climo2_yr2=&Submit=Calculate+Results'\n",
    "\n",
    "    plottingUrl = plottingUrlBase + plotSpecs1 + timeRange + plotSpecs2\n",
    "\n",
    "    # Access the plotting URL and search for the \"Data file\" link\n",
    "    response = requests.get(plottingUrl)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch plotting URL: {response.status_code}\")\n",
    "\n",
    "    # Use a regular expression to find the \"Data file\" link\n",
    "    match = re.search(r'href=([\\'\"]?)(/tmp/gcos_wgsp/[^\\'\"\\s>]+)\\1>Data file<', response.text)\n",
    "    if not match:\n",
    "        raise Exception(\"Data file link not found in the response.\")\n",
    "    \n",
    "    data_url = match.group(2)\n",
    "\n",
    "    # Download the data\n",
    "    response = requests.get('https://psl.noaa.gov/' + data_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch data file: {response.status_code}\")\n",
    "\n",
    "    data = response.text\n",
    "\n",
    "    # Read the data into a DataFrame\n",
    "    data_io = io.StringIO(data)\n",
    "    pmm = pd.read_csv(data_io, sep=r'\\s+', header=None, names=[\"time\", \"PMM\"])\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    pmm = pmm.dropna()\n",
    "\n",
    "    # Apply the conversion function to the 'Date' column\n",
    "    pmm['time'] = pmm['time'].apply(convert_fraction_to_date)\n",
    "\n",
    "    # Put this time as closest to the first day of the month\n",
    "    pmm['time'] = pmm['time'] + pd.DateOffset(days=15)\n",
    "    pmm['time'] = pmm['time'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "    # Save to the data directory without the index\n",
    "    pmm.to_csv(data_dir / 'climate_indices' / 'pmm.csv', index=False)\n",
    "    print('PMM data retrieved and saved to data directory')\n",
    "    return pmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDO: Pacific Decadal Oscillation\n",
    "\n",
    "Monthly SST anomalies in the North Pacific Ocean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pdo_ncei(data_dir=data_dir):\n",
    "    url = 'https://www.ncei.noaa.gov/pub/data/cmb/ersst/v5/index/ersst.v5.pdo.dat'\n",
    "    response = requests.get(url)\n",
    "    data = response.text\n",
    "    data_io = io.StringIO(data)\n",
    "    PDO = read_psl_format(data_io, 'PDO')\n",
    "    PDO.to_csv(data_dir / 'pdo.csv', index=False)\n",
    "    print('PDO data retrieved and saved to data directory')\n",
    "\n",
    "    return PDO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST: Bivariate EnSo Timeseries \n",
    "\n",
    "This climate index combines the atmospheric component of ENSO (SOI) with the oceanic component (Nino 3.4 SST). \n",
    "\n",
    "For more info, see: https://psl.noaa.gov/people/cathy.smith/best/details.html\n",
    "Smith, C.A. and P. Sardeshmukh, 2000, The Effect of ENSO on the Intraseasonal Variance of Surface Temperature in Winter., International J. of Climatology, 20 1543-1557."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_climateIndex(ClimateIndex, data_dir=data_dir):\n",
    "    PSLurlBase = 'https://psl.noaa.gov/data/'\n",
    "\n",
    "    # Define the URL based on the ClimateIndex\n",
    "    if ClimateIndex in ['TNA', 'ONI','PNA']:\n",
    "        url = PSLurlBase + 'correlation/' + ClimateIndex.lower() + '.data'\n",
    "    \n",
    "    elif ClimateIndex in ['AAO']:\n",
    "        url = PSLurlBase + 'timeseries/month/data/' + ClimateIndex.lower() + '.data'\n",
    "\n",
    "    elif ClimateIndex == 'AO':\n",
    "        url = PSLurlBase + 'timeseries/month/data/' + ClimateIndex.lower() + '.long.data'\n",
    "\n",
    "    elif ClimateIndex == 'DMI':\n",
    "        url = 'https://psl.noaa.gov/gcos_wgsp/Timeseries/Data/dmieast.had.long.data'\n",
    "\n",
    "    elif ClimateIndex == 'BEST':\n",
    "        url = PSLurlBase + 'correlation/censo.data'\n",
    "    \n",
    "    elif ClimateIndex == 'PDO':\n",
    "        url = 'https://psl.noaa.gov/pdo/data/pdo.timeseries.sstens.data'\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"ClimateIndex '{ClimateIndex}' is not recognized\")\n",
    "    \n",
    "    print(f\"Retrieving data from: {url}\")\n",
    "\n",
    "    # Fetch and format the data\n",
    "    data_io = read_psl_url(url)\n",
    "    climate_data = read_psl_format(data_io, ClimateIndex)\n",
    "\n",
    "    # Save the data to a CSV file\n",
    "    output_file = data_dir / 'climate_indices' / (ClimateIndex.lower() + '.csv')\n",
    "    climate_data.to_csv(output_file, index=False)\n",
    "    print(f\"{ClimateIndex} data retrieved and saved to {output_file}\")\n",
    "\n",
    "    return climate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONI = retrieve_climateIndex('ONI')\n",
    "TNA = retrieve_climateIndex('TNA')\n",
    "PNA = retrieve_climateIndex('PNA')\n",
    "# AAO = retrieve_climateIndex('AAO')\n",
    "AO = retrieve_climateIndex('AO')\n",
    "# DMI = retrieve_climateIndex('DMI')\n",
    "BEST = retrieve_climateIndex('BEST')\n",
    "\n",
    "PDO = retrieve_climateIndex('PDO')\n",
    "\n",
    "import re\n",
    "PMM = retrieve_pmm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all DataFrames and their corresponding names\n",
    "CIs = {\n",
    "    'BEST': BEST,\n",
    "    # 'DMI': DMI,\n",
    "    'AO': AO,\n",
    "    # 'AAO': AAO,\n",
    "    'PNA': PNA,\n",
    "    'TNA': TNA,\n",
    "    'ONI': ONI,\n",
    "    'PDO': PDO,\n",
    "    'PMM': PMM\n",
    "}\n",
    "\n",
    "# Start with the first DataFrame (BEST in this case)\n",
    "merged_CI = CIs['BEST']\n",
    "\n",
    "# Loop through the rest of the DataFrames and merge them on 'time'\n",
    "for name, df in CIs.items():\n",
    "    if name != 'BEST':\n",
    "        merged_CI = pd.merge(merged_CI, df, on='time', how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate to 1990 through 2024\n",
    "merged_CI = merged_CI[(merged_CI['time'] >= '1990-01-01') & (merged_CI['time'] <= '2024-12-31')]\n",
    "\n",
    "#replace placeholder values with NaN\n",
    "merged_CI.replace([-999], np.nan, inplace=True)\n",
    "\n",
    "merged_CI.to_csv(data_dir / 'climate_indices' / 'climate_indices.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get standard deviation of the climate indices\n",
    "merged_CI = pd.read_csv(data_dir / 'climate_indices' / 'climate_indices.csv')\n",
    "\n",
    "# Calculate the standard deviation of each column except 'time'\n",
    "std_dev = merged_CI.drop(columns='time').std()\n",
    "\n",
    "# calculate the mean of each column except 'time'\n",
    "mean = merged_CI.drop(columns='time').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the climate indices such that they have a standard deviation of 1\n",
    "merged_CI_norm = merged_CI.copy()\n",
    "for col in merged_CI_norm.columns[1:]:\n",
    "    # normalize the column by subtracting the mean and dividing by the standard deviation\n",
    "    merged_CI_norm[col] = merged_CI_norm[col] - mean[col]\n",
    "    merged_CI_norm[col] = merged_CI_norm[col] / std_dev[col]\n",
    "\n",
    "merged_CI_norm['time'] = pd.to_datetime(merged_CI_norm['time'])\n",
    "\n",
    "#save to the data directory\n",
    "merged_CI_norm.to_csv(data_dir / 'climate_indices' / 'climate_indices_norm.csv', index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_CI_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "indices = merged_CI_norm.columns[1:]\n",
    "\n",
    "# Adjusting the x-axis limits to range from 1993 to 2024\n",
    "fig, axs = plt.subplots(len(indices), 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "# Loop through each index and its corresponding subplot\n",
    "for i, index in enumerate(indices):\n",
    "    axs[i].plot(merged_CI_norm['time'], merged_CI_norm[index], label=index)\n",
    "    axs[i].set_ylabel(index)  \n",
    "    axs[i].set_ylim(-3, 3)  # Set y-axis limits to\n",
    "    axs[i].tick_params(axis='y', labelsize=8)  # Set smaller tick size for y-axis\n",
    "\n",
    "# Set the x-axis label only for the last subplot\n",
    "axs[-1].set_xlabel('Year')\n",
    "\n",
    "# Set x-axis major ticks to every 5 years and limit to 1993 to 2024\n",
    "axs[-1].set_xlim(pd.Timestamp('1990-01-01'), pd.Timestamp('2024-01-01'))\n",
    "axs[-1].xaxis.set_major_locator(mdates.YearLocator(5))\n",
    "axs[-1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "# Set a common title for the figure\n",
    "fig.suptitle('Normalized Climate Indices over Time', fontsize=16)\n",
    "\n",
    "# Adjust spacing between subplots, reducing vertical gaps\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96], h_pad=0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":style: plain\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLI311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
