{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importing libraries')\n",
    "%run setup.ipynb\n",
    "print('Importing plotting rules and functions')\n",
    "import plotting_functions\n",
    "print('Importing time series functions')\n",
    "from tseries_functions import *\n",
    "from tide_functions import calculate_ntr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e20138",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(data_dir / 'rsl_hawaii_noaa.nc')\n",
    "\n",
    "# change all the station_ids to integers\n",
    "ds['station_id'] = ds['station_id'].astype(int)\n",
    "\n",
    "\n",
    "# get the ntr for each station\n",
    "calculate_ntr(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ntr data\n",
    "station_ids = ds['station_id'].values\n",
    "\n",
    "station_id = station_ids[5]\n",
    "print('Doing station', station_id)\n",
    "station_name = ds.station_name.sel(station_id=station_id).item()\n",
    "print('Station name:', station_name)\n",
    "mhhw = ds.MHHW.sel(station_id=station_id).item()\n",
    "msl = ds.MSL.sel(station_id=station_id).item()\n",
    "mllw = ds.MLLW.sel(station_id=station_id).item()\n",
    "ntrpath = f'ntr_data/ntr_{station_id}.csv'\n",
    "\n",
    "\n",
    "ntr_data = pd.read_csv(Path(data_dir / ntrpath), parse_dates=['time'])\n",
    "\n",
    "#inspect it, does it look sane?\n",
    "ntr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e75814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leap days for day of year analysis\n",
    "ntr_data_noleap = ntr_data[~((ntr_data['time'].dt.month == 2) & (ntr_data['time'].dt.day == 29))].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr_data_noleap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each year, get daily maximum\n",
    "ntr_data_noleap['year'] = ntr_data_noleap['time'].dt.year\n",
    "ntr_data_noleap['month'] = ntr_data_noleap['time'].dt.month\n",
    "ntr_data_noleap['doy'] = ntr_data_noleap['time'].dt.dayofyear\n",
    "ntr_data_noleap['day'] = ntr_data_noleap['time'].dt.day\n",
    "ntr_data_noleap['date'] = pd.to_datetime(ntr_data_noleap[['year', 'month', 'day']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd55500",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_max = ntr_data_noleap.groupby('date').max().reset_index()\n",
    "daily_max['year'] = daily_max['date'].dt.year\n",
    "daily_max['month'] = daily_max['date'].dt.month\n",
    "daily_max['doy'] = daily_max['date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr_data_noleap['tide_plus_trend'] = ntr_data_noleap['tide'] + ntr_data_noleap['trend'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a125a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ntr(ntr_data):\n",
    "\n",
    "    rec_length = (ntr_data['time'].max() - ntr_data['time'].min()).days\n",
    "    \n",
    "    \n",
    "    ntr_decadal, ntr_highFreq = butterworth_lowpass(ntr_data['ntr'], time_diffs, 1/decadal, order=3) \n",
    "   \n",
    "    #interannual\n",
    "    ntr_interannual, ntr_highFreq = butterworth_lowpass(ntr_highFreq, time_diffs, 1/annual, order=3)\n",
    "\n",
    "    # intraannual\n",
    "    ntr_subannual, ntr_highFreq = butterworth_lowpass(ntr_highFreq, time_diffs, 1/monthly, order=5)\n",
    "\n",
    "    ntr_monthly, ntr_highFreq = butterworth_lowpass(ntr_highFreq, time_diffs, 1/weekly, order=5)\n",
    "\n",
    "    # Remove high frequencies (weekly to hourly)\n",
    "    # ntr_weekly is timescales longer than 7 days but less than 1 month\n",
    "    ntr_weekly, ntr_highFreq = butterworth_lowpass(ntr_highFreq, time_diffs, 1/daily, order=5)\n",
    "\n",
    "    rank_tide = ntr_data['tide'].rank(method='first', ascending=False)\n",
    "\n",
    "    # make dataframe of filtered data\n",
    "    ntr_filtered = pd.DataFrame({'time': ntr_data['time'], \n",
    "                             'ntr': ntr_data['ntr'], \n",
    "                             'sea_level': ntr_data['sea_level'],\n",
    "                             'sea_level_detrended': ntr_data['sea_level_detrended'],\n",
    "                             'tide': ntr_data['tide'],\n",
    "                             'Trend': ntr_data['trend'],\n",
    "                            #  'Interdecadal': ntr_interdecadal, \n",
    "                             'Decadal': ntr_decadal, \n",
    "                             'Interannual': ntr_interannual, \n",
    "                             'Seasonal': ntr_data['seasonal_cycle'], \n",
    "                             'Subannual': ntr_subannual, \n",
    "                             'Monthly': ntr_monthly,\n",
    "                             'Weekly': ntr_weekly, \n",
    "                             'Storms & HF': ntr_highFreq,\n",
    "                             'Rank Tide': rank_tide } )\n",
    "                        #    'NTR Trend': ntr_trend_series['ntr']\n",
    "    \n",
    "\n",
    "    component_names = list(ntr_filtered.columns) \n",
    "    component_names.remove('time')\n",
    "    component_names.remove('ntr')\n",
    "    component_names.remove('sea_level')\n",
    "    component_names.remove('sea_level_detrended')\n",
    "    component_names.remove('Rank Tide')\n",
    "\n",
    "    # add trend back into ntr\n",
    "    # ntr_filtered['ntr'] = ntr_filtered['ntr'] + ntr_filtered['NTR Trend']\n",
    "    \n",
    "    return ntr_filtered, component_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diffs = np.diff(ntr_data['time']).astype('timedelta64[h]').astype(int)\n",
    "\n",
    "# Define timescales (in days)\n",
    "annual = 365.25\n",
    "biannual = 365.25*2\n",
    "semiannual = 365.25/2\n",
    "qtrannual = 365.25/4\n",
    "daily = 7\n",
    "weekly = 365.25/12\n",
    "monthly = 3*365.25/12\n",
    "decadal = 7*365.25\n",
    "\n",
    "ntr_filtered, component_names = filter_ntr(ntr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove leap days for day of year analysis\n",
    "ntr_filtered = ntr_filtered[~((ntr_filtered['time'].dt.month == 2) & (ntr_filtered['time'].dt.day == 29))].copy()\n",
    "\n",
    "ntr_filtered['doy'] = ntr_filtered['time'].dt.dayofyear\n",
    "\n",
    "ntr_filtered_daily = ntr_filtered.groupby('doy').quantile(0.95).reset_index()\n",
    "short_term_components = ['Storms & HF', 'Weekly', 'Monthly', 'Subannual']\n",
    "ntr_filtered_daily['short_term'] = ntr_filtered_daily[short_term_components].sum(axis=1)\n",
    "\n",
    "# confidence intervals for short term variability\n",
    "ntr_filtered_daily['short_term_std'] = (ntr_filtered_daily[short_term_components].std(axis=1))\n",
    "ntr_filtered_daily['short_term_upper'] = (ntr_filtered_daily['short_term'] + 1.96 * ntr_filtered_daily['short_term_std'])\n",
    "ntr_filtered_daily['short_term_lower'] = (ntr_filtered_daily['short_term'] - 1.96 * ntr_filtered_daily['short_term_std'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "total_components = ['ntr', 'tide']\n",
    "ntr_filtered_daily['ntr+tide'] = ntr_filtered_daily[total_components].sum(axis=1)\n",
    "# daily_max_daily = daily_max.groupby('doy').median().reset_index()\n",
    "\n",
    "\n",
    "# confidence intervals for ntr+tide\n",
    "ntr_filtered_daily['ntr_std'] = ntr_filtered_daily['ntr+tide'].std(axis=1)\n",
    "ntr_filtered_daily['ntr+tide_upper'] = (ntr_filtered_daily['ntr+tide'] + 1.96 * ntr_filtered_daily['ntr_std'])\n",
    "ntr_filtered_daily['ntr+tide_lower'] = (ntr_filtered_daily['ntr+tide'] - 1.96 * ntr_filtered_daily['ntr_std'])\n",
    "\n",
    "def smooth_30_day(series):\n",
    "    return series.rolling(window=30, center=True, min_periods=2).mean()\n",
    "\n",
    "\n",
    "daily_max_smooth = daily_max.copy()\n",
    "ntr_filtered_daily_smooth = ntr_filtered_daily.copy()\n",
    "# Apply rolling mean to all relevant columns in one step\n",
    "columns_to_smooth = ['ntr+tide', 'sea_level', 'tide','ntr+tide_upper', 'ntr+tide_lower']\n",
    "daily_max_smooth[columns_to_smooth] = daily_max_smooth[columns_to_smooth].rolling(window=30, center=True, min_periods=2).mean()\n",
    "columns_to_smooth = ['sea_level', 'tide', 'short_term', 'short_term_upper', 'short_term_lower']\n",
    "\n",
    "ntr_filtered_daily_smooth[columns_to_smooth] = ntr_filtered_daily[columns_to_smooth].rolling(window=30, center=True, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee027cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ntr_std\n",
    "daily_max_daily['ntr_std'].plot()\n",
    "daily_max_daily['ntr+tide'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37773b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_percentile(data, percentile=95, n_bootstrap=1000, ci=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval for a given percentile using bootstrapping.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): The data to calculate the percentile for.\n",
    "        percentile (float): The percentile to calculate (default is 95).\n",
    "        n_bootstrap (int): Number of bootstrap resamples (default is 1000).\n",
    "        ci (float): Confidence level for the interval (default is 0.95).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (percentile_value, lower_bound, upper_bound)\n",
    "    \"\"\"\n",
    "    bootstrapped_percentiles = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        resample = np.random.choice(data, size=len(data), replace=True)\n",
    "        bootstrapped_percentiles.append(np.percentile(resample, percentile))\n",
    "    \n",
    "    # Calculate the confidence interval bounds\n",
    "    lower_bound = np.percentile(bootstrapped_percentiles, (1 - ci) / 2 * 100)\n",
    "    upper_bound = np.percentile(bootstrapped_percentiles, (1 + ci) / 2 * 100)\n",
    "    return np.percentile(data, percentile), lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'doy' and calculate the 99th percentile with confidence intervals\n",
    "def calculate_percentile_with_ci(data):\n",
    "    # expects data to be of the form:\n",
    "    # data = group['sea_level'].dropna()\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        p99, lower, upper = bootstrap_percentile(data, percentile=99, n_bootstrap=1000, ci=0.95)\n",
    "        return pd.Series({'sea_level_99th': p99, 'ci_lower': lower, 'ci_upper': upper})\n",
    "    else:\n",
    "        return pd.Series({'sea_level_99th': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a8629",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c79779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leap days\n",
    "ntr_filtered = ntr_filtered[~((ntr_filtered['time'].dt.month == 2) & (ntr_filtered['time'].dt.day == 29))].copy()\n",
    "ntr_filtered['doy'] = ntr_filtered['time'].dt.dayofyear\n",
    "\n",
    "# # Calculate short_term as the sum of components\n",
    "short_term_components = ['Storms & HF', 'Weekly', 'Monthly', 'Subannual']\n",
    "ntr_filtered['short_term'] = ntr_filtered[short_term_components].sum(axis=1)\n",
    "\n",
    "# group by day\n",
    "# ntr_filtered_95 = ntr_filtered.groupby('doy').quantile(0.95).reset_index()\n",
    "# ntr_filtered_max = ntr_filtered.groupby('doy').max().reset_index()\n",
    "\n",
    "ntr_filtered_daily = ntr_filtered.groupby('doy').agg(\n",
    "    sea_level_mean=('detrended_sea_level', 'mean'),\n",
    "    sea_level_std=('detrended_sea_level', 'std'),\n",
    "    sea_level_95th=('detrended_sea_level', lambda x: np.percentile(x, 95)),\n",
    "    sea_level_max=('detrended_sea_level', 'max'),\n",
    "    tide_95th=('tide', lambda x: np.percentile(x, 95)),\n",
    "    Seasonal=('ntr',lambda x: np.percentile(x, 95)),\n",
    "    short_term_95th=('short_term', lambda x: np.percentile(x, 95)),\n",
    "    short_term_std=('short_term', 'std')\n",
    ").reset_index()\n",
    "\n",
    "sea_level_daily_99 = ntr_filtered.groupby('doy').apply(calculate_percentile_with_ci).reset_index()\n",
    "columns_to_smooth = ['sea_level_99th', 'ci_lower', 'ci_upper']\n",
    "sea_level_daily_99[columns_to_smooth] = sea_level_daily_99[columns_to_smooth].rolling(\n",
    "    window=30, center=True, min_periods=1\n",
    ").mean()\n",
    "\n",
    "# Smooth the daily data with a 30-day rolling mean (like a low-pass filter)\n",
    "columns_to_smooth = ['detrended_sea_level_max', 'detrended_sea_level_std', 'detrended_sea_level_95th','tide_95th','short_term_95th','short_term_std','Seasonal']\n",
    "ntr_filtered_daily[columns_to_smooth] = ntr_filtered_daily[columns_to_smooth].rolling(window=30, center=True, min_periods=1).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf34096",
   "metadata": {},
   "outputs": [],
   "source": [
    "msl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_level_daily_99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2055c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# Plot ntr+tide\n",
    "sns.lineplot(data=sea_level_daily_99-msl, x='doy', y='sea_level_99th', ax=ax, label='total', color='black')\n",
    "ax.fill_between(\n",
    "    sea_level_daily_99['doy'],\n",
    "    sea_level_daily_99['ci_upper']-msl,\n",
    "    sea_level_daily_99['ci_lower']-msl,\n",
    "    color='gray',\n",
    "    alpha=0.2,\n",
    "    label='total CI'\n",
    ")\n",
    "\n",
    "# Plot short_term\n",
    "sns.lineplot(data=ntr_filtered_daily, x='doy', y='short_term_95th', ax=ax, label='short-term', color='red')\n",
    "ax.fill_between(\n",
    "    ntr_filtered_daily['doy'],\n",
    "    ntr_filtered_daily['short_term_95th'] - ntr_filtered_daily['short_term_std'],\n",
    "    ntr_filtered_daily['short_term_95th'] + ntr_filtered_daily['short_term_std'],\n",
    "    color='red',\n",
    "    alpha=0.2,\n",
    "    label='short-term CI'\n",
    ")\n",
    "\n",
    "\n",
    "sns.lineplot(data=ntr_filtered_daily, x='doy', y='Seasonal', ax=ax, label='seasonal', color='green')\n",
    "sns.lineplot(data=ntr_filtered_daily, x='doy', y='tide_95th', ax=ax, label='tide (95th percentile)', color='blue')\n",
    "\n",
    "# sns.lineplot(tp98, ax=ax, label='tide + trend 98th percentile', color='orange')\n",
    "ax.set_title(f'High water climatology at {station_name} ({station_id})')\n",
    "ax.set_ylabel('meters')\n",
    "ax.set_xlabel('Day of Year')\n",
    "\n",
    "# set xlim to 1-365\n",
    "ax.set_xlim(1, 365)\n",
    "\n",
    "# replace numbers with month names\n",
    "month_starts = [1, 32, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335]\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "ax.set_xticks(month_starts)\n",
    "ax.set_xticklabels(month_names)\n",
    "\n",
    "# now add top 10 events at station\n",
    "top_events = daily_max.nlargest(10, 'sea_level')\n",
    "msl = ds.MSL.sel(station_id=station_id).item()\n",
    "\n",
    "top_events['doy'] = top_events['date'].dt.dayofyear\n",
    "\n",
    "ax.scatter(top_events['doy'], top_events['sea_level']-msl, color='black', zorder=5, label='Top 10 Events')\n",
    "for i, row in top_events.iterrows():\n",
    "    ax.text(row['doy'], row['sea_level']-msl+0.03, f\"{row['date']:%Y-%m-%d}\", color='black', ha='center', rotation=90)\n",
    "\n",
    "ax.set_title('High Water Climatology')\n",
    "ax.set_ylabel('Meters, rel. to MSL')\n",
    "ax.set_xlim(1, 365)\n",
    "ax.set_ylim(-0.1, 1)\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.2), ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359ef1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLI311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
