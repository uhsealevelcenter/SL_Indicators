{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonstationary Extremes\n",
    "\n",
    "```{glue:figure} TimeDependentReturnValue\n",
    ":scale: 30%\n",
    ":align: right\n",
    "```\n",
    "\n",
    "As with our stationary annual extremes, for the nonstationary extremes we'll be using the Generalized Extreme Value (GEV) distribution. This time, however, we allow for a time-dependent distribution, meaning that our extremes will vary from year to year (see figure). The GEV is a family of continuous probability distributions developed to combine the Gumbel, Fréchet, and Weibull families, also known as type I, II, and III extreme value distributions, respectively. It is widely used in different fields, particularly for modeling the largest or smallest values among a large set of independent, identically distributed random values (extreme value analysis).\n",
    "\n",
    "\n",
    "### Cumulative Distribution Function (CDF)\n",
    "\n",
    "The cumulative distribution function of the GEV, which is used to calculate return periods, is given by:\n",
    "\n",
    "\\begin{align}\n",
    "F(x; \\mu, \\sigma, \\xi) = \\begin{cases} \n",
    "\\exp\\left(-\\left(1 + \\xi \\frac{x - \\mu}{\\psi}\\right)^{-\\frac{1}{\\xi}}\\right) & \\text{if } \\xi \\neq 0, \\; 1 + \\xi \\frac{x - \\mu}{\\psi} > 0 \\\\\n",
    "\\exp\\left(-\\exp\\left(-\\frac{x - \\mu}{\\psi}\\right)\\right) & \\text{if } \\xi = 0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "- $\\mu $ (location parameter) determines the center of the distribution.\n",
    "- $ \\psi $ (scale parameter) is strictly positive and scales the distribution.\n",
    "- $ \\xi $ (shape parameter) describes the tail behavior of the distribution. It can be positive (heavy tail, Fréchet), zero (exponential tail, Gumbel), or negative (bounded tail, Weibull).\n",
    "\n",
    "Assuming we are not dealing with a random stationary process (as with our stationary annual extremes), each parameter in the above equation can be the combination of one or more time-dependent components. These effects could include annual variability (seasonality), sea level rise (long-term trend and possible acceleration), a long-term trend for the extreme values, long-term climate variability effects explained by climate indices or the 18.61-yr nodal cycle.\n",
    "\n",
    "The sum total of these covariates on each parameter are:\n",
    "\\begin{align}\n",
    "\\mu(t) &= \\mu_{SLR}(t) + \\mu_S(t)exp^{[\\mu_{LT}(t)]} + \\mu_N(t) + \\mu_{CLI}(t),\\\\\n",
    "\\psi_t(t) &= \\psi_S(t)exp^{[\\psi_{LT}(t)]} + \\psi_N(t) + \\psi_{CLI},\\\\\n",
    "\\xi_t(t) &= \\xi_S(t)exp^{[\\xi_{LT}(t)]}\n",
    "\\end{align}\n",
    "\n",
    "Note this includes a nonlinear component to allow for a long-term (LT) variability in the seasonality (S) in the location and scale parameters. The following codes will not include some of these terms - this is a general equation!\n",
    "\n",
    "\n",
    "This example script explores the following steps:\n",
    "- step1: seasonal pattern of extreme events (seasonality on location, scale, and shape parameters remain constant)\n",
    "- step2: long-term trends in location parameter (linear and a possible acceleration)\n",
    "- step3: Checking Covariate (BEST index) in the GEV location parameter\n",
    "- step4: Checking Covariate (BEST index) in the GEV scale parameter\n",
    "- step5: Checking Nodal cycle (18.6 year period, waves) in the GEV location parameter\n",
    "\n",
    "Original Author: Melisa Menendez, menendezm@unican.es\n",
    "\n",
    "Updated for Billy & Ayesha SERDP work: 09-March-2017 (author: Ayesha Genz)\n",
    "\n",
    "Updated for Python with some added explanation: July 2024 (author: Julia Fiedler)\n",
    "\n",
    "Further explanation of this code (and writeup) can be found in {cite:t}`mendez_analyzing_2007`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll import everything from our usual setup notebook, and then we'll load up the special scripts we'll need for doing the nonstationary GEV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.ipynb\n",
    "import sys\n",
    "sys.path.append(\"../python/nonstationaryGEV\")\n",
    "import helpers, models, plotting, imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions\n",
    "All of the functions for computing the nonstationary GEV currently live in the nonstationary GEV directory.\n",
    "\n",
    "### Step-wise solver\n",
    "This function iteratively improves the model solution by choosing which parameters to use and evaluating if the change leads to a statistically significant improvement using a $\\chi^2$ metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation\n",
    "\n",
    "Since we are using monthly data, we can define our location parameter as a harmonic series to represent seasonal effects, like so:\n",
    "\n",
    "```{margin}\n",
    "```{note}\n",
    "Note that some portions of the original code have a dyadic series, so while our normal series would have n going from 1 to $\\infty$ we will have $n=1,2,4$, representing the annual cycle, the seasonal cycle, and the quarterly cycle. (We neglect a tri-annual cycle.)\n",
    "```\n",
    "\n",
    "$$\n",
    "f(t) = A_0 + \\sum_{n=1}^{\\infty} \\left( A_n \\cos(2n \\pi t) + B_n \\sin(2n \\pi t) \\right),\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "For example, if we can describe our location, scale and shape parameters in the GEV as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mu(t) &= \\beta_{SLR}(t) + [\\beta_0 + \\beta_1 cos(2\\pi t) + \\beta_2 sin(2\\pi t) + \\beta_3 cos(4\\pi t) + \\beta_4 sin(4\\pi t)]e^{[\\beta_{LT}(t)]}\\\\\\\n",
    " &+ \\beta_{N_1} cos(2\\pi t/T_N) + \\beta_{N_2} sin(2\\pi t/T_N) + \\beta_{PMM}PMM(t),\\\\\n",
    "\\psi(t) &= [\\alpha_0 + \\alpha_1 cos(2\\pi t) + \\alpha_2 sin(2\\pi t)]e^{[\\alpha_{LT}(t)]} \\\\\n",
    "\\xi(t) &=  [\\gamma_0 + \\gamma_1 cos(2\\pi t) + \\gamma_2 sin(2\\pi t)]\n",
    "\\end{align}\n",
    "\n",
    "then the vector we'd use for our MLE fit should be\n",
    "\n",
    "$\\theta = (\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\beta_4,\\beta_{N_1},\\beta_{N_2},\\beta_{LT},\\beta_{BEST},\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_{LT},\\gamma_0,\\gamma_1,\\gamma_2)$.\n",
    "\n",
    "Here, $\\beta_{0,1,2,3,4}$ model the seasonal variation of the location parameter, $\\beta_{N1,N2}$ are the amplitudes of the nodal cycle, $\\beta_{LT}$ is the long term trend of the location parameter, and $\\beta_{PMM}$ is the amplitude (relative influence) of the [PMM] climate index in the location parameter. Similarly, $\\alpha_{0,1,2}$ model the [cyclical] variation of the scale parameter, and $\\alpha_{LT}$ models its long-term trend. For the shape parameter, only the seasonal variation is modeled ($\\gamma_{0,1,2}$).\n",
    "\n",
    "We'll use this vector $\\theta$ to calculate the maximum likelihood function $L$,\n",
    "\n",
    "$L(\\theta | t_i,z_i) = \\prod_{i=1}^m g[z_i;\\mu(t_i),\\psi(t_i),\\xi(t_i)]$,\n",
    "\n",
    "where $g$ is the PDF of the GEV (equation 1). In other words, this means that the likelihood of observing the entire dataset is the product of the likelihoods of observing each individual time point under the assumed GEV model (Maximum Likelihood Estimation, MLE).\n",
    "\n",
    "\n",
    "### Parameter Search\n",
    "\n",
    "\n",
    "The code we use below takes a varying number of parameters (a differently sized vector of regression parameters) for different models. There are three models we can run: GEV_SeasonalMu, GEV_S_T_Cv, or GEV_S_T_Cv_Nodal. Because there are so many parameters to estimate (a high-dimensional problem), a Shuffled Complex Evolution (SCE-UA,\\cite{Duan}) optimization scheme is used to determine the best parameter fits. The SCE-UA is used often in hydrology, engineering, and other environmental sciences, using random clusters for initial starting points in its search algorithm and allowing them to \"evolve\" to a better solution. Other optimization schemes exist, but for now this is what we're using.\n",
    "\n",
    "\n",
    "#### Seasonality in Location Model\n",
    "\n",
    "Our first step is to look at seasonality only, using the GEV_SeasonalMu model, such that $\\mu(t)$, $\\psi(t)$, and $\\xi(t)$ are:\n",
    "\n",
    "\\begin{align}\n",
    "\\mu(t) &=  \\beta_0 + \\beta_1 cos(2\\pi t) + \\beta_2 sin(2\\pi t) + \\\\\n",
    "&\\beta_3 cos(4\\pi t) + \\beta_4 sin(4\\pi t) + \\beta_5 cos(8\\pi t) + \\beta_6 sin(8\\pi t),\\\\\n",
    "\\psi &= \\alpha_0, \\\\\n",
    "\\xi &=  \\gamma_0 \n",
    "\\end{align}\n",
    "\n",
    "and we'll arrange our vector of 9 regression parameters within the fitness function like so:\n",
    "\n",
    "$\\theta = [\\beta_0, \\alpha_0, \\gamma_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5, \\beta_6].$\n",
    "\n",
    "In this way we assume that only the location parameter changes within a year, whereas the shape and scale of the GEV distribution are constant.\n",
    "\n",
    "#### Seasonality, Trend, and Covariate Model\n",
    "For the GEV_S_T_Cv model , we'll allow a long-term trend into the location parameter and a covariate into the scale and location parameter:\n",
    "```{margin}\n",
    "Note this trend can be approximated as linear if our long-term trend parameter $\\beta_{LT}$ is small. \n",
    "\n",
    "Mathmematically, as $\\beta_{LT}$ goes to zero, $e^{\\beta_{LT}·t}$ goes to $1 + \\beta_{LT}·t$ and our expression (for the non-cyclical part of the location parameter) simplifies to $\\beta_0 (1 +  \\beta_{LT}·t)$. \n",
    "\n",
    "That is the equation of a line: $\\beta_0 + \\beta_0·\\beta_{LT}·t$, where slope of the line is $\\beta_0·\\beta_{LT}$. \n",
    "```\n",
    "\n",
    "\\begin{align}\n",
    "\\mu(t) &= \\beta_0e^{\\beta_{LT}(t)}  + \\beta_1 cos(2\\pi t) + \\beta_2 sin(2\\pi t) + \\beta_3 cos(4\\pi t) + \\beta_4 sin(4\\pi t) \\\\\n",
    "&+ \\beta_5 cos(8\\pi t) + \\beta_6 sin(8\\pi t)   + \\beta_{PMM}PMM(t),\\\\\n",
    "\\psi &= \\alpha_0 + \\alpha_{PMM}PMM(t), \\\\\n",
    "\\xi &=  \\gamma_0 \n",
    "\\end{align}\n",
    "\n",
    "and we'll arrange our vector of 12 regression parameters within the fitness function like so:\n",
    "\n",
    "$\\theta = [\\beta_0, \\alpha_0, \\gamma_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5, \\beta_6,\\beta_{LT},\\beta_{PMM},\\alpha_{PMM}].$\n",
    "\n",
    "#### Seasonality, Trend, Covariate and Nodal cycle Model\n",
    "For the GEV_S_T_Cv_N model , we'll add in the nodal cycle:\n",
    "\n",
    "\\begin{align}\n",
    "\\mu(t) &= \\beta_0 e^{\\beta_{LT}(t)} + \\beta_1 cos(2\\pi t) + \\beta_2 sin(2\\pi t) + \\beta_3 cos(4\\pi t) + \\beta_4 sin(4\\pi t) \\\\\n",
    "&+ \\beta_5 cos(8\\pi t) + \\beta_6 sin(8\\pi t) + \\beta_{PMM}PMM(t) + \\beta_{N_1} cos(2\\pi t/T_N) + \\beta_{N_2} sin(2\\pi t/T_N) ,\\\\\n",
    "\\psi &= \\alpha_0 + \\alpha_{PMM}PMM(t), \\\\\n",
    "\\xi &=  \\gamma_0 \n",
    "\\end{align}\n",
    "\n",
    "and we'll arrange our vector of 16 regression parameters within the fitness function like so:\n",
    "\n",
    "$\\theta = [\\beta_0, \\alpha_0, \\gamma_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5, \\beta_6,\\beta_{LT},\\beta_{PMM},\\alpha_{PMM},\\beta_{N1},\\beta_{N2}].$\n",
    "\n",
    "### Input to the fitness function\n",
    "Note that the input \"x\" to the fitness function works like a switch - if it is a parameter to include, it is 1 (True). If it is not used, it is 0 (False). It is at most 7 elements long (in this incarnation!)\n",
    "\n",
    "- x: [1/0 1/0 1/0 1/0 1/0 1/0 1/0]\n",
    "- x[0]: annual cycle, location parameter.\n",
    "- x[1]: semi-annual cycle, location parameter.\n",
    "- x[2]: quarterly cycle, location parameter.\n",
    "- x[3]: Trend, location parameter.\n",
    "- x[4]: Covariate, location parameter.\n",
    "- x[5]: Covariate, scale parameter.\n",
    "- x[6]: Nodal cycle, location parameter\n",
    "- Max number of parameters: 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-output",
     "hide-input"
    ]
   },
   "source": [
    "## Format data input to the model\n",
    "Now we'll take our sea level data, extract the monthly maxima from our timeseries, and format it for input into the model. We need the yearday of each monthly maximum, and if using a covariate model (e.g. SOI, ENSO, wave height), the yearday of that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "base_dir = Path(data_dir).parent\n",
    "dirs = imports.make_directoryDict(base_dir)\n",
    "\n",
    "rsl_hourly = xr.open_dataset(data_dir/ 'rsl_hawaii_noaa.nc')\n",
    "rsl_hourly['station_name'] = rsl_hourly['station_name'].astype(str)\n",
    "\n",
    "runWithoutModel = True # setting this to be True means that we won't have to re-run the model every single time.\n",
    "# check if output directory exists, if not create it\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True)\n",
    "\n",
    "# check if all directories in dirs exist, if not create it\n",
    "for dir in dirs.values():\n",
    "    if not dir.exists():\n",
    "        dir.mkdir(parents=True)\n",
    "\n",
    "# make sub-directories for each station in rsl_hourly in model_output_dir\n",
    "for sid in rsl_hourly.station_id:\n",
    "    sidString = str(sid.values)\n",
    "    # maybe needs some leading zeros, for now we'll leave it.\n",
    "    station_dir = dirs['model_output_dir'] / sidString\n",
    "    if not station_dir.exists():\n",
    "        station_dir.mkdir(parents=True)\n",
    "\n",
    "# remove best.txt, T.txt, CI.txt, Y.txt, mio.txt, scein.dat, and sceout.dat from working directory\n",
    "for file in ['best.txt', 'T.txt', 'CI.txt', 'Y.txt', 'mio.txt', 'scein.dat', 'sceout.dat']:\n",
    "    if (Path(file)).exists():\n",
    "        (Path(file)).unlink()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the station names that correspond to order of the data, because I keep forgetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hilo, Hilo Bay, Kuhio Bay', 'Sand Island, Midway Islands',\n",
       "       'Kawaihae', 'Mokuoloe', 'Kahului, Kahului Harbor', 'Honolulu',\n",
       "       'Nawiliwili'], dtype='<U27')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsl_hourly.station_name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = rsl_hourly.station_id.values\n",
    "runWithoutModel = True\n",
    "return_period = [2,10,50,100]\n",
    "year0plot = 1993\n",
    "saveToFile = True\n",
    "numProcesses = 8 # number of processes to run in parallel, select 1 if you want to run in serial\n",
    "climateIndex = ['PMM','BEST','ONI','PDO','AO','PNA','TNA'] #add more, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIname = climateIndex[0]  # We'll run this for PMM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following script to make a csv of peak correlations of climate indices and the monthly maxima. These peak correlations exist at certain time lags from each other, i.e. monthly max sea levels in Hawaii tend to correlate well with Niño3, offset by 19 months {\\cite:t}`long_hawaii_2020`.  The code will produce two files: one has best-fit lags, the other has some of the lags set to certain months. (If you are not running the Hawaii stations you may need to adjust this to your region, based on known literature.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /Users/jfiedler/Documents/SL_Hawaii_data\n"
     ]
    }
   ],
   "source": [
    "%run get_CI_lags.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell preps the model input data by extracting the monthly maximum time series from the given NOAA tide gauge. If a climate covariate is given (CIname), it will create a covariate timeseries from data we've previously downloaded in the [data wrangling](notebooks/SL_Data_Wrangling.ipynb) notebook and processed via the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_dir': PosixPath('/Users/jfiedler/Documents/SL_Hawaii_data/data'),\n",
       " 'output_dir': PosixPath('/Users/jfiedler/Library/CloudStorage/GoogleDrive-jfiedler@hawaii.edu/Shared drives/PI Indicators 2025/SL Indicators/Hawaii_Region_Output/extremes'),\n",
       " 'input_dir': PosixPath('/Users/jfiedler/Documents/SL_Hawaii_data/model_input'),\n",
       " 'matrix_dir': PosixPath('/Users/jfiedler/Documents/SL_Hawaii_data/matrix'),\n",
       " 'model_output_dir': PosixPath('/Users/jfiedler/Documents/SL_Hawaii_data/data/GEV_model_output'),\n",
       " 'CI_dir': PosixPath('/Users/jfiedler/Documents/SL_Hawaii_data/data/climate_indices'),\n",
       " 'run_dir': PosixPath('/Users/jfiedler/Documents/SL_Hawaii_data/data/model_run')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for model input in  /Users/jfiedler/Documents/SL_Hawaii_data/data/model_run\n"
     ]
    }
   ],
   "source": [
    "stationID = station_ids[0]\n",
    "STNDtoMHHW, station_name, year0, mm = models.prep_model_input_data(rsl_hourly,stationID,dirs, CIname,lag=False)\n",
    "\n",
    "# check for limits file\n",
    "limitsPath = dirs['run_dir'] / 'limits.txt'\n",
    "if not limitsPath.exists():\n",
    "    imports.define_limits(dirs['run_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we've got. It should be t (time, in decimal months starting at year0, which we established a few cells ago), the monthly max (in meters, relative to station datum STND), the hour in which that monthly max occured in datetime format, and the climate covariate for that decimal month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "t",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "monthly_max",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "t_monthly_max",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "CI",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ef8b1e42-3369-4355-9ce4-4bc8406b1c19",
       "rows": [
        [
         "0",
         "0.02333788706739526",
         "2.252",
         "1993-01-08 13:00:00",
         "-0.67928820642344"
        ],
        [
         "1",
         "0.10257285974499089",
         "2.298",
         "1993-02-06 13:00:00",
         "-0.005294596618046765"
        ],
        [
         "2",
         "0.17896174863387979",
         "2.063",
         "1993-03-06 12:00:00",
         "0.9466653356348034"
        ],
        [
         "3",
         "0.2679872495446266",
         "2.192",
         "1993-04-08 02:00:00",
         "1.4936313249668962"
        ],
        [
         "4",
         "0.350068306010929",
         "2.198",
         "1993-05-08 03:00:00",
         "1.4951257675606997"
        ],
        [
         "5",
         "0.4701730418943534",
         "2.188",
         "1993-06-21 02:00:00",
         "1.3725814748688099"
        ],
        [
         "6",
         "0.549408014571949",
         "2.198",
         "1993-07-20 02:00:00",
         "1.5399590453748058"
        ],
        [
         "7",
         "0.6257969034608378",
         "2.225",
         "1993-08-17 01:00:00",
         "1.5892756509703223"
        ],
        [
         "8",
         "0.7049180327868853",
         "2.106",
         "1993-09-15 00:00:00",
         "1.3695925896812031"
        ],
        [
         "9",
         "0.7912112932604736",
         "2.179",
         "1993-10-16 14:00:00",
         "1.2171594451132428"
        ],
        [
         "10",
         "0.870332422586521",
         "2.131",
         "1993-11-14 13:00:00",
         "1.1050762505779779"
        ],
        [
         "11",
         "0.9468351548269581",
         "2.201",
         "1993-12-12 13:00:00",
         "0.743421142877523"
        ],
        [
         "12",
         "1.078096539162113",
         "2.161",
         "1994-01-28 14:00:00",
         "0.1277107942304676"
        ],
        [
         "13",
         "1.1107695810564664",
         "2.045",
         "1994-02-09 13:00:00",
         "-0.292227574628325"
        ],
        [
         "14",
         "1.2308743169398908",
         "2.057",
         "1994-03-25 12:00:00",
         "-0.1054222504028835"
        ],
        [
         "15",
         "1.3198998178506376",
         "2.106",
         "1994-04-27 02:00:00",
         "0.29359392214265967"
        ],
        [
         "16",
         "1.3962887067395264",
         "2.109",
         "1994-05-25 01:00:00",
         "0.5551213760582778"
        ],
        [
         "17",
         "1.4726775956284153",
         "2.07",
         "1994-06-22 00:00:00",
         "0.5401769501202425"
        ],
        [
         "18",
         "1.560336976320583",
         "2.112",
         "1994-07-24 02:00:00",
         "0.5954713260909732"
        ],
        [
         "19",
         "1.633879781420765",
         "2.112",
         "1994-08-20 00:00:00",
         "0.903326500414501"
        ],
        [
         "20",
         "1.674863387978142",
         "1.984",
         "1994-09-04 00:00:00",
         "0.928732024509161"
        ],
        [
         "21",
         "1.7667349726775956",
         "2.121",
         "1994-10-07 15:00:00",
         "0.8674598781632161"
        ],
        [
         "22",
         "1.8459699453551912",
         "2.039",
         "1994-11-05 15:00:00",
         "0.8928654022578761"
        ],
        [
         "23",
         "1.9166666666666665",
         "2.1",
         "1994-12-01 12:00:00",
         "0.9944874986365164"
        ],
        [
         "24",
         "2.0834471766848814",
         "2.124",
         "1995-01-30 13:00:00",
         "1.2156650025194393"
        ],
        [
         "25",
         "2.124430783242259",
         "2.12",
         "1995-02-14 13:00:00",
         "0.9526431060100174"
        ],
        [
         "26",
         "2.165414389799636",
         "2.041",
         "1995-03-01 13:00:00",
         "0.5835157853405449"
        ],
        [
         "27",
         "2.325250455373406",
         "2.031",
         "1995-04-29 01:00:00",
         "1.0811651690771213"
        ],
        [
         "28",
         "2.3746584699453552",
         "2.077",
         "1995-05-17 03:00:00",
         "1.5803089954075014"
        ],
        [
         "29",
         "2.451047358834244",
         "2.157",
         "1995-06-14 02:00:00",
         "1.350164835961757"
        ],
        [
         "30",
         "2.53028233151184",
         "2.166",
         "1995-07-13 02:00:00",
         "0.9152820411649292"
        ],
        [
         "31",
         "2.6066712204007283",
         "2.067",
         "1995-08-10 01:00:00",
         "0.20243292392064427"
        ],
        [
         "32",
         "2.6857923497267757",
         "2.089",
         "1995-09-08 00:00:00",
         "-0.7734380898330624"
        ],
        [
         "33",
         "2.8186475409836067",
         "2.039",
         "1995-10-26 15:00:00",
         "-1.0827877067503937"
        ],
        [
         "34",
         "2.8921903460837886",
         "2.227",
         "1995-11-22 13:00:00",
         "-0.5283495044492832"
        ],
        [
         "35",
         "2.9770036429872495",
         "2.163",
         "1995-12-23 14:00:00",
         "0.6552490298431146"
        ],
        [
         "36",
         "3.0562386156648453",
         "2.195",
         "1996-01-20 14:00:00",
         "1.2201483303008498"
        ],
        [
         "37",
         "3.1353597449908923",
         "2.221",
         "1996-02-18 13:00:00",
         "1.203709461769011"
        ],
        [
         "38",
         "3.1763433515482697",
         "2.134",
         "1996-03-04 13:00:00",
         "1.1125484635469955"
        ],
        [
         "39",
         "3.300774134790528",
         "1.928",
         "1996-04-19 02:00:00",
         "0.6552490298431145"
        ],
        [
         "40",
         "3.341757741347905",
         "2.038",
         "1996-05-04 02:00:00",
         "0.6148990798104191"
        ],
        [
         "41",
         "3.49738160291439",
         "2.173",
         "1996-06-30 01:00:00",
         "0.8031988466296642"
        ],
        [
         "42",
         "3.5001138433515484",
         "2.212",
         "1996-07-01 01:00:00",
         "0.7583655688155583"
        ],
        [
         "43",
         "3.658469945355191",
         "2.175",
         "1996-08-28 00:00:00",
         "0.6970934224696135"
        ],
        [
         "44",
         "3.7321265938069215",
         "2.186",
         "1996-09-23 23:00:00",
         "1.0064430393869448"
        ],
        [
         "45",
         "3.823998178506375",
         "2.145",
         "1996-10-27 14:00:00",
         "1.0452985468258367"
        ],
        [
         "46",
         "3.8705601092896176",
         "2.205",
         "1996-11-13 15:00:00",
         "0.8629765503818057"
        ],
        [
         "47",
         "3.9823542805100183",
         "2.298",
         "1996-12-24 13:00:00",
         "0.7703211095659865"
        ],
        [
         "48",
         "4.023337887067395",
         "2.226",
         "1997-01-08 13:00:00",
         "0.72100450397047"
        ],
        [
         "49",
         "4.105305100182149",
         "2.181",
         "1997-02-07 13:00:00",
         "0.9436764504471962"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 380
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>monthly_max</th>\n",
       "      <th>t_monthly_max</th>\n",
       "      <th>CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023338</td>\n",
       "      <td>2.252</td>\n",
       "      <td>1993-01-08 13:00:00</td>\n",
       "      <td>-0.679288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.102573</td>\n",
       "      <td>2.298</td>\n",
       "      <td>1993-02-06 13:00:00</td>\n",
       "      <td>-0.005295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.178962</td>\n",
       "      <td>2.063</td>\n",
       "      <td>1993-03-06 12:00:00</td>\n",
       "      <td>0.946665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.267987</td>\n",
       "      <td>2.192</td>\n",
       "      <td>1993-04-08 02:00:00</td>\n",
       "      <td>1.493631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.350068</td>\n",
       "      <td>2.198</td>\n",
       "      <td>1993-05-08 03:00:00</td>\n",
       "      <td>1.495126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>31.636840</td>\n",
       "      <td>2.370</td>\n",
       "      <td>2024-08-20 02:00:00</td>\n",
       "      <td>-1.265110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>31.713115</td>\n",
       "      <td>2.257</td>\n",
       "      <td>2024-09-17 00:00:00</td>\n",
       "      <td>-1.145554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>31.799408</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2024-10-18 14:00:00</td>\n",
       "      <td>-1.049910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>31.878643</td>\n",
       "      <td>2.296</td>\n",
       "      <td>2024-11-16 14:00:00</td>\n",
       "      <td>-0.815282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>31.960610</td>\n",
       "      <td>2.294</td>\n",
       "      <td>2024-12-16 14:00:00</td>\n",
       "      <td>-0.595599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             t  monthly_max       t_monthly_max        CI\n",
       "0     0.023338        2.252 1993-01-08 13:00:00 -0.679288\n",
       "1     0.102573        2.298 1993-02-06 13:00:00 -0.005295\n",
       "2     0.178962        2.063 1993-03-06 12:00:00  0.946665\n",
       "3     0.267987        2.192 1993-04-08 02:00:00  1.493631\n",
       "4     0.350068        2.198 1993-05-08 03:00:00  1.495126\n",
       "..         ...          ...                 ...       ...\n",
       "375  31.636840        2.370 2024-08-20 02:00:00 -1.265110\n",
       "376  31.713115        2.257 2024-09-17 00:00:00 -1.145554\n",
       "377  31.799408        2.240 2024-10-18 14:00:00 -1.049910\n",
       "378  31.878643        2.296 2024-11-16 14:00:00 -0.815282\n",
       "379  31.960610        2.294 2024-12-16 14:00:00 -0.595599\n",
       "\n",
       "[380 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll make a dictionary called 'modelInfo', which will eventually go into our model output. In this case, we're still exploring seasonality, so we'll skip the covariate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# make dictionary of STNDtoMHHW, station_name, year0, mm,t,monthly_max,covariate\n",
    "#make dictionary of stuff that goes into xarray: t,covariate,standard_error,ReturnPeriod,modelName='Model_GEV_S_T_Cv_N.exe',ridString=ridString,savepath=savepath, station_name=station_name,year0=year0)\n",
    "modelInfo = {'t': mm['t'], \n",
    "             'monthlyMax': mm['monthly_max'],\n",
    "             'covariate': None, \n",
    "             'covariateName': CIname, \n",
    "             'stationID': stationID, \n",
    "             'station_name': station_name, \n",
    "             'year0': year0, \n",
    "             'ReturnPeriod': return_period,\n",
    "             'STNDtoMHHW': STNDtoMHHW,\n",
    "             'datum':'STND'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look: Modeling Seasonality\n",
    "Here we'll look at seasonality in the location parameter only. Note that you'll need to ensure you have the spotpy package in order to run the SCEUA algorithm (as set up here). Fun fact, this can take a while. Currently, despite nproc=8, it is running in serial. It takes 7 minutes on my (fast) machine, so...maybe it's time for you to go get another cup of coffee or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "x_s, w_s = models.run_seasonal_model(stationID, dirs,runWithoutModel=runWithoutModel, modelType='GEV_SeasonalMu', nproc=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract a timeseries of the time-dependent return values\n",
    "For each time in a timeseries, we'll evaluate the CDF of the time-dependent GEV at a given return period to get the expected return level. This first function evaluates the extreme value prediction over time for a given return period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Confidence Intervals\n",
    "\n",
    "The confidence intervals on the following codes are calculated with the derivative of the previous prediction, using finite differencing. We will get be getting the derivative for each parameter in $\\theta$, with the goal of solving the total variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have the derivative function, we can use it to calculate the variance of our estimated return level at each timestep by applying a first-order Taylor expansion, which assumes a locally linear relationship between the function and its parameters. The variance  $\\sigma^2$  at each timestep can be approximated as:\n",
    "\n",
    "\n",
    "$\\sigma^2 \\approx \\mathbf{J}^T \\cdot \\mathbf{Cov}(w) \\cdot \\mathbf{J}$\n",
    "\n",
    "\n",
    "where  $\\mathbf{J}$  is the Jacobian (the vector of partial derivatives with respect to the parameters), and  $\\mathbf{Cov}(w)$  is the variance-covariance matrix of the parameters. This approach propagates the uncertainties in the parameters into the uncertainty of the return level. The square root of the variance  $\\sigma$  gives the standard deviation, which quantifies the uncertainty at each timestep and serves as the basis for calculating confidence intervals.\n",
    "\n",
    "Note this equation is functionally equivalent to Coles (2001) Theorem 2.4 (the \"delta method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot monthly extremes and given year return period with the seasonal model\n",
    "\n",
    "\n",
    "```{margin}\n",
    "```{caution}\n",
    "Note that the original code for this plot has the dotted line much lower - the 2 year return, for example, runs through the middle of the data. This corresponds to a 2-month return level, and makes sense if you are looking at each month in isolation (every other year the value is exceeded). This is the case in Méndez et al 2007 (and following papers), and likely comes down to an interpretation of what this line is actually meant to portray. Accounting for the fact that you are sampling monthly data moves the within-a-year return level up to more closely align with the 'stationary' solid line (and the sinusoid formed by the seasonal cycle then varies about the stationary return level). (Julia's opinion here): because this plot is dimensionless (time goes from 0 to 1), we must normalize our data by removing the time component. In this case, the time element is the monthly sampling, so we must scale our probabilities accordingly by (1/12). \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "hide-output",
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/papermill.record/text/plain": "<Figure size 1500x750 with 1 Axes>"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "seasonal_extreme_variations"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figSeasonal, cmap,seaLevelS = plotting.plotExtremeSeasonality(mm['t'],mm['monthly_max'],x_s,w_s,stationID, STNDtoMHHW, dirs, station_name, ReturnPeriod=return_period,SampleRate=12,saveToFile=saveToFile)\n",
    "\n",
    "# change the yaxis to\n",
    "ax = figSeasonal.axes[0]\n",
    "ax.set_ylim(mm['monthly_max'].min()-STNDtoMHHW-0.1, mm['monthly_max'].max()-STNDtoMHHW+0.2)\n",
    "\n",
    "# save the figure\n",
    "savename = 'SeasonalExtremeVariations_'+ stationID +'.png'\n",
    "savedir = os.path.join(output_dir, savename)\n",
    "plt.savefig(savedir, dpi=250, bbox_inches='tight')\n",
    "\n",
    "glue(\"seasonal_extreme_variations\", figSeasonal, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} seasonal_extreme_variations\n",
    ":name: \"seasonal_extreme_variations\"\n",
    "A climatology of extreme sea levels at the {glue:text}`station_name` tide gauge. Monthly maxima (+), determined from hourly sea level data, are shown from 1993-2024. The solid line corresponds to projected sea level return levels at 2,10,50 and 100 years, using this 30-year chunk of data. This line uses a nonstationary GEV to take the seasonal variations of sea level into account. The 2, 10, 50 and 100-year return levels are shown to vary throughout the year (dashed line), with the lowest extremes occuring in March, and highest extremes peaking in July. No long-term trend is used in this model, and the scale and location parameters are held constant.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if we should include a long-term trend (mean sea level rise) in our location parameter\n",
    "I mean, it's probably statistically significant, but hey, we should test just in case. First, we'll make a function to save our modeled return levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Significance of Linear Trend: 100.00%\n",
      "Estimated Trend on monthly Maxima values is: 3.73 +/- 0.41 mm/year\n",
      "Model already saved to netcdf file.\n"
     ]
    }
   ],
   "source": [
    "x_T, w_T, SignifTrend = models.run_long_term_trend_model(x_s, w_s, stationID, dirs, modelInfo, runWithoutModel=runWithoutModel, modelType='GEV_S_T_Cv', nproc=numProcesses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the nodal and perigean cycles in Location parameter\n",
    "We will only include the nodal and perigean cycles in our final model if adding it significantly improves our model thus far. We will add it to the long-term trend model if the long-term trend is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trend is significant! \n",
      "Include long-term trend and nodal cycle in final model.\n",
      "Statistical Significance of adding Nodal cycle: 99.89%\n",
      "Model exists:  /Users/jfiedler/Documents/SL_Hawaii_data/data/GEV_model_output/1617760/RL_muN.nc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_N, w_N, wcomp, SignifN = models.run_nodal_model(x_T, w_T, x_s, w_s, SignifTrend, stationID, dirs, modelInfo, runWithoutModel=True, modelType='GEV_S_T_Cv_Nodal', nproc=numProcesses)\n",
    "x_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the covariate in the location parameter.\n",
    "Let's check if the covariate inclusion in the location parameter is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Significance of PMM in location param: 100.00%\n",
      "Model already saved to netcdf file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cvte1, w_cvte1, wcomp, SignifCvte1 = models.run_covariate_in_location_model(x_N, w_N, wcomp, stationID, SignifTrend, dirs, modelInfo, runWithoutModel=runWithoutModel, modelType='GEV_S_T_Cv_Nodal',saveModel=True, nproc=numProcesses)  \n",
    "x_cvte1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the covariate in scale parameter\n",
    "Let's check if the covariate inclusion in the scale parameter is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m x_cvte2, w_cvte2, wcomp, SignifCvte2 = \u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_covariate_in_scale_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cvte1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_cvte1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwcomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstationID\u001b[49m\u001b[43m,\u001b[49m\u001b[43mSignifCvte1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelInfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunWithoutModel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrunWithoutModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelType\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGEV_S_T_Cv_Nodal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msaveModel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnproc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumProcesses\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/SL_Hawaii/notebooks/../python/nonstationaryGEV/models.py:264\u001b[39m, in \u001b[36mrun_covariate_in_scale_model\u001b[39m\u001b[34m(x_cvte1, w_cvte1, wcomp, ridString, SignifCvte1, dirs, modelInfo, runWithoutModel, modelType, saveModel, nproc)\u001b[39m\n\u001b[32m    262\u001b[39m         w_cvte2, mio, standard_error = (np.array(output[key]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmio\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstandard_error\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     w_cvte2, mio, standard_error = \u001b[43mrun_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cvte2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnproc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m diffe = w_cvte2[\u001b[32m0\u001b[39m] - wcomp[\u001b[32m0\u001b[39m]\n\u001b[32m    268\u001b[39m p = \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/SL_Hawaii/notebooks/../python/nonstationaryGEV/models.py:41\u001b[39m, in \u001b[36mrun_fitness\u001b[39m\u001b[34m(x, dirs, modelType, nproc)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_fitness\u001b[39m(x, dirs, modelType, nproc):\n\u001b[32m     39\u001b[39m     remove_files(dirs)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[43mfitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnproc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     w = np.loadtxt(dirs[\u001b[33m'\u001b[39m\u001b[33mrun_dir\u001b[39m\u001b[33m'\u001b[39m] / \u001b[33m'\u001b[39m\u001b[33mbest.txt\u001b[39m\u001b[33m'\u001b[39m, dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m     43\u001b[39m     mio = np.loadtxt(dirs[\u001b[33m'\u001b[39m\u001b[33mrun_dir\u001b[39m\u001b[33m'\u001b[39m] / \u001b[33m'\u001b[39m\u001b[33mmio.txt\u001b[39m\u001b[33m'\u001b[39m,dtype=\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/SL_Hawaii/notebooks/../python/nonstationaryGEV/helpers.py:217\u001b[39m, in \u001b[36mfitness\u001b[39m\u001b[34m(x, dirs, modelType, nproc)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.devnull, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m devnull:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         result = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmpirun_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Check if the subprocess ran successfully\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.returncode != \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/SLI311/lib/python3.11/subprocess.py:550\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    552\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/SLI311/lib/python3.11/subprocess.py:1209\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1206\u001b[39m     endtime = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1208\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1209\u001b[39m     stdout, stderr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1211\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1212\u001b[39m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[32m   1213\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/SLI311/lib/python3.11/subprocess.py:2115\u001b[39m, in \u001b[36mPopen._communicate\u001b[39m\u001b[34m(self, input, endtime, orig_timeout)\u001b[39m\n\u001b[32m   2108\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_timeout(endtime, orig_timeout,\n\u001b[32m   2109\u001b[39m                         stdout, stderr,\n\u001b[32m   2110\u001b[39m                         skip_check_and_raise=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[32m   2112\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   2113\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mfailed to raise TimeoutExpired.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2115\u001b[39m ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[38;5;28mself\u001b[39m._check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[32m   2118\u001b[39m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[32m   2119\u001b[39m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/SLI311/lib/python3.11/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28mself\u001b[39m._selector.poll(timeout)\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "x_cvte2, w_cvte2, wcomp, SignifCvte2 = models.run_covariate_in_scale_model(x_cvte1, w_cvte1, wcomp, stationID,SignifCvte1, dirs, modelInfo, runWithoutModel=runWithoutModel, modelType='GEV_S_T_Cv_Nodal',saveModel=True, nproc=numProcesses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of the parameter names that correspond to the 'x_cvte2' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "param_names = ['Annual seasonal cycle',\n",
    "    'Semiannual seasonal cycle',\n",
    "    'Triannual seasonal cycle',\n",
    "    'Long-term Trend in Location',\n",
    "    'Covariate in Location',\n",
    "    'Covariate in Scale',\n",
    "    'Nodal Cycle']\n",
    "\n",
    "# Write out the parameter names, where if x[i] = 1, the parameter is included, otherwise it is removed from the list\n",
    "parameters_included = [param for i, param in zip(x_cvte2, param_names) if i == 1]\n",
    "\n",
    "\n",
    "# Format the parameter names for display, where each parameter is separated by a comma and the last parameter is preceded by 'and'\n",
    "parameters_included = ', '.join(parameters_included[:-1]) + ', and ' + parameters_included[-1]\n",
    "glue(\"parameters_included\", parameters_included)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the timeseries\n",
    "\n",
    "Finally we'll calculate the time dependent return value, with confidence intervals from above, using only the nodal + long-term trend + seasonality model. The plotTimeDependentReturnValue code evaluates the likelihood function using the modeled parameters for a given set of years and return periods (in years). It will also return the 95% confidence intervals, with the assumption of Guassian distribution ($\\pm 1.96 \\sigma$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plotting.plotTimeDependentReturnValue(stationID, \n",
    "                                            STNDtoMHHW, \n",
    "                                            dirs['model_output_dir'], \n",
    "                                            station_name, \n",
    "                                            output_dir, \n",
    "                                            mm, \n",
    "                                            year0plot, \n",
    "                                            saveToFile=True,\n",
    "                                            specModel='RL_muN.nc')\n",
    "\n",
    "# set y-limits on the axes\n",
    "ax = fig.axes[0]\n",
    "ax.set_ylim(mm['monthly_max'].min()-STNDtoMHHW-0.1, mm['monthly_max'].max()-STNDtoMHHW+0.1)\n",
    "\n",
    "\n",
    "# save the figure\n",
    "\n",
    "savename = 'TimeDependentReturnValue_'+ stationID +'.png'\n",
    "savedir = os.path.join(output_dir, savename)\n",
    "plt.savefig(savedir, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# glue the figure\n",
    "glue(\"TimeDependentReturnValue\", fig, display=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} TimeDependentReturnValue\n",
    ":name: \"TimeDependentReturnValue\"\n",
    "Time series of the nonstationary GEV-based extreme water level return values for {glue:text}`station_name`. The monthly maxima (+) are displayed spanning data from {glue:text}`year0` to 2023. The return levels (at 2, 10, 50, and 100 year levels) are shown in solid lines. The 95% Confidence Interval (shaded area) also varies in time, and is shown only for the 2 and 10 year return levels. Years 2008 and 2020 are highlighted on these curves to show the differences in values when taking seasonality, a long-term trend, and the 18.6 nodal-cycle into account. This particular best-fit model for {glue:text}`station_name` includes the {glue:text}`parameters_included`. Note we have not included the perigean cycle in this model. That's in another notebook!!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make it interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The mean of the location parameter should be the mean of the monthly maxima\n",
    "meanmaxSL = w_cvte2[1]-STNDtoMHHW\n",
    "#The standard deviation should be the standard deviation of the monthly maxima  \n",
    "rangemaxSL = w_cvte2[2]\n",
    "fig = plotting.plotTimeDependentReturnValue_plotly(stationID, \n",
    "                                                   STNDtoMHHW, \n",
    "                                                   dirs['model_output_dir'], \n",
    "                                                   station_name, \n",
    "                                                   mm, \n",
    "                                                   year0plot, \n",
    "                                                   meanmaxSL, \n",
    "                                                   rangemaxSL,\n",
    "                                                   specModel='RL_muN.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a combined dataset for further plotting and analysis\n",
    "\n",
    "You'll want to go back and run the above codes for every station to make the maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# One dataset to rule them all\n",
    "\n",
    "# Find all RL_muN.nc files\n",
    "file_pattern = str(dirs['model_output_dir'] / '**/RL_muN.nc')\n",
    "file_paths = glob.glob(file_pattern, recursive=True)\n",
    "\n",
    "# Initialize an empty list to hold each dataset\n",
    "datasets = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Extract important attributes\n",
    "    station_id = int(ds.attrs.get('station_id'))    \n",
    "    station_name = ds.attrs.get('station_name')\n",
    "    x = ds.attrs.get('x')  # Assuming x is stored as an attribute and has varying lengths\n",
    "    \n",
    "    # Ensure 'year' and 'return_level' are coordinates and are preserved\n",
    "    year = ds['year'] if 'year' in ds else ds.coords.get('year')\n",
    "    return_level = ds['return_level'] if 'return_level' in ds else ds.coords.get('return_level')\n",
    "    \n",
    "    # Expand the dataset with the 'station' dimension (coordinate)\n",
    "    ds = ds.expand_dims({'station_id': [station_id]})\n",
    "    \n",
    "    # Store x as a variable with its own dimension (e.g., 'x_dim')\n",
    "    ds['x'] = xr.DataArray(x, dims=['x_dim'])\n",
    "    \n",
    "    # Keep station_name as an attribute of the dataset\n",
    "    ds.attrs['station_name'] = station_name\n",
    "    \n",
    "    # Add the dataset to the list\n",
    "    datasets.append(ds)\n",
    "\n",
    "# Combine all datasets along the 'station' dimension\n",
    "try:\n",
    "    combined_ds = xr.concat(datasets, dim='station_id', combine_attrs='override')\n",
    "    print(\"Combined dataset created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during dataset concatenation: {e}\")\n",
    "\n",
    "# remove attributes that are not needed\n",
    "attributes_to_remove = ['station_name', 'model_parameters', 'model_standard_error', 'model', 'x', 'record_id']\n",
    "for attr in attributes_to_remove:\n",
    "    combined_ds.attrs.pop(attr, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# add lat, lon, and station_name to the dataset from rsl_hourly\n",
    "rids = [str(rid) for rid in combined_ds['station_id'].values]\n",
    "lats = []\n",
    "lons = []\n",
    "MHHW = []\n",
    "for rid in rids:\n",
    "    lat = rsl_hourly.sel(station_id=rid)['lat'].values\n",
    "    lon = rsl_hourly.sel(station_id=rid)['lon'].values\n",
    "    mhhw = rsl_hourly.sel(station_id=rid)['MHHW'].values\n",
    "    lats.append(lat)\n",
    "    lons.append(lon)\n",
    "    MHHW.append(mhhw)\n",
    "\n",
    "combined_ds['lat'] = xr.DataArray(lats, dims='station_id')\n",
    "combined_ds['lon'] = xr.DataArray(lons, dims='station_id')\n",
    "combined_ds['MHHW'] = xr.DataArray(MHHW, dims='station_id')\n",
    "\n",
    "# add station_name\n",
    "station_names = rsl_hourly.sel(station_id=rids)['station_name'].values\n",
    "combined_ds['station_name'] = xr.DataArray(station_names, dims='station_id')\n",
    "\n",
    "# save the combined dataset to a netcdf file\n",
    "# combined_ds.to_netcdf(output_dir / 'combined_return_levels.nc')\n",
    "combined_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "crs = ccrs.PlateCarree()\n",
    "fig, axs = plt.subplots(2,2,figsize=(10, 8), subplot_kw={'projection': crs})\n",
    "# make ax,fig\n",
    "# open the cmems data\n",
    "\n",
    "xlims = [-179, -152]\n",
    "ylims = [15, 30]\n",
    "\n",
    "scatters = []\n",
    "\n",
    "# plt.colorbar(maxplt,ax=axs[0],label='Sea Level (m)', location='bottom')           \n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "\n",
    "    scatter = ax.scatter(combined_ds['lon'], combined_ds['lat'], transform=crs, s=100, \n",
    "               c=combined_ds['ReturnLevel'].sel(Year=2020)[:,i]-combined_ds['MHHW'], cmap='YlOrRd',\n",
    "               linewidth=0.5, edgecolor='black')\n",
    "    scatters.append(scatter)\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, color='lightgrey')\n",
    "\n",
    "    # set extent\n",
    "    ax.set_extent([xlims[0], xlims[1], ylims[0], ylims[1]], crs=crs)\n",
    "\n",
    "    #add grid\n",
    "    gl = ax.gridlines(draw_labels=False, linestyle=':', color='black',\n",
    "                      alpha=0.5,xlocs=ax.get_xticks(),ylocs=ax.get_yticks())\n",
    "    if i>=0:\n",
    "        gl.bottom_labels = True\n",
    "\n",
    "    if i==0:\n",
    "        gl.left_labels = True\n",
    "\n",
    "    #make all labels tiny\n",
    "    gl.xlabel_style = {'size': 8}\n",
    "    gl.ylabel_style = {'size': 8}\n",
    "\n",
    "    # add text to top left of plot\n",
    "    ax.text(0.5, 0.95, f'{return_period[i]}-year flood: 2020', color='black', fontsize=10, weight='bold',\n",
    "            transform=ax.transAxes, ha='center', va='top', zorder=10)\n",
    "\n",
    "# add colorbar to the bottom of the plot which follows vmin, vmax and cmap of the scatter plot\n",
    "# colorbar should be \n",
    "cbar = plt.colorbar(axs[1, 1].collections[0], ax=axs, orientation='horizontal', pad=0.05, aspect=50,extend='both')\n",
    "cbar.set_label('Meters above MHHW')\n",
    "\n",
    "# set the limits of the colorbar\n",
    "for scatter in scatters:\n",
    "    scatter.set_clim(vmin=0.3, vmax=0.7)\n",
    "\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(output_dir / 'TimeDependentReturnValueMap.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "glue(\"TimeDependentReturnValueMap\",fig,display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} TimeDependentReturnValueMap\n",
    ":name: \"TimeDependentReturnValueMap\"\n",
    "Map of the nonstationary GEV-based extreme water level return values for Hawaiian Island region stations. Year 2020 is shown here. Each nonstationary model includes some combination of seasonality, a long-term trend, and the 18.6 nodal-cycle, provided each parameter improves the significance of the GEV fit. No covariates are used for these particular best-fit models.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "crs = ccrs.PlateCarree()\n",
    "fig, axs = plt.subplots(2,2,figsize=(10, 8), subplot_kw={'projection': crs})\n",
    "# make ax,fig\n",
    "# open the cmems data\n",
    "\n",
    "xlims = [-179, -152]\n",
    "ylims = [15, 30]\n",
    "\n",
    "scatters = []\n",
    "\n",
    "# plt.colorbar(maxplt,ax=axs[0],label='Sea Level (m)', location='bottom')           \n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "\n",
    "    scatter = ax.scatter(combined_ds['lon'], combined_ds['lat'], transform=crs, s=100, \n",
    "               c=combined_ds['ReturnLevel'].sel(Year=2020)[:,i]-combined_ds['ReturnLevel'].sel(Year=2008)[:,i], cmap='YlOrRd',\n",
    "               linewidth=0.5, edgecolor='black')\n",
    "    scatters.append(scatter)\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, color='lightgrey')\n",
    "\n",
    "    # set extent\n",
    "    ax.set_extent([xlims[0], xlims[1], ylims[0], ylims[1]], crs=crs)\n",
    "\n",
    "    #add grid\n",
    "    gl = ax.gridlines(draw_labels=False, linestyle=':', color='black',\n",
    "                      alpha=0.5,xlocs=ax.get_xticks(),ylocs=ax.get_yticks())\n",
    "    if i>=0:\n",
    "        gl.bottom_labels = True\n",
    "\n",
    "    if i==0:\n",
    "        gl.left_labels = True\n",
    "\n",
    "    #make all labels tiny\n",
    "    gl.xlabel_style = {'size': 8}\n",
    "    gl.ylabel_style = {'size': 8}\n",
    "\n",
    "    # add text to top left of plot\n",
    "    ax.text(0.5, 0.95, f'{return_period[i]}-year flood: 2020-2008 difference', color='black', fontsize=10, weight='bold',\n",
    "            transform=ax.transAxes, ha='center', va='top', zorder=10)\n",
    "\n",
    "# add colorbar to the bottom of the plot which follows vmin, vmax and cmap of the scatter plot\n",
    "# colorbar should be \n",
    "cbar = plt.colorbar(axs[1, 1].collections[0], ax=axs, orientation='horizontal', pad=0.05, aspect=50,extend='both')\n",
    "cbar.set_label('Meters above MHHW')\n",
    "\n",
    "# set the limits of the colorbar\n",
    "for scatter in scatters:\n",
    "    scatter.set_clim(vmin=0, vmax=0.15)\n",
    "\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(output_dir / 'TimeDependentReturnValueMap2008.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "glue(\"TimeDependentReturnValueMap2008\",fig,display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":style: plain\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLI311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
