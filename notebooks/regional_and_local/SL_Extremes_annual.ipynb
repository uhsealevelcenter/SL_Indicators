{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annual Extremes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} return_curve\n",
    ":align: center\n",
    "```\n",
    "\n",
    "In this notebook, we’ll look at the highest annual water levels and how often they occur, using a statistical approach called extreme value theory. We'll then be able to put these highest water levels (and modeled 1-year, 2-year, and 10-year events) in context with the [rankings](notebooks/regional_and_local/SL_Rankings_annual.ipynb) we developed earlier. We’ll use the Generalized Extreme Value (GEV) distribution to model these extreme events. The GEV distribution helps us describe the largest values in a dataset using three main parameters:\n",
    "\n",
    "- Location ($\\mu$): where the center of the distribution is (similar to the average).\n",
    "- Scale ($\\Psi$): how spread out the values are.\n",
    "- Shape ($\\xi$): how the extremes behave at the tails (how likely very rare events are).\n",
    "\n",
    "For this analysis, we’ll assume the overall pattern of water levels doesn’t change over time (this is called _stationarity_). This means we use past data to predict future extremes, assuming the underlying conditions stay the same. However, in reality, things like sea level rise, land movement, and climate cycles (like ENSO or PDO) can change these patterns over time. If you’re interested in how changing conditions affect extremes, see the [non-stationary extremes](notebooks/nonstationaryGEV/monthly_extremes_non-stationaryGEV.ipynb) notebook. For now, we’ll keep it simple and focus on the stationary case, using data that’s been adjusted to remove long-term trends.\n",
    "\n",
    "## Setup\n",
    "As with previous sections, we first need to import the necessary libraries, establish our input/output directories, and set up some basic plotting rules. We'll do this by running another notebook called [setup](notebooks/setup.ipynb) and import our plotting functions, which will also set our data and output paths. If you have not run the [datawrangling notebook](notebooks/SL_Data_Wrangling.ipynb), you will need to do this before running this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../setup.ipynb\n",
    "\n",
    "# import utide\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "# check to make sure that data_dir/rsl_daily_hawaii.nc exists, if not, make warning to run datawrangling notebook\n",
    "if not (data_dir / 'rsl_daily_hawaii.nc').exists():\n",
    "    print(f'rsl_daily_hawaii.nc not found in {data_dir}. Please run the data wrangling notebook first')\n",
    "else:\n",
    "    print(f'rsl_daily_hawaii.nc found in {data_dir}. Proceed.')\n",
    "\n",
    "\n",
    "# check to make sure that data_dir/rsl_hawaii.nc exists, if not, make warning to run datawrangling notebook\n",
    "if not (data_dir / 'rsl_hawaii_noaa.nc').exists():\n",
    "    print(f'rsl_hawaii_noaa.nc not found in {data_dir}. Please run the data wrangling notebook first')\n",
    "else:\n",
    "    print(f'rsl_hawaii_noaa.nc found in {data_dir}. Proceed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Daily and Hourly datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#import rsl_daily\n",
    "\n",
    "with xr.open_dataset(data_dir/ 'rsl_daily_hawaii.nc') as ds:\n",
    "\n",
    "    #make rsl_daily a subset - do not include TGs with more than 25% NaNs\n",
    "    data_coverage = ds['rsl_mhhw'].count(dim='time')/len(ds.time)\n",
    "\n",
    "    #drop all locations with data_coverage less than 80%\n",
    "    rsl_daily = ds.where(data_coverage>0.80,drop=True)\n",
    "\n",
    "with xr.open_dataset(data_dir/ 'rsl_hawaii_noaa.nc') as ds:\n",
    "    rsl_hourly = ds.sel(station_id = rsl_daily.station_id.values)\n",
    "    \n",
    "\n",
    "# truncate time period to end at the end of 2025\n",
    "rsl_daily = rsl_daily.sel(time = slice(None, '2025-01-01'))\n",
    "rsl_hourly = rsl_hourly.sel(time = slice(None, '2025-01-01'))\n",
    "\n",
    "# check the last time in rsl_daily and rsl_hourly\n",
    "print(rsl_daily.time[-1].values)\n",
    "print(rsl_hourly.time[-1].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "### Remove outliers and obtain trend\n",
    "Here we are using the FULL data set, not just from 1993-onward.\n",
    "\n",
    "First we'll nan out the tsunami time periods, and then calculate the trend. Tsunami data is obtained from {cite:t}{`noauthor_ncei_nodate-1`}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch tsunami event data for Hawaii from NOAA API\n",
    "url = \"https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/tsunamis/events?itemsPerPage=200&page=1&regionCode=80\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Convert to DataFrame\n",
    "events = pd.DataFrame(data['items'])\n",
    "\n",
    "# remove events that do not have 'month' or 'day' or 'hour' columns\n",
    "events = events.dropna(subset=['month', 'day', 'hour'])\n",
    "\n",
    "# # Let's use 'year', 'month', 'day', 'hour', 'minute' for start time\n",
    "def parse_event_time(row):\n",
    "    # Some events may not have hour/minute\n",
    "    year, month, day = row['year'], row['month'], row['day']\n",
    "    hour = row.get('hour', 0) or 0\n",
    "    minute = row.get('minute', 0) or 0\n",
    "    return pd.Timestamp(year=int(year), month=int(month), day=int(day), hour=int(hour), minute=int(minute))\n",
    "\n",
    "# Build a list of start times (you can add a fixed window, e.g., 36 hours, for each event)\n",
    "tsunami_periods = []\n",
    "for _, row in events.iterrows():\n",
    "    try:\n",
    "        start = parse_event_time(row)\n",
    "        end = start + pd.Timedelta(hours=36)  # 36 hours to match previous analysis\n",
    "        tsunami_periods.append((start, end))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse event: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Manually add on 2021 Alaska, 2022 Tonga, 2011 Tohoku, 1960 Chile, 1964 Alaska\n",
    "tsunami_periods.append((pd.Timestamp(\"2021-07-29\"), pd.Timestamp(\"2021-07-30 12:00\")))\n",
    "tsunami_periods.append((pd.Timestamp(\"2022-01-15\"), pd.Timestamp(\"2022-01-16 12:00\")))\n",
    "tsunami_periods.append((pd.Timestamp(\"2011-03-11\"), pd.Timestamp(\"2011-03-12 12:00\")))\n",
    "tsunami_periods.append((pd.Timestamp(\"1960-05-22\"), pd.Timestamp(\"1960-05-23 12:00\")))\n",
    "tsunami_periods.append((pd.Timestamp(\"1964-03-27 12:00\"), pd.Timestamp(\"1964-03-29\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save a copy of rsl_hourly WITH tsunamis in it for our top 10 events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsl_hourly_with_tsunamis = rsl_hourly.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll mask our tsunami periods (fill with nan) so that they don't affect our GEV distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start, end in tsunami_periods:\n",
    "    rsl_hourly['sea_level'].loc[dict(time=slice(start, end))] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll adjust our data to MHHW datum, and calculate the trend so that we can remove it from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyextremes import get_extremes, get_return_periods, plotting, EVA\n",
    "\n",
    "# cut data off at end of 2024\n",
    "rsl_hourly = rsl_hourly.sel(time=slice(None, '2024-12-31'))\n",
    "\n",
    "rsl_hourly['sea_level_mhhw'] = rsl_hourly['sea_level'] - rsl_hourly['MHHW']\n",
    "\n",
    "# get trend, trend_rate, trend_mag\n",
    "trend_mag, trend_line, trend_rate = tsf.process_trend_with_nan(rsl_hourly['sea_level_mhhw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot to make sure we're looking at the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = rsl_hourly['sea_level_mhhw'].plot(x='time', col='station_id', col_wrap=2, sharey=False, sharex=True, figsize=(8, 8))\n",
    "\n",
    "# Use g.axs to iterate over the axes in the FacetGrid\n",
    "for ax, sid in zip(g.axs.flat, rsl_hourly.station_id):\n",
    "    # Accessing the station_name coordinate for the current station_id directly\n",
    "    trend_line.sel(station_id = sid).plot(ax = ax)\n",
    "    station_name = rsl_hourly['station_name'].sel(station_id = sid).values\n",
    "    sid_str = str(sid.values)\n",
    "    ax.set_title(f\"{station_name} ({sid_str})\")\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Sea Level (m MHHW)')\n",
    "\n",
    "    # remove ylabel if plot on the right\n",
    "    # g.axs.shape gives (nrow, ncol)\n",
    "    ncol = g.axs.shape[1]\n",
    "    idx = np.where(g.axs.flat == ax)[0][0]\n",
    "    if (idx % ncol) == (ncol - 1):\n",
    "        ax.set_ylabel('')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll turn this data into a timeseries and detrend it (remove the trend line calculated above). This now puts everything relative to mean sea level for the calculated time period. Eventually we'll set it back to MHHW (NTDE 83-01 epoch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the linear trend, this will put everything at MSL.\n",
    "rsl_detrended = rsl_hourly['sea_level_mhhw']-trend_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create an xarray full of zeros, so that we can later fill it up with values for our return periods at each station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zero xarray\n",
    "z = np.zeros((len(rsl_hourly['station_id'].values), 4))\n",
    "return_periods = [2,10,50,100]\n",
    "return_period_xr = xr.DataArray(z, coords=[rsl_hourly['station_id'].values, return_periods],\n",
    "                  dims=['station_id', 'return period'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll turn each detrended sea level record into a pandas dataseries for extreme value analysis. This extra formatting step is necessary to ensure that the timesteps are handled correctly. Then we'll set everything relative to MHHW again.\n",
    "\n",
    "For now, we'll pick a station to highlight. We'll choose our 7th station (idx = 6), Nawiliwili on Kauai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "station_name = rsl_hourly['station_name'][idx].item()\n",
    "station_id = rsl_hourly['station_id'][idx].item()\n",
    "glue('station_name',station_name,display=False)\n",
    "\n",
    "def convert_to_detrendedMHHW(idx: int):   \n",
    "    sea_level_data_detrended = pd.Series(rsl_detrended[idx], index=pd.to_datetime(rsl_hourly['time'].values))\n",
    "    sea_level_data = pd.Series(rsl_hourly['sea_level_mhhw'][idx], index=pd.to_datetime(rsl_hourly['time'].values))\n",
    "\n",
    "    N = 360  # minimum number of valid points per year (adjust as needed)\n",
    "    std_threshold = 0.05\n",
    "\n",
    "    grouped = sea_level_data_detrended.groupby(sea_level_data_detrended.index.year)\n",
    "    valid_years = [\n",
    "        year for year, group in grouped\n",
    "        if group.count() >= N and group.std() >= std_threshold\n",
    "    ]\n",
    "    sea_level_data_detrended = sea_level_data_detrended[sea_level_data_detrended.index.year.isin(valid_years)]\n",
    "\n",
    "    # drop the nan values\n",
    "    sea_level_data_detrended = sea_level_data_detrended.dropna()\n",
    "    sea_level_data = sea_level_data.dropna()\n",
    "\n",
    "    # set everything relative to MHHW from MSL coordinates\n",
    "    MSLtoMHHW = np.round(rsl_hourly['MSL'][idx].values-rsl_hourly['MHHW'][idx].values,4)\n",
    "    MSLtoMHHW\n",
    "\n",
    "    sea_level_data_detrended = sea_level_data_detrended + MSLtoMHHW\n",
    "    # label the data\n",
    "    sea_level_data_detrended.name = 'sea level [m MHHW, detrended]'\n",
    "    sea_level_data_detrended.index.name = 'time (GMT)'\n",
    "    return sea_level_data_detrended\n",
    "    # plot sea_level_data_detrended\n",
    "\n",
    "sea_level_data_detrended =  convert_to_detrendedMHHW(idx)   \n",
    "\n",
    "\n",
    "\n",
    "ax = sea_level_data_detrended.plot() # This should be in MHHW datum, check to see if it makes sense.\n",
    "\n",
    "# add title\n",
    "ax.set_title(station_name)\n",
    "ax.set_ylabel('Water Level (m, MHHW)\\nLinearly Detrended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model, and extract the extreme values.\n",
    "Here, we are going to find extremes using the Block Maxima (BM) method. This will find all extreme values by selecting a block size (here we've chosen a year for the annual max, and 30 days for the monthly max). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are a few functions to help with the analysis\n",
    "\n",
    "def safe_idxmax(x):\n",
    "    if x.dropna().empty:\n",
    "        return pd.NaT\n",
    "    return x.idxmax()\n",
    "\n",
    "def get_BM_values(sea_level_data_detrended, timescale= 'YE'):\n",
    "    BM_times = sea_level_data_detrended.resample(timescale).apply(safe_idxmax).dropna()\n",
    "    BM_values = sea_level_data_detrended.loc[BM_times]\n",
    "    BM_values.index = BM_times.index  # Set index to year-end for consistency\n",
    "    # If you want the actual timestamp as index:\n",
    "    BM_actual = pd.Series(BM_values.values, index=BM_times.values)\n",
    "    if timescale == 'YE':\n",
    "        BM_actual.name = 'Annual Maxima'\n",
    "    elif timescale == 'ME':\n",
    "        BM_actual.name = 'Monthly Maxima'\n",
    "    return BM_actual\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "min_last_block_ratio = 0.9\n",
    "BM = get_BM_values(sea_level_data_detrended)\n",
    "model = EVA(BM)\n",
    "model.get_extremes(extremes_type='high', method='BM', block_size='365D',min_last_block=min_last_block_ratio, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the extreme events, here calculated with BM method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the annual maxima (BM) as a time series\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "#plot sea_level_detrended\n",
    "sea_level_data_detrended.plot(ax=ax, linestyle='-', label='Detrended Sea Level', linewidth=0.2)\n",
    "BM.plot(ax=ax, marker='o', linestyle='', label='Annual Maxima', markersize=5)\n",
    "ax.set_xlabel('time (GMT)')\n",
    "ax.set_ylabel('sea level [m MHHW, detrended]')\n",
    "ax.legend()\n",
    "ax.title.set_text(f'{station_name} ({station_id}) Annual Maxima (BM)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "We are going to take our extremes and fit them to a model. The pyextremes package will select the 'best model' for this using the Akaike Information Criterion (AIC), with the goal of losing the least amount of information in a given model. Using BM, we could have either a GEV or a Gumbel (specialized case of the GEV) distribution. While we could force it to either distribution, for this case we'll the AIC play out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_model()\n",
    "\n",
    "distribution_name = model.distribution.name\n",
    "if distribution_name == 'genextreme':\n",
    "    distribution_name = 'GEV'\n",
    "    \n",
    "glue(\"distribution name\", model.distribution.name, display=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using the {glue:text}`distribution name` distribution. Now, we'll print out the shape, location, and scale parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using the GEV distribution, we'll have 3 parameters (shape, scale, and location) using the Maximum Likelihood Estimation (MLE). \n",
    "If we are using the gumbel distribution, there are only 2 free parameters: location and scale (as the GEV approaches Gumbel when the shape parameter goes to zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "if model.distribution.name == 'gumbel_r':\n",
    "    shape = 0\n",
    "else:\n",
    "    shape = model.distribution.mle_parameters['c']\n",
    "scale = model.distribution.mle_parameters['scale']\n",
    "loc = model.distribution.mle_parameters['loc']\n",
    "\n",
    "# Format with two decimal places\n",
    "shape_str = f'{shape:.2f}'\n",
    "scale_str = f'{scale:.2f}'\n",
    "loc_str = f'{loc:.2f}'\n",
    "\n",
    "glue(\"shape\", shape_str, display=False)\n",
    "glue(\"scale\", scale_str, display=False)\n",
    "glue(\"loc\", loc_str, display=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the {glue:text}`station_name` tide gauge, the shape parameter $\\xi$ is {glue:text}`shape`, corresponding to a {glue:text}`distribution name` distribution of the extremes. The location parameter $\\mu$ tells us that the distribution is centered around {glue:text}`loc`, and the scale parameter ($\\psi$  = {glue:text}`scale`} shows how tightly the extremes cluster with each other (little dispersion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get a closer look at the return level curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_returnLevelCurve(sid):\n",
    "    fig,ax = model.plot_return_values(alpha=0.95, return_period=np.arange(1.3,500,2),n_samples=1000, plotting_position='weibull')\n",
    "    ax.set_ylim(0.1,1.25)\n",
    "\n",
    "    # add lines for the 2, 5, 10, 50, and 100 year return periods\n",
    "    return_periods = [2,10,50,100]\n",
    "\n",
    "    colors = ['r','g','b','c']\n",
    "    for return_period, color in zip(return_periods, colors):\n",
    "        ax.axvline(x=return_period, color=color, label=f'{return_period} year return period')\n",
    "        ax.text(return_period, 0.15, f'{return_period} yr', rotation=90, \n",
    "            verticalalignment='bottom',backgroundcolor='w',horizontalalignment='center',\n",
    "            fontsize=8)\n",
    "\n",
    "    fig.set_size_inches(5,3)\n",
    "    ax.set_title(station_name)\n",
    "    ax.set_ylabel('Annual Maxima\\n (m, MHHW, detrended)')\n",
    "    # save the figure\n",
    "    figname = 'SL_extremes_return_values_'+ station_name + '.png'\n",
    "    plt.savefig(output_dir / figname, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_returnLevelCurve(sid)\n",
    "\n",
    "glue('return_curve', fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} return_curve\n",
    ":name: \"return_curve\"\n",
    "\n",
    "Return level curve for {glue:text}`station_name`, using annual maxima (block maxima method). The 2,10, 50 and 100 yr periods are shown as vertical lines. Confidence intervals for the GEV model are shown in shaded blue. Note that the extreme annual maximum (almost 1m above MHHW) does not fall neatly on this distribution. In this case, this extreme is the result of exception water levels due to the landfall of Hurricane Iniki in 1992. Hurricane landfalls are rare events in Hawaii, and thus it should be expected that an event like this does not align with distributions made with our historical observations.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(sid):\n",
    "    # make new figure with axes 1-3\n",
    "    fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "    # make pdf of extremes\n",
    "    pdf_extremes = np.linspace(model.extremes.min(), model.extremes.max(), 500)\n",
    "    pdf = model.model.pdf(model.extremes_transformer.transform(pdf_extremes))\n",
    "\n",
    "    axs[0].hist(\n",
    "                model.extremes.values,\n",
    "                bins=np.histogram_bin_edges(a=model.extremes.values, bins=20),\n",
    "                density=True,\n",
    "                rwidth=0.8,\n",
    "                facecolor=\"#5199FF\",\n",
    "                edgecolor=\"None\",\n",
    "                lw=0,\n",
    "                alpha=0.25,\n",
    "                zorder=5,\n",
    "            )\n",
    "\n",
    "    axs[0].plot(pdf_extremes, pdf, color=\"#F85C50\", lw=2, ls=\"-\", zorder=15)\n",
    "\n",
    "    axs[0].scatter(\n",
    "                model.extremes.values,\n",
    "                np.full(shape=len(model.extremes), fill_value=0),\n",
    "                marker=\"|\",\n",
    "                s=40,\n",
    "                color=\"k\",\n",
    "                lw=0.5,\n",
    "                zorder=15,\n",
    "            )\n",
    "\n",
    "    # add parameters to the plot\n",
    "    axs[0].text(\n",
    "        0.95,\n",
    "        0.95,\n",
    "        f\"$\\\\xi$: {shape_str}\\n$\\\\psi$: {scale_str}\\n$\\\\mu$: {loc_str}\",\n",
    "        fontsize=11,\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "        transform=axs[0].transAxes  # Updated to 'axs.transAxes' assuming axs is a single axis object\n",
    "    )\n",
    "\n",
    "\n",
    "    axs[0].set_xlabel('Annual Maxima [m]')\n",
    "    axs[0].set_ylabel(\"Probability density\")\n",
    "\n",
    "    model.plot_probability('PP',ax=axs[1])\n",
    "    model.plot_probability('QQ',ax=axs[2])\n",
    "\n",
    "    axs[0].set_title('Histogram of Annual Maxima')\n",
    "    axs[1].set_title('Probability Plot: P-P')\n",
    "    axs[2].set_title('Probability Plot: Q-Q')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # add a supertitle with station name and station id\n",
    "    fig.suptitle(f'Station: {station_name}, ID: {sid.values}', fontsize=12)\n",
    "\n",
    "    # move other stuff down\n",
    "    axs[0].set_title('Histogram of Annual Maxima')\n",
    "    axs[1].set_title('Probability Plot: P-P')\n",
    "    axs[2].set_title('Probability Plot: Q-Q')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # save the figure\n",
    "    figname = 'SL_extremes_histogram_pp_qq_'+ station_name + '.png'\n",
    "\n",
    "    plt.savefig(output_dir / figname, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plot_stats(sid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get return periods of 2, 10, 50 and 100 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary of the model\n",
    "return_periods=[2,5, 10, 50, 100]\n",
    "\n",
    "summary = model.get_summary(\n",
    "    return_period=return_periods,\n",
    "    alpha=0.95, n_samples=1000\n",
    ")\n",
    "\n",
    "#change name \"return period\" to \"return period (yrs)\"\n",
    "summary = summary.rename(columns={'return period': 'return period (yrs)','return value':'Water Level (m, MHHW)','lower ci':'Lower CI (m, MHHW)','upper ci':'Upper CI (m, MHHW)'})\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# get return periods (2,5, 10,50,100) for all stations:\n",
    "stat_names = ['Water Level (m, MHHW)', 'Lower CI (m, MHHW)', 'Upper CI (m, MHHW)']\n",
    "\n",
    "# Create an empty DataArray with dimensions: station_id, return period, stat\n",
    "z = np.zeros((len(rsl_hourly['station_id'].values), len(return_periods), len(stat_names)))\n",
    "return_period_xr = xr.DataArray(\n",
    "    z,\n",
    "    coords={\n",
    "        'station_id': rsl_hourly['station_id'].values,\n",
    "        'return period': return_periods,\n",
    "        'stat': stat_names\n",
    "    },\n",
    "    dims=['station_id', 'return period', 'stat']\n",
    ")\n",
    "\n",
    "if not (output_dir / 'return_period_stationary_CI.nc').exists():\n",
    "    for rec_id in np.arange(0, len(rsl_daily['station_id']), 1):\n",
    "        sea_level_data_detrended_temp = convert_to_detrendedMHHW(rec_id)\n",
    "        min_last_block_ratio = 0.9\n",
    "        BM = get_BM_values(sea_level_data_detrended_temp)\n",
    "        model = EVA(BM)\n",
    "        model.get_extremes(extremes_type='high', method='BM', block_size='365D', min_last_block=min_last_block_ratio, errors='ignore')\n",
    "        model.fit_model()\n",
    "        summary = model.get_summary(\n",
    "            return_period=return_periods,\n",
    "            alpha=0.95, n_samples=1000\n",
    "        )\n",
    "        summary = summary.rename(columns={\n",
    "            'return period': 'return period (yrs)',\n",
    "            'return value': 'Water Level (m, MHHW)',\n",
    "            'lower ci': 'Lower CI (m, MHHW)',\n",
    "            'upper ci': 'Upper CI (m, MHHW)'\n",
    "        })\n",
    "\n",
    "        # Assign the values for each stat\n",
    "        for i, stat in enumerate(stat_names):\n",
    "            return_period_xr[rec_id, :, i] = summary[stat].values\n",
    "\n",
    "        # Save to NetCDF\n",
    "        return_period_xr.to_netcdf(output_dir / 'return_period_stationary_CI.nc')\n",
    "else:\n",
    "    print('return_period_stationary_CI.nc already exists, skipping calculation.')\n",
    "    return_period_xr = xr.open_dataarray(output_dir/ 'return_period_stationary_CI.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an intergrated rankings + return periods table\n",
    "\n",
    "First, make some functions to get the top 10 ranked recorded water levels. This is copied from the rankings notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_table(rsl_subset,station_id):\n",
    "    # make a table of the top 10 values, sorted by size and with date\n",
    "    top_10_values_max = get_top_ten(rsl_subset, station_id, mode='max')\n",
    "    top_10_values_min = get_top_ten(rsl_subset, station_id, mode='min')\n",
    "\n",
    "    top_10_table = pd.concat([top_10_values_max,top_10_values_min])\n",
    "\n",
    "    return top_10_table\n",
    "\n",
    "def get_top_ten(rsl_subset, station_id, mode='max'):\n",
    "    # Convert data to a pandas Series\n",
    "    sea_level_series = rsl_subset.sea_level.sel(station_id=station_id).to_series()\n",
    "\n",
    "   # Select top 100 values based on the mode\n",
    "    if mode == 'max':\n",
    "        top_values = sea_level_series.nlargest(100)\n",
    "    elif mode == 'min':\n",
    "        top_values = sea_level_series.nsmallest(100)\n",
    "    else:\n",
    "        raise ValueError('mode must be either \"max\" or \"min\"')\n",
    "\n",
    "    # Filter to find unique events spaced by at least 3 days\n",
    "    filtered_dates = []\n",
    "    top_10_values = pd.Series()\n",
    "\n",
    "    for date, value in top_values.items():\n",
    "        if all(abs((date - pd.to_datetime(added_date)).days) > 3 for added_date in filtered_dates):\n",
    "            filtered_dates.append(date)\n",
    "            top_10_values[date] = value\n",
    "        if len(filtered_dates) == 10:\n",
    "            break\n",
    "    rank = np.arange(1,11)\n",
    "\n",
    "    station_name = str(rsl_subset['station_name'].sel(station_id=str(station_id)).values)\n",
    "\n",
    "    top_10_values = pd.DataFrame({'rank': rank, 'date': top_10_values.index, 'sea level (m)': top_10_values.values})  \n",
    "    top_10_values['station_name'] = station_name\n",
    "    top_10_values['station_id'] = station_id\n",
    "    top_10_values['type'] = mode\n",
    "\n",
    "    #round the date to the nearest hour\n",
    "    top_10_values['date'] = top_10_values['date'].dt.round('h')\n",
    "\n",
    "    return top_10_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First make the rankings list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rsl_subset = rsl_hourly_with_tsunamis.copy()\n",
    "rsl_subset['sea_level'] = rsl_subset['sea_level'] - rsl_subset['MHHW']\n",
    "\n",
    "SL_rankings10 = get_top_10_table(rsl_subset,station_id)\n",
    "\n",
    "# take only the max type\n",
    "SL_rankings10 = SL_rankings10[SL_rankings10['type']=='max']\n",
    "SL_rankings10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll add in the return period table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix this dataframe in with the SL_rankings table:\n",
    "\n",
    "# extract only 'Highest' and 'Highest Date' columns\n",
    "SL_rankings = SL_rankings10[['date','sea level (m)']]\n",
    "\n",
    "#rename 'Highest' to sea level (m, MHHW)\n",
    "SL_rankings = SL_rankings.rename(columns={'sea level (m)':'Water Level (m, MHHW)'})\n",
    "\n",
    "# Ensure 'Water Level (m, MHHW)' column is numeric for both DataFrames\n",
    "SL_rankings['Water Level (m, MHHW)'] = pd.to_numeric(SL_rankings['Water Level (m, MHHW)'], errors='coerce')\n",
    "summary['Water Level (m, MHHW)'] = pd.to_numeric(summary['Water Level (m, MHHW)'], errors='coerce')\n",
    "\n",
    "# Merge the SL_rankings DataFrame with the summary DataFrame on 'water level (m, MHHW)'\n",
    "SL_rankings_merge = pd.merge(SL_rankings, summary.reset_index(), on='Water Level (m, MHHW)', how='outer')\n",
    "\n",
    "SL_rankings_merge = SL_rankings_merge.sort_values('Water Level (m, MHHW)',ascending=False)\n",
    "\n",
    "SL_rankings_merge['Date or Return Period'] = SL_rankings_merge.apply(\n",
    "    lambda row: row['date'] if pd.notnull(row['date']) else row['return period'], axis=1\n",
    ")\n",
    "\n",
    "SL_rankings_merge = SL_rankings_merge.drop(columns=['date','return period','Lower CI (m, MHHW)','Upper CI (m, MHHW)'])\n",
    "# SL_rankings_merge = SL_rankings_merge.drop(columns=['date','return period'])\n",
    "\n",
    "\n",
    "# Add a column for feet\n",
    "SL_rankings_merge['Water Level (ft, MHHW)'] = SL_rankings_merge['Water Level (m, MHHW)']*3.28084\n",
    "\n",
    "# Rearrange columns to Date or Return Period, water level (m, MHHW), water level (ft, MHHW)\n",
    "SL_rankings_merge = SL_rankings_merge[['Date or Return Period','Water Level (m, MHHW)','Water Level (ft, MHHW)']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_rankings_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the merged rankings and return period table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the date to be in the format 'YYYY-MM-DD' if a date, or '10-year flood' if a return period\n",
    "SL_rankings_merge['Date or Return Period'] = SL_rankings_merge['Date or Return Period'].apply(\n",
    "    lambda x: x.strftime('%Y-%m-%d') if isinstance(x, pd.Timestamp) else f'{int(x)}-year flood' if isinstance(x, (int, float)) else x\n",
    ")\n",
    "\n",
    "# Format the water level columns to have 2 decimal places\n",
    "SL_rankings_merge['Water Level (m, MHHW)'] = SL_rankings_merge['Water Level (m, MHHW)'].round(2)\n",
    "SL_rankings_merge['Water Level (ft, MHHW)'] = SL_rankings_merge['Water Level (ft, MHHW)'].round(2)\n",
    "\n",
    "# export the table to a csv file\n",
    "csvname = 'SL_extremes_' + station_name + '.csv'\n",
    "SL_rankings_merge.to_csv(output_dir / csvname, index=False)\n",
    "\n",
    "\n",
    "SL_rankings_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into a table with great_tables\n",
    "from great_tables import GT, html, md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the DataFrame\n",
    "SL_rankings_merge_copy = SL_rankings_merge.copy()\n",
    "\n",
    "# replace all spaces with underscores in top_10_display column names\n",
    "# SL_rankings_merge_copy.columns = SL_rankings_merge_copy.columns.str.replace(' ', '_')\n",
    "\n",
    "# replace column names \n",
    "SL_rankings_merge_copy.columns = ['Date_or_Return_Period', 'mMHHW', 'ftMHHW']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a great_table\n",
    "tablename = \"SL_rankings_merge_\" + station_name + \".png\"\n",
    "savepath = output_dir / tablename\n",
    "\n",
    "(\n",
    "    GT(SL_rankings_merge_copy)\n",
    "    .tab_header(title='Highest Sea Level Events', subtitle='Station: '+ station_name + '(' + str(station_id) + ')')\n",
    "    .cols_align('center')\n",
    "    .cols_label(Date_or_Return_Period=html(\"Date or <br>Return Period\"), mMHHW=html('Water Level <br>(m, MHHW)'),ftMHHW = html('Water Level <br>(ft, MHHW)'))\n",
    "    .tab_source_note(source_note='Source: NOAA CO-OPS Hourly Water Level')\n",
    "    .save(savepath)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin} EXPLAINER\n",
    "Why is this table different from other similar tables? (For example, the [NASA flooding analysis tool](https://sealevel.nasa.gov/flooding-analysis-tool/observed-flooding?station-id=1617760&units=meters))\n",
    "\n",
    "A couple of reasons why things might be different: 1) This table uses hourly data, which can \"smooth out\" the true maxima compared to 6-minute averages, and 2) uncertainty increases with longer return periods. When estimating rare events (like 50- or 100-year floods), our results are fundamentally limited by the length and variability of the historical record.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a map\n",
    "### Plot the Return Periods at each station\n",
    "Note that we are neglecting the confidence intervals for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = ccrs.PlateCarree()\n",
    "fig, axs = plt.subplots(2,2,figsize=(10, 8), subplot_kw={'projection': crs})\n",
    "\n",
    "cmems = xr.open_dataset(data_dir / 'cmems_L4_SSH_0.125deg_1993_2025hawaii.nc')\n",
    "\n",
    "xlims = [181, 208]\n",
    "ylims = [15,31]\n",
    "\n",
    "\n",
    "# plt.colorbar(maxplt,ax=axs[0],label='Sea Level (m)', location='bottom')           \n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "\n",
    "    ax.scatter(rsl_hourly['lon'], rsl_hourly['lat'], transform=crs, s=100, \n",
    "               c=return_period_xr[:,i], vmin=0.2, vmax=0.9, cmap='YlOrRd',\n",
    "               linewidth=0.5, edgecolor='black')\n",
    "    \n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, color='lightgrey')\n",
    "\n",
    "    # set extent\n",
    "    ax.set_extent([xlims[0], xlims[1], ylims[0], ylims[1]], crs=crs)\n",
    "\n",
    "    #add grid\n",
    "    gl = ax.gridlines(draw_labels=False, linestyle=':', color='black',\n",
    "                      alpha=0.5,xlocs=ax.get_xticks(),ylocs=ax.get_yticks())\n",
    "    if i>=0:\n",
    "        gl.bottom_labels = True\n",
    "\n",
    "    if i==0:\n",
    "        gl.left_labels = True\n",
    "\n",
    "    #make all labels tiny\n",
    "    gl.xlabel_style = {'size': 8}\n",
    "    gl.ylabel_style = {'size': 8}\n",
    "\n",
    "    # add text to top left of plot\n",
    "    ax.text(0.5, 0.95, f'{return_periods[i]}-year flood', color='black', fontsize=10, weight='bold',\n",
    "            transform=ax.transAxes, ha='center', va='top', zorder=10)\n",
    "\n",
    "# add colorbar to the bottom of the plot which follows vmin, vmax and cmap of the scatter plot\n",
    "# colorbar should be \n",
    "cbar = plt.colorbar(axs[1, 1].collections[0], ax=axs, orientation='horizontal', pad=0.05, aspect=50)\n",
    "cbar.set_label('Meters above MHHW')\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(output_dir / 'SL_extremes_map.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# plot the monthly maxima\n",
    "# monthly_max = get_extremes(sea_level_data_detrended, method='BM', \n",
    "#                            extremes_type='high', block_size='30D', min_last_block=0.9, errors='ignore')\n",
    "# annual_max = get_extremes(sea_level_data_detrended, method='BM', extremes_type='high', block_size='365.245D', min_last_block=0.9, errors='ignore')\n",
    "\n",
    "BM_year = get_BM_values(sea_level_data_detrended)\n",
    "BM_month = get_BM_values(sea_level_data_detrended, timescale='ME')\n",
    "model = EVA(BM_year)\n",
    "model.get_extremes(extremes_type='high', method='BM', block_size='365D',min_last_block=min_last_block_ratio, errors='ignore')\n",
    "\n",
    "\n",
    "model.get_extremes(extremes_type='high', method='BM', block_size='365D',min_last_block=min_last_block_ratio, errors='ignore')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "BM_month.plot(ax=ax, marker='.',linestyle='', label='Monthly Maxima', markersize=2)\n",
    "BM_year.plot(ax=ax, marker='o', linestyle='', label='Annual Maxima', markersize=5)\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Sea Level [m, MHHW]')\n",
    "ax.set_title(f'Extreme Water Levels, detrended\\n {station_name} ({station_id})')\n",
    "\n",
    "colors = ['#264653',  # dark cyan\n",
    "          '#2a9d8f',  # teal\n",
    "          '#e9c46a',  # sandy yellow\n",
    "          '#f4a261']  # sandy orange\n",
    "\n",
    "# add the 2, 10, 50, and 100 year return levels in horizontal lines\n",
    "for i,period in enumerate(return_periods):\n",
    "    ax.axhline(summary.loc[period, 'Water Level (m, MHHW)'], label=f'{period} yr RL', color=colors[i],linewidth=0.5)\n",
    "\n",
    "# add bold black line at 0 for MHHW\n",
    "ax.axhline(0, color='black', linewidth=2)\n",
    "ax.legend()\n",
    "# put the legend at the bottom of the plot in one row\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=3)\n",
    "ax.set_xlim([sea_level_data_detrended.index[0], sea_level_data_detrended.index[-1]])\n",
    "\n",
    "# save the figure\n",
    "figname = 'SL_extremes_monthly_annual_maxima_' + station_name + '.png'\n",
    "plt.savefig(output_dir / figname, dpi=300, bbox_inches='tight')\n",
    "\n",
    "glue('fig', 'SL_extremes_timeseries', display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} SL_extremes_timeseries\n",
    ":name: \"SL_extremes_timeseries\"\n",
    "\n",
    "Extreme water levels at {glue:text}`station_name`, plotted with the background long-term trend removed. The 2,10,50, and 100 return levels are shown in blue, green, yellow and orange respectively. Both monthly and yearly maxima are obtained using the block maxima method. The GEV-derived stationary return levels are derived using the pyextremes python package. One key takeaway from this figure: removing the trend shows that we've had some significant events in the past that do not show up in our rankings. \n",
    "```\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add monthly maxima as a line\n",
    "fig.add_trace(go.Scatter(x=BM_month.index, y=BM_month, mode='markers', name='Monthly Maxima', marker=dict(size=2)))\n",
    "\n",
    "# Add annual maxima as scatter points\n",
    "fig.add_trace(go.Scatter(x=BM_year.index, y=BM_year, mode='markers', name='Annual Maxima'))\n",
    "\n",
    "\n",
    "colors = ['#264653',  # dark cyan\n",
    "          '#2a9d8f',  # teal\n",
    "          '#e9c46a',  # sandy yellow\n",
    "          '#f4a261']  # sandy orange\n",
    "\n",
    "\n",
    "# Add return levels as labeled lines\n",
    "for i, period in enumerate(return_periods):\n",
    "    fig.add_trace(go.Scatter(x=[BM_month.index.min(), BM_month.index.max()],\n",
    "                             y=[summary.loc[period, 'Water Level (m, MHHW)']] * 2,\n",
    "                             mode='lines',\n",
    "                             line=dict(color=colors[i], width=1),\n",
    "                             name=f'{period} year RL'))\n",
    "    \n",
    "# Add a bold black line at 0 for MHHW\n",
    "fig.add_hline(y=0, line=dict(color='black', width=2))\n",
    "\n",
    "# Set titles and labels\n",
    "fig.update_layout(title=f'Extreme Water Levels, Stationary GEV: {station_name} ({station_id})',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Sea Level [m, MHHW, detrended]',\n",
    "                  legend=dict(x=0.5, xanchor='center', y=-0.2, orientation='h', bgcolor='rgba(255,255,255,0.8)'),\n",
    "                  plot_bgcolor='white',  # Set plot background to white\n",
    "                  paper_bgcolor='white')  # Set overall background to white\n",
    "\n",
    "# Set x-axis limits if needed\n",
    "fig.update_xaxes(range=[BM_month.index.min(), BM_month.index.max()])\n",
    "fig.update_layout(width=800, height=500,autosize=False)\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# save the figure\n",
    "figname = 'SL_extremes_monthly_annual_maxima_' + station_name + '.html'\n",
    "\n",
    "fig.write_html(output_dir / figname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Is this interactive plot not showing up? Try the already made version [here](https://uhslc.soest.hawaii.edu/jfiedler/SeaLevelIndicators/output/Hawaii_Region_Output/SL_extremes_monthly_annual_maxima_Nawiliwili.html).\n",
    "```\n",
    "\n",
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":style: plain\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLI311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
